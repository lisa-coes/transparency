
## Herramientas para los análisis reproducibles

```{r include=FALSE}
if (!require("pacman")) install.packages("pacman") # instalar pacman
pacman::p_load(dplyr,       # Manipulacion de datos 
               sjmisc,      # descriptivos y frecuencias
               sjPlot,      # tablas, plots y descriptivos
               summarytools,# resumen de dataframe
               texreg,      # tablas de regresion
               knitr,       # tablas kable
               kableExtra,  # tablas kable personalizadas
               ggplot2,     # plots
               corrplot,    # matrices correlaciones
               webshot     # tomar screenshot de html
               )

load("input/data/proc/datos_proc.RData") # Cargar la base de datos directo desde dataverse
```

### Introducción

Por un lado tenemos todos los materiales que nos facilitan la reproducibilidad en términos computacionales, no obstante, la mera existencia de estos elementos no nos garantiza que un proyecto sea reproducible _per se_, dado que es importante tener en consideración _cómo_ se relacionan entre sí, para *regenerar* los resultados de un trabajo publicado. 

**Propósitos:**

* Primero, introducir un flujo de trabajo para los análisis reproducibles a través de un ejemplo práctico. 

* Segundo, ofrecer una guía de buenas prácticas referentes estándares de documentación de “inicio-a-fin” para una publicación en ciencias sociales (inspirado en [_Soup-to-Nuts_](https://www.projecttier.org/tier-classroom/soup-nuts-exercises/#shorter-exercises-for-teaching-transparency-and-reproducibility) de TIER).


**Principios:**

(i) _Reproducibilidad_: La documentación debe permitir regenerar completamente los resultados del estudio original. En primer lugar, se debe comenzar con los datos originales  brutos idénticos a aquellos con los que el autor comenzó la investigación, Luego, la posibilidad de realizar las mismas rutinas de código para preparar los datos finales para el análisis. Finalmente, se debe disponer de las rutinas de código que permitan regenerar los mismos resultados publicados, por ejemplo, las tablas o figuras presentes en la investigación.

(ii) _Independencia_: Toda la información necesaria para regenerar los resultados del estudio debe estar presente en la documentación. Esto refiere a que no debe ser necesario solicitar ninguna información adicional al autor. 

(iii) _Realismo_: La documentación debe estar organizada y presentada con suficiente claridad para que bajo un criterio realista, sea factible que un investigador independiente con un nivel de expertise razonable tenga la posibilidad de regenerar completa e independientemente los resultados del estudio sin mayores dificultades.

### Estructura de carpetas y documentación

El esquema de trabajo se basa en la estructura de proyectos del Protocolo [IPO](). Adicionalmente, se tomarán en consideración los estándares propuestos por el Protocolo [DRESS](), principalmente en lo que refiere a la documentación de cada uno de los componentes del proyecto. Para ello, trabajaremos en base a tres secciones:

I. Input


II. Procesamiento

III. Output


#### Software

Siguiendo los principios orientadores para una ciencia abierta, el uso de software de código abierto es una dimensión que contribuye a la apertura de un proyecto de investigación, dado que no requiere la adquisición de un software de pago. Por este motivo, la propuesta de trabajo se realizará utilizando el paquete estadístico R.

### Secciones de un proyecto

Debe ser pensado como un proyecto autocontenido, con una estructura de carpetas jerárquicas con archivos para cada función. 


####  Nivel principal

_Contenido_

El nivel principal corresponde al nivel base donde se encuentra toda la documentación de referencia general para el proyecto. Primero, el objetivo de documentar este nivel es exponer ordenadamente el contenido del proyecto completo de manera jerárquica, es decir, el contenido de subcarpetas y su función. Segundo, se proveen orientaciones para conducir a una correcta ejecución de las rutinas que permitan regenerar los resultados de la investigación. Los contenidos principales son:

1. Detalle del nivel principal y las subcarpetas organizadas según su contenido. Una manera amigable de representar esta estructura es a través de un “árbol de directorios”, el cual ilustra la jerarquía de las carpetas y los principales archivos contenidos. 


```
repro-lisa/
│
│   readme.md 
│   paper.Rmd 
│
├───input: (archivos de datos y utilidades)
│   │
│   ├───bib (subcarpeta  de input con documentos de bibliografía)
│   │
│   ├───data (subcarpeta de input con datos originales y procesados, junto con documentación)
│   │   ├───original (datos originales del estudio)
│   │   │   │   
│   │   │   └───documentacion (documentación de datos originales)
│   │   │
│   │   └───proc
│   │       │   
│   │       └───documentacion (documentación de datos procesados)
│   │
│   ├───images (subcarpeta de input con imágenes externas)
│   │
│   └───prereg (subcarpeta de input con documentación para preregistro)
│
├───procesamiento: (rutinas de código de preparación y análisis de datos)
│
└───output: (resultados de análisis en figuras y tablas)  
    ├───imagenes (subcarpeta de output para guardar figuras)
    │
    └───tablas (subcarpeta de output para guardar tablas)
```



> Tip: Cómog enerar un directory tree automatizado en [Windows](t.ly/c58k),[linux](t.ly/WlMC) y [MacOS](t.ly/ZCXK)

2. Instrucciones para la configuración (setup) del paquete estadístico necesario para ejecutar las rutinas de código. Esto considera el número de versión del software, los complementos necesarios que sean adicionales al paquete estándar y cualquier otra información especial sobre el software que el lector necesite conocer para reproducir los resultados del estudio. 

3. Instrucciones de “inicio-a-fin” para regenerar los resultados a través de referencias directas al uso de los archivos de procesamiento de datos en la preparación y análisis. En este apartado se incluye el detalle de los objetivos de cada rutina de código de manera independiente.



_Presentación_

Los contenidos descritos se deben incluir en un archivo que lleve de nombre “readme.md/txt/pdf”. Una sugerencia de estructura interna de este documento es la siguiente:

1. Estructura y contenido del proyecto reproducible
    * Esquema tipo “Árbol de directorios”
    * Descripción de cada subcarpeta, sus archivo y roles
2. Instrucciones y rutinas de ejecución de resultados
    * Instrucciones para configuración del software
    * Instrucciones para la ejecución de rutinas de código de “inicio-a-fin”.
  
  
#### I. Input

##### a) Explicación de Input 

La carpeta de input tiene la función de albergar todos los archivos necesarios para la elaboración del procesamiento y el análisis de los datos. El protocolo IPO propone cuatro subcarpetas: _data_ para los datos originales, _bib_ para los archivos de citado, _imageness_ para las imagenes que se vayan a incluir en el documento de preparación o el paper y _prereg_ donde se albergan todos los archivos necesarios para documentar un pre registro.

Separar los archivos en distintas carpetas permite, por una parte, ordenar mejor el flujo de trabajo ya que sabemos con certeza dónde está cada archivo y no tenemos que navegar en las carpetas para encontrarlo y, por otra parte, hace más reproducible el trabajo para terceros al facilitar la documentación del flujo.

##### b) Documentación de Input 

La descripción de los contenidos de la subcarpeta Input deben estar adecuadamente identificados y documentados. Como se ha indicado, existen cuatro subcarpetas que incluyen datos, bibliografía, imágenes y documentación de pre-registro. A continuación se mostrarán una serie de sugerencias de documentación de los archivos y subcarpetas de Input.

##### Datos (input/data)

###### Datos originales (input/data/original). 

Para toda fuente de datos original, se debe proveer la siguiente información:


a. Citación bibliográfica en un formato estándar (p. ej. American Psychological Association, Chicago, etc). Sugerimos revisar el componente de [“Datos Abiertos”](https://lisa-coes.netlify.app/02componentes/ ) para la publicación de datos. 
b. La fecha de creación de la base de datos o el día en que se accedió por primera vez por parte del autor.

c. Una descripción respecto a cómo se puede acceder a una copia de esta base de datos. Se debe ser lo suficientemente claro como para que un usuario independiente pueda acceder a los datos sin requerir información adicional.

d. Un libro de códigos de todas las variables de la base de datos. Sugerimos revisar el apartado [“¿Cómo hacer un libro de códigos?”](https://lisa-coes.netlify.app/como-hacer-codebook/).  


###### Datos procesados (input/data/proc).


Para toda fuente de datos procesada, es posible identificar dos tipos:

* Base de datos intermedia, la cual contiene información que, por un lado, puede ser complementaria con una base de datos principal. Por ejemplo, tenemos una base de datos con información de individuos pertenecientes a zonas/territorios (regiones o países), a la cual necesitamos incorporar información adicional que proviene de fuentes externas. En este caso, podemos generar una base procesada intermedia, para luego proceder a combinar ambas fuentes de datos.
* Base de datos final, es una versión final de una base de datos que contiene las variables completamente procesadas para realizar los análisis.

En estos casos se sugiere proveer la siguiente información:


a. Libro de códigos de la base procesada. Para ello, las variables deben estar correctamente etiquetadas.
b. Fecha de creación y versión de la base de datos procesada.


##### Referencias bibliográficas (input/bib)

Para toda fuente de referencia bibliográfica, se sugiere incorporar un archivo único en formato BibTeX (.bib) creado a través de algún gestor de referencias (p. ej. Zotero) el cual incluya todas las referencias empleadas en la publicación. Para la presentación del archivo se sugiere emplear un nombre breve, como por ejemplo “referencias.bib”. Además, se sugiere incorporar un archivo único en formato _Citation Style Language_ (.csl). Este archivo es un insumo básico para la citación en formato reproducible, ya que entrega el estilo de citado al documento. Generalmente se utilizan estilos de la _American Psychological Association_ o la _American Sociological Association_, sin embargo existen más de 10.000 estilos distintos. En el siguiente repositorio se pueden descargar los archivos .csl de distintos tipos de estilos: https://www.zotero.org/styles.

En el caso de no usar un gestor de referencias, simplemente se puede omitir esta sección. 

##### Imágenes (input/images)

Para las imágenes o figuras empleadas en la publicación, sugerimos emplear formatos o extensiones estándar como Joint Photographic Experts Group (JPEG) o Portable Graphics Format (PNG). En el caso de usar imágenes o fotografías con copyright, se debe referenciar correctamente o administrar permisos de uso de este material visual.

En el caso de no emplear imágenes externas, simplemente se puede omitir esta sección.

##### Preregistro (input/prereg)

Para toda la documentación que refiere al pre-registro de un estudio, se sugiere emplear una plantilla preestablecida para dicho propósito (ver Preregistros [LINK]). Para la presentación de la documentación referida al pre-registro se sugiere emplear un documento que lleve de nombre “preregistro.pdf”. 

En el caso de no haber pre-registrado el estudio, simplemente se puede omitir esta sección.


**Resumen de presentación**

```
├───input: 
│   │   readme-input.md 
│   │
│   ├───bib 
│   │       referencias.bib
│   │       apa6.csl
│   │
│   ├───data 
│   │   ├───original 
│   │   │   │   ELSOC_W01_v4.01_R.RData
│   │   │   └───documentacion 
│   │   │                   codebook_W01_S2016_ESP.pdf
│   │   │                   Questionnaire_W01_S2016_ESP.pdf
│   │   │                   User_Manual_ELSOC_Wave_01.pdf        
│   │   │
│   │   └───proc
│   │       │   datos_proc.RData
│   │       └───documentacion 
│   │                       codebook_datos_proc.pdf
│   │
│   ├───images 
│   │       logo-elsoc.png
│   │
│   └───prereg 
│           preregistro.Rmd 
│           preregistro.pdf 
```



#### II. Procesamiento

##### a) Explicación de Procesamiento 

En la sección de procesamiento, se espera albergar al menos dos documentos, uno que contenga el procesamiento de los datos y otro documento de análisis de datos. La principal razón para separar ambas actividades en documentos distintos es hacer más clara la lectura del código para la reproducibilidad. Esto quiere decir que, cualquier tercero que vaya a leer el código pueda comprender cómo se llegó a los resultados de forma paulatina, evitando a toda costa que, por ejemplo, en la mitad de la revisión del código emerja una pregunta tipo _¿y de dónde salió esta variable?_ En esta sección propondremos un flujo de trabajo para la generación de ambos documentos, así cómo también dejaremos planteadas algunas buenas prácticas para hacer estos documentos reproducibles.


###### Documento de procesamiento de los datos {-}

Esta sección cumple una función muy importante para el desarrollo de un artículo: la de procesar los datos que darán paso a los análisis del estudio. Considerando eso, el objetivo final de este documento es generar una **base de datos procesada**, que contenga solamente los datos importantes para analizar. El flujo que proponemos consta de dos partes, una de aspectos generales y otra sobre los procedimientos a realizar por cada variable:

###### Flujo para el documento de procesamiento de datos {-}

**Aspectos generales:**
**Cargar los paquetes estadísticos: el primer paso para comenzar un procesamiento de datos es instalar y activar los paquetes estadísticos que vamos a utilizar. Generalmente, los paquetes que necesitamos para el procesamiento de datos están contenidos en la colección de `tidyverse` (más información en https://www.tidyverse.org/). Recomendamos comentar cada paquete instalado con una frase corta que represente las principales funciones del paquete, con tal de que cualquier persona que quiera reproducir el código entienda por qué se está instalando este paquete sin tener la necesidad de acudir al manual.

Existe un paquete llamado `pacman`, el cual tiene la función de administrar los paquetes instalados. Recomendamos utilizar este paquete debido a su función `p_load`, la cual permite juntar tanto la instalación cómo la activación de distintos paquetes en una sola línea de código. Específicamente, esta función discrimina sí los paquetes están instalados o no, en el caso de no estarlo, los instala y sí ya están instalados, los activa. Un ejemplo a continuación:

```{r, eval=FALSE}
# Instalar paquetes usando R base
install.packages(“tidyverse”) # Instala el paquete
library(tidyverse)                   # Activa el paquete

# Instalar paquetes usando libreria “pacman” 
pacman::p_load(tidyverse, # Paquetes para el procesamiento de datos
               ggplot2    # Gráficos
              )
```
Cómo `pacman` también es un paquete, para poder utilizar sus funciones debemos instalarlo. Si bien podemos instalarlo de la forma clásica, recomendamos el siguiente código basado en un argumento condicional, donde si no está instalado le solicitamos a R que lo instale, y sí ya estaba instalado, que no haga nada. Ejemplo:

```{r, eval=FALSE}
# Instalar pacman a partir de un argumento condicional
if (!require("pacman")) install.packages("pacman")  #Si falta pacman, instalar
```

**Cargar la base de datos original:** Cargar la base de datos original es el punto de partida para el procesamiento de los datos, y cómo tal, es muy importante que esta acción sea reproducible. Cargar la base de datos original de forma que no se pueda reproducir abre la posibilidad de que todo el código que elaboremos a posterior sea inutilizable. De manera breve, dejamos estipulado cuál sería una forma no reproducible de cargar la base de datos, y una forma que sí es reproducible. 

Cargar la base de datos de forma no reproducible consiste en cargar la base de datos incluyendo la ruta completa hacia el archivo, esto quiere decir que, tenemos que especificar la ruta desde el disco duro que estamos trabajando hasta el archivo de la base de datos. Esta manera no es reproducible ya que solo aplica para el computador personal de quien está trabajando. Ejemplo:

```{r, eval=FALSE}
# Cargar base de datos forma no reproducible:

load("C:/Usuario/Documentos/repro-lisa/input/data/original/ELSOC_W01_v4.01_R.RData")

# Nota: Con “root” nos referimos a la carpeta raíz del proyecto, por lo que el nombre puede cambiar dependiendo del proyecto.
```

En cambio, la forma reproducible consiste en utilizar las direcciones basándonos en una carpeta raíz (forma 1) o cargarla directamente desde la web (forma 2). Primero, para poder establecer una carpeta raíz tenemos dos opciones; una opción es establecer manualmente un directorio de trabajo. Un directorio de trabajo es una ruta en la cual R asume que están los archivos que nos interesa trabajar, por lo que no es necesario especificar la ruta completa cada vez que necesitemos cargar un archivo. Para conocer cuál es el directorio de trabajo en el que está fijado R podemos utilizar el comando `getwd()`. Si queremos cambiar este directorio utilizamos el comando `setwd(“ruta hacia el archivo”)`. Para que esta opción sea reproducible, es importante **documentar** y explicar el proceso de establecer un directorio de trabajo. Una segunda opción para trabajar con una carpeta raíz es a través de  un documento .Rproj, estos son una forma reproducible de establecer una dirección de trabajo a través de RStudio. Es un archivo que solo tiene la finalidad de indicarle a R que la carpeta asociada al .Rproj es la carpeta raíz, por lo que todos los archivos estarán comprendidos dentro (o en subcarpetas). Este archivo es transferible, por lo que sí un tercero lo abre en su computador, no tendrá problemas en cargar la base de datos ya que R asumirá que la carpeta raíz es la asociada al .rproj. En el caso de trabajar con RStudio, recomendamos el uso de documentos .rproj.

La segunda forma de cargar la base de datos de forma reproducible es directamente desde la web, al hacer esto nos ahorramos el tener que descargar los datos. Sin embargo, es importante especificar que, en caso de cargar los datos directamente desde la web, estos deben estar albergados en un repositorio estable en el tiempo, ya que en caso de dejar de funcionar el URL el código deja de ser reproducible. También, es importante agregar que el link que utilicemos debe ser uno que lleve directamente a la descarga del archivo. En este caso, se están cargando los datos de ELSOC desde el repositorio Harvard Dataverse. A continuación, un ejemplo del código de ambas formas reproducibles utilizando ELSOC:

```{r, eval=FALSE}
# Cargar base de datos forma reproducible 1

getwd() # Para conocer cuál es el directorio de trabajo actualmente fijado

setwd("C:/Usuario/Documentos/root") # Para fijar un nuevo directorio de trabajo. Acá lo fijamos en la carpeta raíz del proyecto.

# Si se utiliza un documento .rproj se pueden omitir los pasos relacionados a establecer un directorio de trabajo

load("repro-lisa/input/data/original/ELSOC_W01_v4.01_R.RData") # Cargar la base de datos mientras estamos trabajando sobre un archivo .Rproj

# Cargar base de datos forma reproducible 2

load(url("https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/0KIRBJ/DWXZL1")) # Cargar la base de datos directo desde dataverse
```

**Revisar la base de datos:** Una vez cargada la base de datos original, recomendamos siempre revisar para que todo esté en orden. Esto lo podemos hacer con el comando `View()`, poniendo entre las comillas el nombre del objeto que le hemos dado a la base de datos. Cuando decimos “ver que todo esté en orden” nos referimos a diagnosticar si la base ha sido correctamente cargada. Por ejemplo, a veces podría suceder que la base de datos está en formato .csv con las columnas separadas por punto y coma (“;”), por lo que sí no lo especificamos en el código, tendríamos una base de datos con toda la información colapsada en una sola columna.

**Seleccionar las variables que se utilizarán:** Generalmente no ocupamos todas las variables dentro de una base de datos, en especial en la investigación basada en encuestas con datos secundarios. Es por eso que el comienzo del procesamiento de datos consta de seleccionar las variables que utilizaremos para los análisis. Aquí utilizamos el comando `select()` del paquete `dplyr`, tal y cómo se ve a continuación:

``` {r, eval=FALSE}
# Cargamos nuestra base de datos

load("input/data/original/ELSOC_W01_v4.01_R.RData") # Cargar la base de datos desde dirección local

# Revisamos que todo esté en orden

names(elsoc_2016l)
View(elsoc_2016l)

# Seleccionamos las variables

elsoc_proc<- 
  elsoc_2016 %>%
  dplyr::select(
    idencuesta, #identificador individual
    m0_sexo, #sexo
    m0_edad, #edad
    m01, #nivel de educación
    m13, # ingresos
    d01_01 #estatus social subjetivo
)
```

Del ejemplo destacamos dos buenas prácticas en lo que respecta a reproducibilidad. La primera es especificar que queremos llamar la función `select()` del paquete `dplyr` a través de los dos puntos `::`. Esto es especialmente útil cuando distintos paquetes comparten los nombres de las funciones. La segunda práctica se relaciona a separar cada variable seleccionada con un espacio, con tal de que cada variable sea una línea. Dentro de cada línea, comentar el nombre sustantivo de la variable. Por ejemplo, especificar que `d01_01` corresponde al estatus social subjetivo. 

**Renombrar las variables:** Si bien no es estrictamente necesario renombrar las variables, sí se recomienda para facilitar tanto el propio trabajo cómo el de alguien que vaya a emplear el mismo código. Generalmente, en la investigación de encuestas con datos secundarios nos encontramos con grandes bases de datos, con nombres técnicos y poco autoexplicativos. La principal recomendación aquí es cambiar estos nombres por nombres **cortos** y **autoexplicativos**. Por ejemplo, cambiar `d01_01` por `ess`, aludiendo a que es un indicador sobre estatus social subjetivo. Un ejemplo de renombrar se ve a continuación:

``` {r, eval=FALSE}

# Renombramos las variables

elsoc_proc <- 
  elsoc_proc %>%
  dplyr::rename(
    id = idencuesta, #identificador individual
    sexo = m0_sexo, #sexo
    edad = m0_edad, #edad
    educ = m01, #nivel de educación
    ingresos = m13, #ingresos
    ess = d01_01 #estatus social subjetivo
)
```

El proceso de renombrar variables también se puede hacer en conjunto al de seleccionar variables, sin embargo acá lo presentamos de forma separada para ser más esquemáticos.

**Procedimientos a realizar por cada variable:**

Una vez hemos cumplido con los aspectos generales del procesamiento, podemos pasar a la revisión de variable a variable. Aquí proponemos el siguiente flujo:

* Descriptivo incial: calcular una tabla de frecuencias o de medidas de tendencia central y dispersión para conocer el estado de la variable previo a cualquier modificación. 

*Recodificación: aquí se toman las decisiones respecto a la recodificación de los datos perdidos y otro tipo de valores a modificar  (e.g. errores de tipeo). Es importante que las decisiones sobre la recodificación queden bien estipuladas y transparentadas. Por ejemplo, en caso de hacer imputación en alguna variable, dejarlo comentado en el código.

* Etiquetado: el etiquetado es una forma simple y eficiente de poder dar más información acerca de una variable. En el caso de R, generalmente se usa el paquete `sjlabelled` para etiquetar tanto una variable (una columna de la base de datos) cómo una categoría de respuesta de la variable. En el caso de bases de datos sobre encuestas, generalmente una base bien documentada trae etiquetas predeterminadas que hacen alusión a las preguntas del cuestionario.

* Descriptivo final: recomendamos que, posterior a haber hecho las recodificaciones correspondientes, revisar de nuevo las frecuencias o las medidas de tendencia central de las variables, para diagnosticar que no hemos cometido errores en el procesamiento. Un ejemplo común, es que etiquetemos de forma errónea cada categoría de la variable. Esto tendría un impacto directo en la interpretación de los datos.

* Otros ajustes: en esta última parte del flujo por variable, recomendamos efectuar toda modificación específica y relevante para la forma que analizaremos los datos. Por ejemplo, si fuésemos a construir un índice con algunas de las variables.

Para esta sección de flujo de trabajo, también sugerimos, a modo de práctica de orden, que todo título y subtítulo estén especificados y jerarquizados. En detalle, proponemos que por cada título correspondiente a la variable a recodificar, cada aspecto del flujo sea un subtítulo, algo cómo:

- Título: Estatus social subjetivo
    - Subtítulo 1: Descriptivo inicial
    - Subtítulo 2: Recodificación
    - Subtítulo 3: Etiquetado
    - Subtítulo 4: Descriptivo final
    - Subtítulo 5: Otros ajustes
    
    
En R, esto lo podemos hacer de dos maneras. La primera es que, cuando trabajamos con scripts (.R) usar gato “#” para comentar, y cuatro guiones (“----”) entre el texto que queremos denominar cómo un título, ejemplo:

```{r, eval=FALSE}
# ---- 1. Estatus social subjetivo ----
# ---- 1.1 Descriptivo inicial----
# ---- 1.2 Recodificación ----
# ---- 1.3 Etiquetado ----
# ---- 1.4 Descriptivos final ----
# ---- 1.5 Otros ajustes ----
```
Al hacer esto, R irá ordenando los títulos de forma en la que vayan apareciendo en el script, como se ve en la Figura N° \@ref(fig:titulosscript).

```{r titulosscript, echo=FALSE, fig.cap="Ventana de script con el recuadro de títulos", fig.align = 'center', out.width = '100%'}
knitr::include_graphics(path = "docs/images/titulosscript.png")
```
    
La segunda forma de ordenar el flujo a través del trabajo con documentos dinámicos, específicamente R Markdown. R Markdown es un lenguaje que combina código en R y escritura en texto plano Markdown. Para ordenar los títulos del flujo, podríamos usar los gatos ("#") que denotan jerarquía de títulos en Markdown. Recomendamos usar los gatos "#" para subtitular las variables, y los pasos del flujo especificarlos con un comentario dentro del chunk. Ejemplo:


###### Sexo {-}

```{r}
# Descriptivo inicial
sjmisc::frq(elsoc_proc$sexo)

# Recodificación
elsoc_proc$sexo <- factor(elsoc_proc$sexo,labels = c('Hombre', 'Mujer'))

# Etiquetado
elsoc_proc$sexo <- sjlabelled::set_label(elsoc_proc$sexo, label = c("Tipo de sexo"))

# Descriptivo final
sjmisc::frq(elsoc_proc$sexo)

# Otros ajustes
# No aplica
```

En resumen, es una buena práctica contar con un orden óptimo para el documento de procesamiento de datos -y para cualquier documento en general-. En R, se puede lograr esto de dos formas. Cuando se trabaja con scripts (.R) se puede usar la nomenclatura "# ---- Título ----", y cuando se trabaja con documentos R Markdown (.Rmd) se pueden usar las jerarquías de títulos con los gatos ("#"). 

Para finalizar esta sección, en la Figura N° \@ref(fig:flujovar) esquematizamos el flujo propuesto.

```{r flujovar, echo=FALSE, fig.cap="Esquema de flujo de trabajo por cada variable", fig.align = 'center', out.width = '100%'}
# knitr::include_graphics(path = "docs/images/flujovar.png")
```


El último momento de la sección de procesamiento consiste en guardar la base de datos con los cambios realizados. En el caso de R, esto se reduce solamente a una línea de código, sin embargo, queremos aprovechar el momento para explicar una práctica muy importante para la reproducibilidad: **las rutas relativas**

Cómo hemos visto, el protocolo IPO busca proponer una estructura de carpetas simple, eficiente y reproducible para el procesamiento y el análisis de los datos. Este se basa en una carpeta raíz con tres subcarpetas: _input:_, _procesamiento_ y _output_, a lo cual, si le sumamos un documento .rproj, tenemos una estructura transferible entre computadores. Sin embargo, ¿qué ocurre cuándo necesitamos movernos entre carpetas para cargar archivos? Por ejemplo, si estamos trabajando en el documento de procesamiento y necesitamos insertar una imagen. Acá es donde entran en juego las rutas relativas, estas son una forma simple de moverse entre carpetas asumiendo una carpeta raíz. En la práctica, requiere solamente dos nociones:

* Si queremos avanzar entre carpetas, debemos usar _forward slashes_ (“/”) por cada subcarpeta que avancemos (e.g. input/data/archivo.R)
* Si queremos retroceder entre carpetas, debemos usar _forward slashes_ más dos puntos por cada nivel que retrocedamos (e.g. ../archivo.R)


Retomemos el ejemplo, si estamos trabajando en el documento de procesamiento (el cual se encuentra en la carpeta "procesamiento" y tenemos que cargar alguna imagen, podemos utilizar la siguiente ruta: "../input/images/imagen.png".

Estando en conocimiento del funcionamiento de las rutas relativas, el código para guardar la base de datos sería algo así:

```{r, eval=FALSE}
save(elsoc_proc,file = "../input/data/proc/datos_proc.Rdata")
```

###### Documento de análisis de datos {-}

Una vez contamos con nuestra base de datos procesada, es hora de la acción. En la sección del análisis de datos se procede a elaborar todas las tablas, gráficos, pruebas estadísticas etc. que vayan a ser introducidos en el artículo final. Es importante que se piense en este documento cómo un reporte de análisis en sí mismo, es decir, debe estar dirigido al público y no solo ser un documento de trabajo interno para el equipo de investigación. 

Al igual que para la sección de procesamiento de datos, aquí también recomendamos un flujo de trabajo para hacer el código reproducible y eficiente. Dividimos el flujo en dos secciones, primero, una que contenga los análisis necesarios para probar las hipótesis de investigación. Segundo, una sección con análisis secundarios y/o exploratorios que sean relevantes para lo que el artículo busca plantear. Veremos esto con un poco más de detalle.

###### Flujo para el documento de procesamiento de datos {-}

**Análisis para el artículo**

En esta parte del flujo, se espera trabajar con las variables que forman parte de las hipótesis de investigación, efectuando dos importantes pasos. El primer paso consiste en la descripción de variables y el segundo en el contraste de hipótesis de las mismas.

** Paso 1: Descripción de las variables:** En concreto, en este paso efectuamos al menos dos actividades. Primero, comenzamos por elaborar la **tabla general de descriptivos** que suele introducirse en la sección de método en un reporte de investigación. Recomendamos introducir esta tabla para que el lector conozca en detalle cómo se distribuyen las variables importantes de la muestra. En R, existen muchos paquetes que permiten elaborar este tipo de tabla, de los cuales mencionaremos tres y uno de ellos será el que recomendamos.

La función `stargazer` del paquete con el mismo nombre permite mostrar las medidas de tendencia central y las medidas de dispersión de la base de datos que introducimos. El código y una demostración del output es el siguiente:

```{r}
stargazer::stargazer(elsoc_proc,type = "text")
```
La función `descr` de la librería `sjmisc` también muestra los descriptivos de una base de datos:

```{r}
sjmisc::descr(elsoc_proc$ess)
```
Una forma de crearla de forma más estética es especificar los estadísticos que queremos visualizar y combinar la función `descr` con la función `kable` de la librería del mismo nombre. Esto permite elaborar una tabla apta para formato HTML.
```{r}
sjmisc::descr(elsoc_proc$ess,
      show = c("label","range", "mean", "sd", "NA.prc", "n"))%>%
      kable(.,"markdown")
```

La última forma de elaborar una tabla de descriptivos general es utilizar la función `dfsummary` de la librería `summarytools`. En contraste a las otras dos tablas, esta tabla abarca no solo medidas de tendencia central o dispersión, sino que también frecuencias de variables categóricas. Dicho de otra forma, esta función discrimina si las variables que estamos introduciendo son cuantitativas o categóricas y muestra los estadísticos correspondientes. Además, entrega una pequeña visualización de los datos: un gráfico de barras si las variables son categóricas y un histograma sí son cuantitativas. El código es el siguiente:

```{r}
df<- summarytools::dfSummary(elsoc_proc,
               plain.ascii = FALSE,
               style = "grid",
               tmp.img.dir = "/tmp",
               graph.magnif = 0.75,
               headings = F,  # encabezado
               varnumbers = F, # num variable
               labels.col = T, # etiquetas
               na.col = F,    # missing
               graph.col = T, # plot
               valid.col = T, # n valido
               col.widths = c(5,10,10,10,10)
               )
df$Variable <- NULL        # Borrar variable column
view(df,method = "render") # Visualizar tabla
```

La segunda actividad que podemos hacer en este paso del análisis de datos es **explorar las relaciones entre variables**. Acá las tablas o gráficos que elaboremos dependen de la naturaleza de las variables, donde:

* Relación entre dos variables categóricas: tabla de contingencia
* Relación entre una variable categórica y una continua: tabla de promedios por cada categoría
* Relación entre dos variables continuas: correlaciones.

La función `sjt.xtab` de la librería `sjPlot` construye de forma eficiente y estética una tabla de contingencia entre las dos variables señaladas. Además, muestra el valor relativo a la prueba de Chi2 en caso de que sea importante efectuar una prueba de hipótesis entre las variables expuestas en la tabla. 

```{r}
sjt.xtab(elsoc_proc$educ, elsoc_proc$sexo)
```
Se le pueden agregar los porcentajes si se considera necesario haciendo la siguiente modificación al código:

```{r}
sjt.xtab(elsoc_proc$educ, elsoc_proc$sexo, show.col.prc = TRUE)
```
La función `plot_grpfrq` de la librería `sjPlot` es una forma simple de graficar la distribución de una variable cuantitativa por cada categoría de la variable categórica. En el siguiente código se muestra cómo elaborar un gráfico de cajas por categoría:

```{r}
sjPlot::plot_grpfrq(elsoc_proc$ess,elsoc_proc$educ,
            type = "box")
```

Por último, tenemos varias funciones para representar la relación entre dos variables cuantitativas (correlaciones). La función `sjt.corr()` de `sjPlot` es una forma eficiente y estética de presentar una tabla de correlaciones:

```{r, eval = FALSE}
sjt.corr(elsoc_proc)
```
Además, podemos representar una correlación a través de matrices de correlación con la función `corrplot.mixed()` de la librería `corrplot`, agregando un paso previo que es crear un objeto que contenga las correlaciones a graficar. Esta matriz contribuye a gráficar de forma más digerible la información al introducir colores y formas. Color rojo significa correlaciones negativas y color azul correlaciones positivas, a su vez, mientras más grande el círculo más grande el coeficiente de correlación.

```{r, eval = FALSE}
M <- cor(elsoc_proc) # Paso previo: crear un objeto que albergue las correlaciones con la función base de R “cor”.

corrplot.mixed(M) # Graficar una matriz de correlación
```
Por último, también se puede visualizar la correlación entre dos variables cuantitativas a través de nubes de puntos. La función `plot_scatter()` de `sjPlot` es una buena opción para efectuar esta tarea:

```{r}
plot_scatter(elsoc_proc, edad, sexo)
```

**Paso 2: Contraste de hipótesis:**

El segundo paso para el documento de análisis de datos es escribir el código que permitirá producir y reproducir las técnicas de análisis inferencial. 

**Análisis secundario y/o exploratorio**

Una vez concluidos los análisis que son necesarios para reportar en el artículo, podemos dedicar nuestro tiempo y esfuerzos en una sección secundaria o exploratoria. La existencia de esta sección depende de sí, efectivamente, es relevante para efectos de la investigación el efectuar algún análisis adicional. Un artículo eminentemente confirmatorio podría no necesitar una sección específica. Sin embargo, nuestro punto es que, para efectos de claridad y transparencia del proceso de investigación, es importante dejar por separado todo análisis que sea adicional al análisis principal. Además, esto permite ser más eficiente en la preparación del material suplementario en caso de que una revista lo requiera.

Recomendamos el mismo flujo que vimos para la sección de análisis principal: descriptivos y contrastes de hipótesis.

```{r produccion2, echo=FALSE, fig.cap="Flujo de trabajo para procesamiento y análisis de datos", fig.align = 'center', out.width = '100%'}
knitr::include_graphics(path = "images/produccion2.png")
```

###### Buenas prácticas {-}

Terminaremos esta sección de procesamiento listando algunas buenas prácticas necesarias para asegurar la reproducibilidad de ambos documentos descritos:

* Nunca hacer trabajo manual, siempre automatizar el proceso en código fácilmente leíble. Esto implica: evitar a toda costa el uso de softwares que no permitan la reproducibilidad (e.g. Microsoft Excel)

* Asegurarse que el código siempre produzca el mismo resultado. Un ejemplo es cuando por algún tipo de análisis se necesitan generar números aleatorios. En R, para poder reproducir la generación de esos números aleatorios se utiliza la función `set.seed()`.

* Trabajar con scripts. Para poder automatizar el procesamiento y análisis de los datos, la principal recomendación es trabajar con documentos “script” que albergan el código y permiten su rápida ejecución. En el caso de R, se pueden utilizar documentos .R.

* Escribir con minúscula, sin espacios, sin ñ y sin tildes. Dos razones para esto, primero es que R está construido en base al idioma inglés, por lo que el utilizar “ñ” o tildes puede generar errores en la reproducción del código. Segundo, R es _case sensitive_, lo que significa que reconoce si el código está escrito con mayúsculas o con minúsculas. Sin embargo, no todos los softwares cumplen con esta característica, por lo que a modo de recomendación general no se incluyen mayúsculas en partes importantes del código. Por ejemplo, que lo único que diferencia dos variables sea una mayúscula.

* Indentar el código. La indentación es una característica del trabajo con código en general (no solo a nivel de software estadístico) y se refiere a la jerarquía en los niveles del código. En R, la recomendación es que por cada jerarquía de código se añaden dos espacios. Indentar permite una lectura más fácil del código. Uno de los ejemplos más conocidos es la elaboración de funciones condicionales de tipo `if-else`.

* Comentar el código. Cómo señalamos a lo largo de la explicación, comentar el código es sustancial para que cualquier persona no asociada al proyecto (o incluso uno mismo en el futuro) pueda entender para qué sirve cada función y reproducir los documentos sin problemas.

* Especificar las versiones de paquetes. Los paquetes están mejorando día a día, es por eso que a veces un código elaborado bajo cierta versión podría no funcionar con versiones futuras (e.g. que cambie el nombre de una función para la elaboración de tablas). Para que esto no sea un problema, siempre se recomienda utilizar la función `sessionInfo()` la cual entrega toda la información de versiones, tanto del software cómo de los paquetes. La recomendación es poner el output de esta función cómo parte del script, con tal de que quien lee el código pueda saber en detalle con qué paquetes y qué versiones se construyó el código. Otra recomendación un tanto más avanzada es fijar las versiones de los paquetes utilizados en el script a través de paquetes especializados. Un ejemplo es `groundhog`: https://groundhogr.com/.

* Elaborar código autocontenido. El código autocontenido es quizás el ápice de la reproducibilidad en el código, ya que no solamente es el documento el que es reproducible en sí mismo, sino que cada bloque de código. Esto implica que cada bloque de código no depende de otro: por ejemplo, elaborar una tabla con la función `stargazer()` no depende de que hayamos cargado el paquete al principio del documento, sino que el comando para cargar el paquete está dentro del mismo bloque de código. Veremos esto con más detalle en el ejemplo reproducible.

* Nombrar variables: nombres sustantivos y cortos. Esto es una recomendación que dimos durante la explicación, pero es bueno recordarla. Para que el código sea lo más fácilmente trabajable, debemos renombrar correctamente las variables que estamos trabajando.

* Etiquetado o buen diccionario de variables. Además de renombrar las variables, recomendamos etiquetar de forma sustantiva las variables que se utilizarán y/o hacer un buen diccionario de ellas.

* Utilizar UTF8. Si bien el lenguaje nativo de R es el inglés, sí se está escribiendo en español estipular siempre que se pueda que el formato del código sea UTF-8. El formato UTF-8 acepta caracteres especiales del español (cómo tildes y “ñ”), lo cual es especialmente relevante que funcione cuando se visualizan tablas o gráficos.

* Documentos dinámicos. Los documentos dinámicos son textos que combinan tanto código cómo texto en formato plano. En el caso de R, esto se logra mediante los documentos R Markdown. Recomendamos utilizar este formato para el flujo de trabajo propuesto acá ya que más compatible con la reproducibilidad y la eficiencia en el trabajo con datos, en contraste al uso de scripts (.R)

* Trabajar con rutas relativas. Cómo vimos, las rutas relativas permiten cargar o guardar archivos en directorios específicos sin necesidad de estipular la dirección local del computador. Esto contribuye a la reproducibilidad.

* Usar StackOverflow. Stack Overflow es un foro donde programadores, ingenieros y en general cualquier que utiliza código en su día a día puede hacer o responder preguntas respecto a código. Es una gran herramienta para cuando los códigos no funcionan.

##### b) Documentación de Procesamiento 

Para una correcta comprensión de la subcarpeta de procesamiento, sus archivos y roles, es importante tener una descripción adecuada de cada una de sus partes. Al estar mayormente orientado a la programación de rutinas de código, los documentos de preparación y análisis debieran contener la mayor parte de la información para una correcta comprensión del procedimiento.

No obstante, es relevante precisar de qué manera estos documentos se vinculan con otros archivos dentro del proyecto. Por un lado, el documento de preparación requiere de una fuente de datos inicial, por tanto está directamente relacionada con la carpeta Input y la subcarpeta de datos originales (`input/data/original`). Por otro lado, el documento de análisis requiere de una fuente de datos procesada, por tanto está directamente relacionada con la carpeta Input y la subcarpeta de datos procesados (`input/data/proc`). Además, se debe tener en consideración que los productos del análisis, es decir, los resultados de la investigación, pueden ser reportados en forma de figuras o tablas, las cuales son almacenadas en la carpeta Output y en las subcarpetas imágenes y tablas, respectivamente.

Para una correcta ejecución de las rutinas de código, es importante describir adecuadamente la relación entre los archivos de preparación y análisis. Para ello, se sugiere incorporar un archivo de nombre “readme-proc.md/txt/pdf”, en donde se describa brevemente dicha vinculación. Para ello sugerimos los siguientes puntos a describir:

1. Para la ejecución de la preparación, precisar la ubicación de la o las fuentes de datos originales. (p.ej. “input/data/original/original-data.dta”)
2. Para el cierre de la preparación, precisar la ruta donde se deben almacenar la base de datos procesada y su extensión (p.ej. “input/data/original/proc-data.RData”)
3. Para la ejecución de los análisis se debe precisar el origen de la base procesada que fue creada en el punto 2.
. Para los archivos de resultados provenientes del análisis de los datos, tales como figuras o tablas, debemos precisar la ruta donde se almacenarán y su nombre. 

**Resumen de presentación** 

```
├───procesamiento: 
        readme-proc.md 
        proc_analisis.Rmd
        proc_preparacion.Rmd
```

#### III. Output

##### a) Explicación de Output 

En la sección de output se espera albergar toda tabla, figura o gráfico relevante producto del código de análisis de datos. Para esto, el protocolo IPO propone dos carpetas, una de imágenes y otra de tablas. La idea de esta carpeta es que podamos guardar toda figura que vaya a ser parte del documento final del artículo.

##### b) Documentación de Output 

Como se ha señalado anteriormente, tenemos dos tipos de archivos contenidos dentro de las subcarpetas de Output. Para una correcta identificación de cada uno de estos elementos sugerimos seguir las siguientes indicaciones:

a. Para las imágenes, sugerimos usar nombres breves e incorporar numeración. Por ejemplo “figura01.png”, según el orden de aparición en la publicación.
b. Para el caso de los cuadros o tablas, existen distintas extensiones para almacenarlas como archivos independientes (tex/txt/md/html/xls). Para ello, sugerimos emplear nombres cortos e incorporar numeración. Por ejemplo, “tabla01.xls”, según el orden de aparición en la publicación. 

**Resumen de presentación**

```
└───output:  
    ├───imagenes 
    │       figura01.png
    │       figura02.png
    │
    └───tablas 
            tabla01.xls
            tabla02.xls

```