# Transparencia

La presente sección tiene trabaja en torno al concepto de **transparencia**, basandose en los siguientes objetivos:

- Entregar una definición precisa de qué se entiende por transparencia
- Exponer las razones que llevan a considerar necesaria una promoción de la transparencia
- Describir las formas en las que se puede promover la transparencia

Al final de esta sección el lector tendrá un manejo del concepto de transparencia y su relevancia para promover una ciencia social abierta.

## ¿Qué es la transparencia? Un concepto multidimensional

En términos generales, la transparencia se puede pensar cómo la cualidad de algo que se puede ver a través de él (RAE). Por ejemplo, el vidrio de una ventana hace posible ver con claridad lo que está del otro lado, sin embargo, si la ventana comienza a perder su transparencia la observación se torna más difícil. Más importante aun, la perdida de claridad causa una ambigüedad en la observación dando la posibilidad de emitir conclusiones erroneas sobre el otro lado de la ventana. Lo que antes era claramente un gato, una ventana poco transparente podría llevar hacer concluir que es un perro o cualquier otro tipo de animal. Este ejemplo entrega un punto de partida para reflexionar la transparencia en las ciencias sociales: a medida que aumentamos la transparencia podemos discernir con mayor claridad si estamos ante hallazgos científicos creíbles.

El concepto de transparencia ha tomado protagonismo en las ciencias sociales. Cada vez son más los autores que abogan por hacer una ciencia social transparente, así como también las iniciativas que buscan promover la adopción de principios transparentes. No obstante, cabe preguntarse ¿qué es exactamente lo que se está promoviendo? o más específicamente ¿cómo se está entendiendo la transparencia en la literatura? Por ejemplo, @breznau_Does_2021  entiende la transparencia como una forma en los investigadores pueden revelar el proceso, ideas y materiales que sustentan un argumento con tal de contribuir a una comunidad científica más ética. También, @aczel_consensusbased_2020 proponen la transparencia como un principio que permite evaluar y reproducir los hallazgos científicos, asi como también sintetizar investigaciones y contribuir a la ejecución de metanálisis. Estas formulaciones ayudan a ir comprendiendo lo que se dice actualmente en la literatura sobre transparencia, pero no son una presentación exhaustiva del concepto. En esta sección presentaremos dos fuentes que ayudan a comprender la complejidad que significa el concepto de transparencia. La primera será la taxonomía propuesta por @elliott_Taxonomy_2020, y la segunda serán las _TOP Guidelines_ (Guías para la promoción de la apertura y la transparencia) [@nosek_Transparency_2014].

```{r taxonomy, echo=FALSE, fig.cap="Taxonomía de Transparencia", fig.align = 'center', out.width = '100%'}
knitr::include_graphics(path = "docs/images/taxonomy.png")
```

La taxonomía de @elliott_Taxonomy_2020 tiene por objetivo sintetizar la complejidad del concepto de transparencia y las distintas formas que puede tomar. La taxonomía se basa en cuatro grandes preguntas: ¿por qué?, ¿quienes? ¿qué? y ¿cómo? Cada una de estas preguntas tiene al menos una dimensión asociada. La primera pregunta se refiere a los propósitos por los cuales adoptar la transparencia; la segunda pregunta apunta a la audiencia que se beneficia de la adopción de transparencia; la tercera pregunta hace alusión al contenido qué es transparentado y la cuarta pregunta hace alusión cuatro distintas dimensiones sobre cómo adoptar la transparencia: actores, mecanismos, tiempo y lugar. También, se añade una dimensión relacionada a los peligros de la transparencia en investigación. Una representación gráfica puede verse en la Figura N° \@ref(fig:taxonomy).

En la Figura N° \@ref(fig:tabtax) se presenta una versión detallada de las dimensiones de la taxonomía en conjunto a una lista no exhaustiva de variaciones dentro de cada dimensión. Por ejemplo, la dimensión del propósito sintetiza varios de los puntos que ya se han señalado en la literatura sobre ciencia abierta. En general, los propósitos están orientados a mejorar la credibilidad de la ciencia, ya sea en si misma (e.g. facilitación del reanálisis de resultados) o ya sea en su rol instrumental (e.g. promover el desarrollo de política pública de calidad). Otra dimensión relevante a destacar es el contenido de lo que se hace transparente, donde según @elliott_Taxonomy_2020 las variaciones van de lo más concreto como los datos, métodos y materiales hasta algo más complejo como los juicios de valor. Más adelante en el documento trataremos especialmente la transparencia de los datos, métodos y materiales.

```{r tabtax, echo=FALSE, fig.cap="Variaciones por dimensión de transparencia", fig.align = 'center', out.width = '100%'}
knitr::include_graphics(path = "docs/images/table_tax.png")
```


Otro tratamiento extensivo de la idea de transparencia se da en las TOP Guidelines. Estos son principios que buscan alcanzar un formato de investigación reproducible a tracés del aumento de la transparencia en el proceso y los productos de investigación [@nosek_Transparency_2014]. Estos principios sirven de guía a las revistas académicas para poder adherir progresivamente al ideal de transparencia en la ciencia. Son ocho principios:

1. Citación
2. Transparencia de datos
3. Transparencia de métodos análiticos (código)
4. Transparencia de los materiales
5. Transparencia del diseño y el análisis
6. Preregistro de estudios
7. Pre registro de planes de análisis
8. Replicación

A grandes rasgos, el principio de citación propone que las normas de citado deben ampliarse también a los datos y códigos, permitiendo reconocer su autoría intelectual [@nosek_Promoting_2015]. Los principios de transparencia de datos, métodos análiticos, materiales y diseño y análisis (2 a 5) refieren a la transparencia en su forma más concreta: la apertura del proceso de investigación para su evaluación. El detalle puesto a los principios responde a la generelabizilidad que se le busca dar a los principios. Por ejemplo, un estudio observacional cuantitativo no tiene material que transparentar, pero si datos y métodos análiticos. Asi también, un estudio cualitativo quizás no tenga código que transparentar, pero si un diseño y una bitacora detalla del proceso de análisis. En el caso de los principios relacionados al pre-registro, @nosek_Promoting_2015 argumenta que registrar los estudios los hace más descubribles, incluso si no son públicados. Asi también, los preregistros del plan de análisis contribuyen a distinguir entre los análisis confirmatorios y explotatorios (ver @nosek_preregistration_2018 para un manejo detalado del tema). Por último, el principios de replicación fomenta las oportunidades para la corrección de artículos y redirecciona la investigación en vías más prometedoras [@nosek_Promoting_2015].

Cada uno de estos principios cuenta con tres niveles, que sirven para medir el grado de inclusión de la transparencia por parte de una revista cientifica. La Figura N° \@ref(fig:tabtop) muestra cada nivel en relación el principio correspondiente. Se añade un nivel 0 que no cumple los estandares de transparencia con la finalidad de tener una comparación. Por ejemplo, para los estandares de transparencia del método de análisis (código), el nivel 1 dicta que las revistas deben solicitar la existencia del código de análisis, en cambio, el nivel 3 es más estricto en plantear que el código de análisis debe estar almacenado en un repositorio confiable y que el análisis será reproducido durante el proceso de revisión. El mismo método se puede aplicar para la el preregistro del plan de análisis. En el nivel 1 las revistas promueven el uso de preregistros, en cambio, en el nivel 3 los preregistros son obligatorios y también reconocidos. En suma, las TOP Guideliness son una iniciativa que contribuye a la apertura en la ciencia cambiando los requerimientos de las revistas.

Los dos ejemplos entregados (la taxonomía y las TOP Guideliness) perimiten comprender con un poco más de detalle el concepto de transparencia. Sin embargo, aun queda la interrogante del por qué. ¿Cuál es la problemática que lleva a considerar necesaria la adopción de la transparencia? En breve, la respuesta es la crisis en las ciencias, y especificamente en las ciencia sociales.

```{r tabtop, echo=FALSE, fig.cap="Variaciones por dimensión de transparencia", fig.align = 'center', out.width = '100%'}
knitr::include_graphics(path = "docs/images/table_top.png")
```
## ¿Cuál es el problema? La crisis de credibilidad en las ciencias sociales

En los últimos años, ha venido tomando fuerza la idea de que existe una crisis en la ciencia. Se han utilizado distintos nombres para aludir a esta idea. Algunos han hablado de una crisis de reproducibilidad [e.g. @baker_500_2016], siendo el principal problema que el formato actual de publicación cientifica no permite reproducir los hallazgos. Otros usan el término de crisis de "replicabilidad" [e.g. @swiatkowski_Replicability_2017], donde se argumenta que el principal problema es la falta de replicaciones de estudios científicos. También se ha usado el término de crisis de credibilidad [e.g. @bergh_there_2017; @carrier_Facing_2017], aludiendo a que la evidencia científica ha ido perdiendo su carácter de fiable o veraz, ya sea por prácticas poco éticas, como también por acumulación de errores en los procesos inevstigativos. Sea el concepto que se use, la narrativa instalada es que existe una crisis y que hay que encaminar los esfuerzos en abordarla.

Una de las fuentes más comunes para dar cuenta de la existencia de una crisis de reproducibilidad es una encuesta online realizada por la revista _Nature_. En esta encuesta, @baker_500_2016 logró obtener las opiniones de 1,576 investigadores de discplinas de las ciencias naturales sobre tópicos relacionados a la reproducibilidad en las ciencias, tales como sus factores y soluciones. El resultado principal muestra que un 52% de los investigadores encuestados creen que hay una crisis significativa, y un 38% creen que hay una ligera crisis. Es decir, un **90% de los encuestados está de acuerdo en la existencia una crisis**. Cuándo se les pregunta por los factores que contribuyen a esta crisis, un 60% de los investigadores están de acuerdo en que la cultura del _pública o perece_ y el reporte selectivo de resultados siempre o casi siempre contribuyen la crisis. Si bien la encuesta no es una muestra representativa de toda la comunidad cientifica, presenta una panoramica que lleva a, por lo menos, considerar la crisis de la ciencia como tema a investigar.

Actualmente, existe un cuerpo de literatura que se ha dedicado a debatir e intentar demostrar la existencia de una crisis de credibilidad en las ciencias -y en las ciencias sociales-. Esta literatura se ha centrado en buscar los factores que contribuirían a la crisis, así como también en desarrollar distintas herramientas, métodos y prácticas para revertirla. En esta sección señalaremos los factores que con mayor frecuencia se señalan en la literatura. Para esquematizar de mejor manera la presentación de estos factores y la evidencia existente, es que utilizaremos el esquema conceptual de @steneck_Fostering_2006. El esquema parte de una distinción básica entre la _ética en investigación_ y la _integridad en investigación_, englobando ambas bajo el gran concepto de _Conducta Responsable de Investigación (RCR)_ (ver Figura N° \@ref(fig:rcr). A grandes rasgos, la RCR se puede entender como el "llevar a cabo la investigación de forma que se cumplan las responsabilidades profesionales de los investigadores, tal y como las definen sus organizaciones profesionales, las instituciones para las que trabajan y, en su caso, el gobierno y el público" [@steneck_Fostering_2006][^1].

```{r rcr, echo=FALSE, fig.cap="Conducta Responsable de Investigación. Imagen de @abrilruiz_Manzanas_2019 basada en @steneck_Fostering_2006", fig.align = 'center', out.width = '100%'}
knitr::include_graphics(path = "docs/images/rcr.png")
```

[^1]: A raíz de esta definición, y también en base al código de conducta detallado por XX es que @abrilruiz_Manzanas_2019 proponen a la siguiente definición: "la RCR es el conjunto de normas, habitualmente plasmadas en códi- gos de conducta, que pretenden ser una guía para que nuestras acciones se mantengan dentro de la integridad investigadora" (p.144).

Dentro de este concepto, la ética de investigación está relacionada al comportamiento académico visto desde la óptica de los principios morales [@steneck_Fostering_2006], lo que se expresa en tópicos sobre el uso de datos, los consentimientos informados, el trato con pacientes en el caso de las ciencias biomedicas, por dar algunos ejemplos. La definición que ofrece @steneck_Fostering_2006 señala que la ética de investigación se define como "el estudio crítico de los problemas morales asociados o que surgen en el curso de la investigación" (p.56) En cámbio, la integridad en investiación se entiende como "poseer y adherirse firmemente a las normas profesionales, tal y como las señalan las organizaciones profesionales, las instituciones de investigación y, en su caso, el gobierno y el público" [@steneck_Fostering_2006, p.56]. A diferencia de la ética de investigación, el concepto de identidad está regido por los estándares profesionales más que por los principios morales, su función espantear una guía clara para la conducta inevestigativa, de ahí que sea el concepto utilizado por distintos códigos de conducta.

```{r grad, echo=FALSE, fig.cap="Gradación del comportamiento integro en investigación. Imagen de @abrilruiz_Manzanas_2019 basada en @steneck_Fostering_2006", fig.align = 'center', out.width = '100%'}
knitr::include_graphics(path = "docs/images/grad.png")
```

Habiendonos situando dentro del concepto de integridad en la investigación, podemos pasar a delinear las principales prácticas que atentan contra él y que se han propuesto como factores que contribuyen a la crisis en la ciencia. Tanto @steneck_Fostering_2006 como distintos códigos de conducta de universidades e instituciones de financiamiento [] plantean una gradación de prácticas de acuerdo en qué tanto atentan contra la integridad en investigación. La Figura N° \@ref(fig:grad) esquematiza esta idea mostrando dos extremos, donde a la izquierda está el mejor comportamiento (RCR), y a la derechaa el peor comportamiento (FFP). Las FPP son un abreviación en lengua inglésa para referirse a _Fabrication, Falsification, Plagiarism_ (Invención, Falsificación y Plagio). Las tres prácticas que componen la abreviación también se conocen como _mala conducta académica_-. En el medio de la gradación están las _prácticas cuestionables de investigación_ (QRP, por sus siglas en inglés) las cuáles refieren a "acciones que violan los valores tradicionales de la empresa de investigación y que pueden ser perjudiciales para el proceso de investigación" [_National Academies of Science_ 1992 en @steneck_Fostering_2006, p.58]. A diferencia de las FFP, las QRP son prácticas que tienen el potencial de dañar la ciencia y no que la dañan directamente (como las FFP).

En base a estas distinciones, presentaremos algunas situaciones y evidencia que dan cuenta de la FFP y las QRP.


### Mala conducta académica (FFP)

La mala conducta académica suelen ser situaciones que alcanzan gran conbertura mediatica y que no están exentas de polémicas. El libro de @abrilruiz_Manzanas_2019 presenta una serie de situaciones, en distintas discplinas y años, en las que investigadores han sido descubiertos cometiendo mala conducta. Las situaciones son variadas, existen casos de manipulación de imagenes [], exageración de lo registros de laboratorio, o de plano la invención de conjuntos de datos enteros. En esta sección veremos el caso de Diderik Stapel como ejemplo de malas prácticas de investigación y su relación con la transparencia en la investigación.

#### Diderik Stapel

Probablemente, el caso de Diderik Stapel sea uno de los más emblématicos y representativos de este problema. Diderik Stapel era un investigador de la _Tilburg University_ que se dedicaba al campo de la psicologóa social. Su carrera se caracterizó por una trayectoria ejemplar, obtuvo su M.A en psicolgía y comunicaciones el año 1991, se doctoró en psicología social el año 1997 y trabajó como profesor asociado primero en la _University of Gronigen_ (2000-2006) y dede el 2006 en la Tilburg University. Fue fundador del _Tilburg Institute for Behavioral Economics Research_, del cuál el 2010 se convirtió en decano. Así también, fue galardonado con el premio a la trayectoria académica por la _Social of Experimental Social Psychology_. En breve, Stapel era una figura de alto estatus en el mundo académico, entre 1995 y 2015 publicó aproximadamente 150 artículos en revistas cientificas, algunas de las más prestigiosas (e.g. _Science_). Sin embargo, el año 2011 se confirmó que gran parte de su trayectoria académica era una farsa.

Después de la verificación de su culpa ante acusaciones de malas prácticas, la carrera de Diderik Stapel acabó. Fue desvinculado de la Tilburg University, se le rebocó su título de doctorado y toda su trayectoria académica fue investigada acusiociamente. El informe final de la investigación encontró que más de 50 de sus artículos eran fraudulentos. De hecho, Stapel ocupa el tercer lugar en _Retraction Watch_, con 58 de sus artículos retractados. A efectos de este escrito, lo que más destacable es que Stapel cometió conductas de mala conducta académica durante más de 15 años. La pregunta es ¿cómo fue esto posible? ¿cómo ninguno de sus colegas, alumnos o co-autores se dio cuenta de sus malas prácticas? La respuesta breve es por la falta de transparencia durante el proceso de investigación.

Los artículos periodisticos que han profundizado en el caso [e.g. @carey_Fraud_2011] han relatado parte del proceso investigativo de Stapel. Dentro de los procesos de producción y análisis de datos, Stapel se caracterizaba por hacer todo el trabajo solo y "a puertas cerradas". Es decir, nadie más que él tenía acceso a los datos brutos, ni támpoco a la ejecución de las pruebas estadísticas. Generalmente, Stapel compartía con sus colegas y alumnos de doctorado la base de datos lista, con las pruebas estadísticas ya hechas y, claro está, con resultados significativos. Estas prácticas no causaron sospechas durante muchos años, es más, a muchos de sus estudiantes les parecía una práctica normal y eficiente. Además, con el estatus de Stapel ¿qué podría estar mal? Sin embargo, era a raíz de esta práctica que Stapel tenía la oportunidad de inventar y falsificar datos a su conveniencia. Esto explica en gran parte de su trayectoría académica llena de grandes hallazgos.

El caso de Stapel deja un punto base sobre la mesa: la falta de transparencia en el proceso investigativo dio cabida a la mala conducta académica. Cómo nadie más colaboraba con el procesamiento de datos, ni támpoco parecía extraño que asi fuera, las oportunidades para la falsificación de los datos estaba abierta. Ahora, esta no es necesariamente una relación de causalidad, la falta de transparencia no tiene porque terminar en conductas como fabricación o falsificación de datos. Sin embargo, tal y como lo argumentan [@oboyle_Chrysalis_2017] si son una oportunidad para violaciones a la integridad cientifíca más sutiles, tales como las QRP.

### Prácticas cuestionables de investigación (QRP)

Últimamente, el concepto de QRP ha sido bastante abordado en la literatura. Distintos autores han argumentado cómo y por qué las QRP pueden afectar a la credibilidad de la ciencia en su conjunto, y cómo la transparencia puede ser un principio que mitigue esta relación. Los aportes han ido desde opiniones, reflexiones o revisiones de literatura, hasta estudios empíricos que han buscado estimar la prevalencia de estas prácticas y su impacto. En esta sección haremos dos cosas. Primero, veremos algunas de las prácticas especificas que han sido categorizadas como QRP [^2] y segundo, presentaremos la evidencia que ha buscado cuantificar o estimar estas prácticas y ver su impacto en la ciencia.

La primera lista de prácticas que presentamos proviene del _Código Europeo de Conducta para la Integridad en la Investigación_ de la ALLEA. La lista, traducida y textual de @abrilruiz_Manzanas_2019, es la siguiente:

 [^2]: Muchas de las fuentes que se presentan acá son parte de la revisión que hace @abrilruiz_Manzanas_2019 sobre el concepto de QRP.

 1. Manipular la autoría o denigrar el papel de otros investigadores en las publicaciones.

2. Volver a publicar partes sustanciales de publicaciones propias anteriores, incluidas las traducciones, sin reconocer o citar debidamente el original ("autoplagio").

3. Citar de forma selectiva para mejorar los propios resultados o para complacer a los editores, los revisores o los colegas.

4. Retener resultados de la investigación.

5. Permitir que los patrocinadores pongan en peligro la independencia en el proceso de investigación o en la presentación de resultados con el fin de introducir sesgos.

6. Ampliar de manera innecesaria la bibliografía de un estudio.

7. Acusar a un investigador de conducta indebida u otras infracciones de forma maliciosa.

8. Tergiversar los logros de la investigación.

9. Exagerar la importancia y la relevancia práctica de los resultados.

10. Retrasar u obstaculizar inadecuadamente el trabajo de otros investigadores.

11. Emplear la experiencia profesional propia para alentar a que se incumpla la integridad de la investigación.

12. Ignorar supuestos incumplimientos de la integridad de la investigación cometidos por terceros o encubrir reacciones inadecuadas a conductas indebidas u otro tipo de incumplimientos por parte de las instituciones

13. Establecer publicaciones o brindar apoyo a publicaciones que no cumplen el proceso de control de calidad de la investigación (“publicaciones abusivas“)

Otro trabajo que presenta una lista sistematizada de QRP es el de @martinson_Scientists_2005. La lista, textual y traducida de @abrilruiz_Manzanas_2019 es la siguiente (incluye algunas FFP):

1. Falsificar o ("cocinar") datos de investigación.

2. Ignorar los aspectos importantes de los requerimientos de las personas participantes.

3. No divulgar adecuadamente la participación en empresas cuyos productos se basan en la investigación de uno.

4. Relaciones con estudiantes, sujetos de investigación o clientes que pueden ser interpretadas como cuestionables.

5. Usar ideas de otros sin obtener permiso o dar crédito

6. Uso no autorizado de información confidencial en relación con la propia investigación.

7. No presentar datos que contradicen una propia investigación previa.

8. Eludir ciertos aspectos menores de los requerimientos de las personas participantes.

9. Pasar por alto el uso de datos cuestionables o de interpretaciones cuestionables que otros hacen.

10. Cambiar el diseño, la metodología o los resultados de un estudio como respuesta a la presión de una fuente de financiación.

11. Publicar los mismos datos o resultados en dos o más publicaciones.

12. Asignar inapropiadamente los crédios de autoría.

13. Retener detalles de la metodología o los resultados en artículos o propuestas

14. Usar diseños de investigación inadecuados o inapropiados.

15. Eliminar observaciones o puntos de datos de los análisis basados en la intuición de que eran inexactos.

16. Mantener registros inadecuados relacionados con los proyectos de investigación.

Otro ejemplo es el de @john_Measuring_2012, quienes en su estudio sistematizan las prácticas en la siguiente lista -textual y traducida por @abrilruiz_Manzanas_2019-:

1. En un paper, no fueron reportadas todas las variables dependientes de un estudio.

2. Decidir si recopilar más datos después de ver si los resultados fueron significativos.

3. En un paper, no fueron reportadas todas las condiciones del estudio.

4. Dejar de recopilar datos antes de lo esperado porque se encontró el resultado que uno esperaba.

5. En un paper, redondear un valor p (por ejemplo, reportar que un p-value de 0,054 es menor a 0,05.

6. En un paper, selectivamente reportar estudios que ("funcionaron").

7. En un paper, decidir si excluir datos después de analizar el impacto en los resultados.

8. En un paper, reportar un hallazgo inesperado como previsto desde el principio.

9. En un documento, afirmar que los resultados no se ven afectados por variables demográficas (por ejemplo, el género), cuando uno no está realmente seguro (o sabe que lo hacen).

10. Falsificar datos

Otra lista, aunque menos extensiva, es la de @oboyle_Chrysalis_2017:

1. Eliminación o adición de datos después de pruebas de hipótesis.

2. Alterando los datos después de la prueba de hipótesis.

3. Supresión selectiva o adición de variables.

4. Invertir la dirección o reformular hipótesis para respaldar los datos

5. Eliminación o adición post hoc de hipótesis

La mayoría o la totalidad de las prácticas presentadas acá han sido nombradas de distintas formas a lo largo de los años. La lista presentada por @abrilruiz_Manzanas_2019 hace un buen resumen de los términos:

- _Intentional bias in Research_: sesgos que investigadores introducen en la investigación.
- _Fudging, massaging o cooking_: procesar los datos con tal de obtener resultados desados (e.g. que se cumpla una hipotésis que originalmente no se cumpliría). Este concepto suele caer dentro de las FFP.
- _Drylabbing_: reportar experimentos que nunca se realizaron
- _Cherry picking data_: reportar resultados que confirman hipotésis e ignorar los que la contradicen
- _Salami publication o salami slicing_: dividir una invesigación en pequeñas partes y publicar las partes por separado, con tal de contar con más publicaciones.
- _Publication bias_: publicar una investigación solo si el resultado es significativo.
- _Verification bias_: repetir un experimento hasta obtener resultados deseados o extraer casos con el mismo objetivo.
- _Data fishing o p-hacking_: realización de multiples pruebas estadísticas para hallar patrones significativos.
- _HARKing_[^3]: plantear las hipotesis una vez habiendo analizado los resultados, y reportar el proceso inverso.

[^3]: Este concepto no está en la lista original de @abrilruiz_Manzanas_2019, pero lo agregamos dado su reciente uso.

Una vez contamos con un paneo general sobre qué situaciones particulares caben dentro del concepto de QRP (y también de FFP), podemos pasar a revisar la evidencia en turno a su prevalencia. Una de las fuentas más citadas para una primera aproximación a la cuantificación de las QRP es el estudio de @john_Measuring_2012. En este estudio se encuestaron a más de 2000 psicologos sobre su implicación con las QRP. El procedimiento consistió en preguntar sobre las 10 malas prácticas de investigación más comunes. Las preguntas sobre cada QRP estaban divididas en tres formatos: primero, relacionados a su propio comportamiento; segundo, relacionado al comportamiento de otros psicologos y tercero, el porcentaje de psicologos que creían que admitiría haber cometido QRP. Adicionalmente, a aquellos que admitieron haber prácticado alguna QRP se les preguntó acerca de las justificaciones para hacerlo. Los resultados mostraron que un 36.6% de quienes participaron alguna vez habían práctiado alguna QRP. En detalle, analizando los porcentajes práctica a práctica se halló que el 50% de los psicologos encuestados alguna vez reportaron selectivamente estudios que apoyaran su hipotesis; un 35% alguna vez reportaron resultados inesperados como esperados; y un 2% alguna vez reportó datos falsos.

Otro estudio, algo más antiguo en cuanto a fecha -en comparación a @john_Measuring_2012-, pero más potente en cuanto a resultados es el de @fanelli_How_2009. En este estudio se hizo un metanálisis en varios idiomas sobre artículos que incluyeran palabras relacionadas a las malas prácticas en investigación (e.g. _research misconduct_) y que hubieran sido publicados entre 1988 y 2005. De un conjunto de referencias inicial de 3276, se seleccionaron 21 artículos. El criterio de inclusión era que fueran artículos cuantitativos de encuestas que preguntaran directamente sobre malas prácticas (como el de @john_Measuring_2012). Los principales resultados son los siguientes: en promedio, un 1.97% de investigadores admite haber fabricado datos al menos una vez; un 33.7% admite haber realizado QRP como "borrar puntos de los datos basados en un sentimiento visceral". Cuando se les pregunta por los comportamientos de otros, un 14% consiera que sus colegas caen en FFP y un 72% en QRP.

Un estudio que se aleja de la encuesta directa sobre prácticas, pero que contribuye estimando cuál es el potencial de que una QRP afecte los resultados de un artículo es el de @simmons_FalsePositive_2011. En detalle, los autores buscan calcular el _likelihood_ de obtener un falso positivo (error Tipo I) de acuerdo a al nivel de manipulación intencionada de los datos. El procedimiento consistió en calcular 15.000 muestras de 20 observaciones por condición de tratamiento para un experimento hipotetico y determinaron cuatro posibles ajustes a los datos: 1) plantear dos variables dependientes correlacionadas, 2) incrementar la muestra entre 20 y 30 casos según condición de tratamiento, 3) controlar por una variable sociodemográfica (género) o plantear una interacción genero y condición de tratamiento y 4) flexibilidad en eliminar alguna condición de tratamiento. Como se puede ver en la Figura N° \@ref(fig:fp), el resultado principal es que a medida que aumenta la cantidad de manipulación en los datos, el likelihood de obtener un falso positivo aumenta progresivamente. Una situación en donde se realicen los cuatro ajustes planteados tiene un 60.7% de likelihood de encontrar un falso positivo.

```{r fp, echo=FALSE, fig.cap="Likekihood de obtener un falso positivo según nivel de manipulación de datos. Imagen de @simmons_FalsePositive_2011", fig.align = 'center', out.width = '100%'}
knitr::include_graphics(path = "docs/images/fp.png")
```

Una herramienta que se ha utilizado para estimar la existencia de p-hacking en los cuerpos de literatura es la _p-curve_. La p-curve "describe la densidad de los _p-values_ reportados en una literatura, aprovechando el hecho de que si la hipótesis nula fuera verdadera (es decir, sin efecto), los p-values deben distribuirse uniformemente entre 0 y 1" [@christensen_Transparent_2019, p.67.]. De esta manera, en cuerpos de literatura que no sufran de p-hacking, la distribución de p-values debería ser asimetrica a la derecha, en cambio, si existe sesgo por p-hacking la distribución de p-values estaría distribuida de forma asimetrica a la izquierda. @simonsohn_Pcurve_2014 proponen esta herramienta y la prueban en dos muestras de artículos de la _Journal of Personality and Social Psychology (JPSP)_. Las pruebas estadísitcas consistieron en confirmar que la primera muestra de artículos (que presentaban signos de p-hacking) estaba sesgada, en cambio la segunda muestra (sin indicios de p-hacking), no lo estaba. Los resultados corroboraron las hipotesis, en detalle, los artículos que presentaban solamente resultados con covariables, resultaron tener una p-curve asimetrica a la izquierda.

Además del p-hacking, en la literatura también se habla del sesgo de publicación. Como se mencionó, el sesgo de publicación se refiere al reporte selectivo de resultados significativos, dejando sin reportar los no significativos. El estudio de @franco_Publication_2014 busca cuantificar esta situación, especificamente en ciencias sociales. En su estudio encontraron un patrón interesante: "estudios donde la hipotesis principales arrojan resultados nulos son 40% menos probables de ser publicados en una revista cientifica, en contraste a estudios que arrojen resultados significativos" [@christensen_Transparent_2019, p.41]. Los descriptivos se pueden ver en la Figura N° \@ref(fig:written).

```{r written, echo=FALSE, fig.cap="Porcentajes de publicación de acuerdo a significancia de resultados. Imagen de @christensen_Transparent_2019.", fig.align = 'center', out.width = '100%'}
knitr::include_graphics(path = "docs/images/fp.png")
```
...

> Parrafo final de la seccion que vuelva a conectar los factores con la idea de crisis



## ¿Qué podemos hacer? Recomendaciones, prácticas y experiencias

### Recomendaciones

### Experiencias

### Herramientas
