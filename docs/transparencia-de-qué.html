<!DOCTYPE html>
<html lang="es" xml:lang="es">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 ¿Transparencia de qué? | Transparencia en Investigación Social</title>
  <meta name="description" content="Documento de trabajo del equipo Transparencia para el Laboratorio de Ciencia Social Abierta" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 ¿Transparencia de qué? | Transparencia en Investigación Social" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/lisa-coes/transparency" />
  <meta property="og:image" content="https://github.com/lisa-coes/transparency/images/lisa-complete.png" />
  <meta property="og:description" content="Documento de trabajo del equipo Transparencia para el Laboratorio de Ciencia Social Abierta" />
  <meta name="github-repo" content="lisa-coes/transparency" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 ¿Transparencia de qué? | Transparencia en Investigación Social" />
  
  <meta name="twitter:description" content="Documento de trabajo del equipo Transparencia para el Laboratorio de Ciencia Social Abierta" />
  <meta name="twitter:image" content="https://github.com/lisa-coes/transparency/images/lisa-complete.png" />

<meta name="author" content="Investigador a cargo: Juan Carlos Castillo   Asistente de investigación: Julio Iturra   Pasante: Martín Venegas Márquez" />


<meta name="date" content="2021-08-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/lisa.ico" type="image/x-icon" />
<link rel="prev" href="transparencia.html"/>
<link rel="next" href="qué-se-ha-hecho.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/lisa.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/favicon.jpg"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="transparencia.html"><a href="transparencia.html"><i class="fa fa-check"></i><b>2</b> Transparencia</a>
<ul>
<li class="chapter" data-level="2.1" data-path="transparencia-de-qué.html"><a href="transparencia-de-qué.html"><i class="fa fa-check"></i><b>2.1</b> ¿Transparencia de qué?</a>
<ul>
<li class="chapter" data-level="" data-path="transparencia-de-qué.html"><a href="transparencia-de-qué.html#por-qué-adoptarla"><i class="fa fa-check"></i>¿Por qué adoptarla?</a></li>
<li class="chapter" data-level="" data-path="transparencia-de-qué.html"><a href="transparencia-de-qué.html#mala-conducta-académica-ffp"><i class="fa fa-check"></i>Mala conducta académica (FFP)</a></li>
<li class="chapter" data-level="" data-path="transparencia-de-qué.html"><a href="transparencia-de-qué.html#prácticas-cuestionables-de-investigación-qrp"><i class="fa fa-check"></i>Prácticas cuestionables de investigación (QRP)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="qué-se-ha-hecho.html"><a href="qué-se-ha-hecho.html"><i class="fa fa-check"></i><b>2.2</b> ¿Qué se ha hecho?</a></li>
<li class="chapter" data-level="2.3" data-path="herramientas-para-los-diseños-transparentes.html"><a href="herramientas-para-los-diseños-transparentes.html"><i class="fa fa-check"></i><b>2.3</b> Herramientas para los diseños transparentes</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="herramientas-para-los-diseños-transparentes.html"><a href="herramientas-para-los-diseños-transparentes.html#preregistros"><i class="fa fa-check"></i><b>2.3.1</b> Preregistros</a></li>
<li class="chapter" data-level="2.3.2" data-path="herramientas-para-los-diseños-transparentes.html"><a href="herramientas-para-los-diseños-transparentes.html#manos-a-la-obra-cómo-utilizar-un-preregistro"><i class="fa fa-check"></i><b>2.3.2</b> Manos a la obra: cómo utilizar un preregistro</a></li>
<li class="chapter" data-level="2.3.3" data-path="herramientas-para-los-diseños-transparentes.html"><a href="herramientas-para-los-diseños-transparentes.html#otras-herramientas"><i class="fa fa-check"></i><b>2.3.3</b> Otras herramientas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/lisa-coes" target="blank">LISA-COES <i class="fa fa-github"></i></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Transparencia en Investigación Social</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<head>
  <meta name="viewport" content="width=device-width,minimum-scale=1.0,maximum-scale=10.0,initial-scale=1.0">
  <!--script src="https://kit.fontawesome.com/6a26f47516.js"></script-->
  <script src="assets/hideOutput.js"></script>
  <link href="assets/lisa.css" rel="stylesheet">
</head>
        


<div class="hero-image-container"> 
  <img class= "hero-image" src="images/lisa-complete.png">
</div>
<div id="transparencia-de-qué" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> ¿Transparencia de qué?</h2>
<p>Comencemos por lo básico. ¿Qué significa que algo sea transparente? Según la Real Academia Española la transparencia es la cualidad de un cuerpo que permite ver a través de él. Un ejemplo para llevar esta definición a la práctica es el vidrio de una ventana. La transparencia del vidrio nos permite ver con claridad lo que está el otro lado, como por ejemplo un paisaje. Es decir, dada la cualidad transparente de la ventana, podemos analizar con detalle qué compone el paisaje: cuántos árboles hay, cómo está el clima, cuántos animales hay etc. Pero, ¿qué ocurre cuándo la transparencia del vidrio se va perdiendo? La respuesta es simple, pero potente: la claridad con la que veíamos el paisaje se va difuminando. Esta pérdida de claridad puede dar como resultado que nuestra observación del paisaje se torne ambigua y errónea, o dicho de otro modo, cada vez será más difícil evaluar qué es lo que efectivamente está en el paisaje. Desde este ejemplo se puede plantear un símil con la ciencia; el paisaje equivale al proceso de producción científica, y el vidrio representa la claridad con la que podemos analizar este proceso. De esta manera, la base de la idea de transparencia es que permite analizar con claridad un fenómeno, una situación, o en este caso, un proceso.</p>
<p>Entonces, ya tenemos una primera respuesta: <strong>la transparencia puede ser una cualidad de un proceso de producción científica</strong>, una cualidad que ciertamente se ve cómo positiva. Pensemos en cuándo le solicitamos transparencia a las entidades públicas en las finanzas o en la producción de leyes, básicamente estamos demandando algo similar: que el actuar de la entidad sea transparente, con tal de que la ciudadanía lo pueda evaluar con claridad. Quizás el ejemplo más claro son las cuentas públicas que año a año efectúa el gobierno de Chile, o también la ley de transparencia, donde cualquier ciudadano puede presentar un requerimiento para ver los documentos que respaldan una compra del gobierno o la producción de una ley.</p>
<p>Avancemos de los ejemplos anecdóticos al tema que nos convoca ¿Qué implica un proceso científico transparente? Algunos investigadores ya han planteado sus posturas respecto a esta pregunta. Por ejemplo, <span class="citation"><a href="#ref-breznau_Does_2021" role="doc-biblioref">Breznau</a> (<a href="#ref-breznau_Does_2021" role="doc-biblioref">2021</a>)</span> entiende la transparencia como una forma en que los investigadores pueden revelar el proceso, ideas y materiales que sustentan un argumento o una teoría, con tal de contribuir a una comunidad científica más ética. Otra perspectiva es la de <span class="citation"><a href="#ref-aczel_consensusbased_2020" role="doc-biblioref">Aczel et al.</a> (<a href="#ref-aczel_consensusbased_2020" role="doc-biblioref">2020</a>)</span>, quienes proponen a la transparencia como un principio que permite evaluar y reproducir los hallazgos científicos, así como también a sintetizar investigaciones y contribuir a la ejecución de metaanálisis. <span class="citation"><a href="#ref-klein_Practical_2018" role="doc-biblioref">Klein et al.</a> (<a href="#ref-klein_Practical_2018" role="doc-biblioref">2018</a>)</span> proponen que la credibilidad de los productos científicos dependen de la transparencia en la que se basan y que, en consecuencia, avanzar hacia la transparencia simplemente significa adoptar ciertas prácticas de gestión de la investigación que la hagan menos propensa a errores y más reproducibles. Estas perspectivas ofrecen una primera aproximación a las implicancias de un proceso científico transparente, no obstante, no representan una presentación exhaustiva del concepto.</p>
<p>Entonces ¿qué es la transparencia? Para introducir el concepto de una forma que demuestre su multidimensionalidad de manera simple es que nos basamos en la taxonomía de <span class="citation"><a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">Elliott</a> (<a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">2020</a>)</span>. La taxonomía se estructura en torno a cuatro preguntas: <em>¿por qué?, ¿quién? ¿qué? y ¿cómo?</em> Cada una de estas preguntas tiene al menos una dimensión asociada. La primera pregunta (<em>¿por qué?</em>) se refiere a los propósitos por los cuales es necesario adoptar la transparencia; la segunda pregunta (<em>¿quién?</em>) apunta a la audiencia que está recibiendo la información; la tercera pregunta (<em>¿qué?</em>) hace alusión al contenido qué es transparentado y la cuarta pregunta (<em>¿cómo?</em>) consiste en cuatro dimensiones distintas sobre cómo adoptar la transparencia: actores, mecanismos, tiempo y espacios. También, la taxonomía propone una dimensión sobre las amenazas que podrían afectar a las iniciativas que busquen promover la transparencia. Una representación gráfica puede verse en la Figura N° <a href="transparencia-de-qué.html#fig:taxonomy">2.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:taxonomy"></span>
<img src="docs/images/taxonomy.png" alt="Taxonomía de Transparencia" width="75%" />
<p class="caption">
Figura 2.1: Taxonomía de Transparencia
</p>
</div>
<p>Para comprender mejor la taxonomía la Tabla N° <a href="transparencia-de-qué.html#tab:tabtax">2.1</a> presenta una versión detallada de cada dimensión en conjunto a una lista no exhaustiva de variaciones. Al igual que la Figura N° <a href="transparencia-de-qué.html#fig:taxonomy">2.1</a>, la tabla está organizada en dimensiones asociadas a una pregunta, la diferencia es que aquí se añaden ciertas situaciones que ejemplifican cada dimensión de la transparencia, además de que hemos añadido la pregunta “¿Qué podría afectar iniciativas de transparencia?” para comunicar de mejor manera la dimensión de amenazas.</p>
<p>Partiendo por la dimensión del <strong>propósito</strong> la cual sintetiza varios de los puntos que ya se han señalado en la literatura sobre ciencia abierta: la transparencia contribuye a formar una ciencia más replicable, facilita la interacción crítica, facilita el reanálisis de resultados, entre otros. La mayoría de estos propósitos están estrechamente relacionados con el horizonte de una ciencia con mayor credibilidad, cuestión que profundizaremos en breve. Luego, la dimensión de la <strong>audiencia</strong> nos muestra distintos sujetos que podrían recibir la información transparentada, como otros científicos, desarrolladores de política pública o también el público general.</p>
<p>La dimensión de los <strong>contenidos</strong> es quizás la más importante a efectos de proponer una definición de transparencia. De hecho, es esta dimensión la que logra responder a la pregunta que hemos planteado en un principio: ¿transparencia de qué? Según la taxonomía, los distintos contenidos que pueden ser transparentados van desde cosas complejas como los juicios de valor o factores que podrían influenciar las interpretaciones de resultados, hasta otras más concretas como datos, métodos y materiales. Es en esta última en la que nosotros queremos enfatizar. Cuando se piensa en la transparencia de los datos, métodos y materiales, se pueden plantear dos formas de entenderla. Por un lado, está el planteamiento de que la transparencia de un proceso de investigación científico se da a través del libre acceso de sus datos y materiales. Es decir, con tal de que los datos y otros aspectos importantes del estudio estén disponibles públicamente ya es suficiente para denominar a ese estudio cómo transparente. Por otro lado, está la perspectiva de que también es necesario transparentar el diseño de investigación, lo que incluye las hipótesis, las formas de medir las variables y el plan de análisis, entre otros. Específicamente, se busca dejar a disposición del público el diseño de investigación previo a efectuarlo, con tal de robustecer el argumento de que los resultados no fueron alterados posterior a los análisis. Sí bien ambas perspectivas las consideramos en el libro (en el presente componente de transparencia y en el último de datos abiertos), es en la transparencia de los diseños de investigación donde focalizamos las energías en el presente capítulo.</p>
<p>Además de las tres dimensiones anteriores, contamos con cuatro dimensiones que están asociadas a la pregunta por el <em>¿cómo?</em> promover la transparencia. La primera dimensión es la de los <strong>actores</strong> que tienen el potencial de promover la transparencia, tales como periodistas o científicos. La segunda dimensión se trata de los <strong>espacios</strong> donde la transparencia puede ser promovida, donde algunos ejemplos son la divulgación científica, reportes de agencias gubernamentales o los repositorios que albergan la información transparentada. En tercer lugar, está el <strong>marco temporal</strong> en donde se puede optar por promover la transparencia. Aquí es variado, la transparencia puede ser ejercida tanto antes, durante o después del proceso de investigación a través de herramientas distintas y con implicancias distintas. Por ejemplo, en este documento nos centramos en la transparencia del diseño de investigación a través de los preregistros, lo que, generalmente, cabría dentro del antes de la investigación. Por último, están los mecanismos a través de los cuáles la transparencia puede ser ejercida, como en las colaboraciones interdisciplinares o en iniciativas gubernamentales.</p>
<p>La última dimensión sobre las <strong>amenazas</strong> trata sobre ciertas situaciones que podrían afectar las iniciativas sobre transparencia. Un ejemplo bastante esclarecedor es cuando se organiza una iniciativa para promover la transparencia en la investigación (e.g. un taller) y se acaba causando más confusión dado el exceso de información. Es por eso que, tener en cuenta estas amenazas permite anticipar los posibles problemas y orientar la ejecución de las iniciativas para evitarlos. Por ejemplo, este capítulo busca presentar la información relevante sobre la transparencia de la forma más simple posible, con tal de evitar abrumar y/o confundir al lector.</p>
<table class="table table-striped table-bordered" style="font-size: 10px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:tabtax">Tabla 2.1: </span>Variaciones por cada dimensión de transparencia
</caption>
<thead>
<tr>
<th style="text-align:left;">
Pregunta
</th>
<th style="text-align:left;">
Dimensión
</th>
<th style="text-align:left;">
Variaciones
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 8 cm; vertical-align: middle !important;" rowspan="8">
¿Por qué?
</td>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="8">
Proposito
</td>
<td style="text-align:left;width: 5 cm; ">
Facilitar el reanalisis de los resultados
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Hacer a la ciencia más replicable
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Promover la innovación
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Mantener la rendición de cuentas de los expertos
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Facilitar la interacción crítica
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Promover el desarrollo de política pública de alta calidad
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Permitir al público tomar decisiones de acuerdo a sus valores
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Promover la integridad
</td>
</tr>
<tr>
<td style="text-align:left;width: 8 cm; vertical-align: middle !important;" rowspan="7">
¿Quién?
</td>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="7">
Audiencia
</td>
<td style="text-align:left;width: 5 cm; ">
Cientificos haciendo la investigación
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Otros cientificos
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Desarrolladores de política pública
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Políticos
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Periodistas
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Grupos de interés
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Público general
</td>
</tr>
<tr>
<td style="text-align:left;width: 8 cm; vertical-align: middle !important;" rowspan="4">
¿Qué?
</td>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="4">
Contenido
</td>
<td style="text-align:left;width: 5 cm; ">
Datos, métodos, código y materiales
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Interpretaciones de los datos, metodos, y codigos para no especialistas
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Juicios de valor, factores que los influencian o sus implicancias
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Deliberaciones que subyacen a los reportes
</td>
</tr>
<tr>
<td style="text-align:left;width: 8 cm; vertical-align: middle !important;" rowspan="21">
¿Cómo?
</td>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="6">
Actores
</td>
<td style="text-align:left;width: 5 cm; ">
Cientificos que desarrollaron la investigación
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Otros científicos de la misma disciplina u otras
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Periodistas
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Sociedades cientificas
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Agencias gubernamentales
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Organizaciones no gubernamentales y de la sociedad civil
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="5">
Espacios
</td>
<td style="text-align:left;width: 5 cm; ">
Comunicación desde los cientificos (oral o escrita, incluyendo las redes sociales)
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Registros o repositorios
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Divulgación cientifica
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Reportes de agencias gubernamentales
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Reportes de agencias no gubernamentales o grupos de la comunidad
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="5">
Marco temporal
</td>
<td style="text-align:left;width: 5 cm; ">
Antes de que la investigación comience
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Durante el proceso de investigación
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Inmediatamente después de que los datos fueron recolerados
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Despés de la publicación
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Durante o después de las revisiones o el análisis de la investigación
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="5">
Mecanismos
</td>
<td style="text-align:left;width: 5 cm; ">
Discusiones entre cientificos (orales o escritas)
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Colaboraciones interdisciplinares
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Colaboraciones con miembros de la comunidad
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Organos asesores gubernamentales y otras iniciativas
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Procedimientos contradictorios (e.g. cortes científicas)
</td>
</tr>
<tr>
<td style="text-align:left;width: 8 cm; vertical-align: middle !important;" rowspan="6">
¿Qué podría afectar iniciativas de transparencia?
</td>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="6">
Amenazas
</td>
<td style="text-align:left;width: 5 cm; ">
Dañar compañías
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Violación de privacidad
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Generación de escepticismo inapropiado
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Crear un falso sentido de confianza
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Causar confusión
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Facilitación de esfuerzos para acosar o mal guiar.
</td>
</tr>
</tbody>
</table>
<p>En síntesis, un proceso de investigación transparente es uno que se puede evaluar con claridad y facilidad, tal y cómo cuando podemos evaluar la gestión de un ente público a raíz de leyes de transparencia. A partir de la taxonomía de <span class="citation"><a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">Elliott</a> (<a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">2020</a>)</span>, hemos podido comprender el <em>por qué</em>, <em>qué</em>, <em>quién</em> y <em>cómo</em> del concepto de transparencia. Hemos hecho énfasis respecto de los contenidos que se están transparentando y señalado que tanto la apertura de datos como la transparencia en los diseños de investigación son una característica de un proceso de investigación transparente. Sin embargo, no hemos profundizado en la dimensión del <em>por qué</em>. ¿Por qué habría de invertir tiempo y energía en aprender nuevas herramientas y/o prácticas para hacer que la ciencia sea más transparente. Pues, la comunidad de adherentes a la ciencia abierta y la literatura asociada han adoptado el discurso de que efectivamente es necesario cambiar la forma que estamos haciendo hoy en día. Ha emergido la narrativa de que la ciencia está en crisis.</p>
<div id="por-qué-adoptarla" class="section level3 unnumbered">
<h3>¿Por qué adoptarla?</h3>
<p>En los últimos años, ha tomado fuerza la idea de que existe una crisis en la ciencia. No obstante, las perspectivas sobre <em>qué</em> es lo que está en crisis son variadas. Algunos la denominan una crisis de reproducibilidad <span class="citation">(<a href="#ref-peng_reproducibility_2015" role="doc-biblioref">Peng 2015</a>; <a href="#ref-an_Crisis_2018" role="doc-biblioref">An 2018</a>; <a href="#ref-franca_Reproducibility_2019" role="doc-biblioref">França y Monserrat 2019</a>)</span>, otros una crisis de replicabilidad <span class="citation">(<a href="#ref-anvari_replicability_2018" role="doc-biblioref">Anvari y Lakens 2018</a>)</span> y otros de credibilidad <span class="citation">(<a href="#ref-gall_credibility_2017" role="doc-biblioref">Gall, Ioannidis, y Maniadis 2017</a>; <a href="#ref-byington_Solutions_2017" role="doc-biblioref">Byington y Felps 2017</a>)</span>. Así también, otros argumentan que no existe suficiente evidencia para hablar de una crisis en la ciencia <span class="citation">(<a href="#ref-fanelli_Opinion_2018" role="doc-biblioref">Fanelli 2018</a>)</span>. Independiente de sí se use la terminología de la crisis o no, la literatura es consistente respecto a que sí existen ciertas características de la academia y la investigación que, cambiandolas, podrían contribuir a una ciencia más creíble y de mayor calidad, la pregunta es ¿qué se puede cambiar para mejorar la ciencia?</p>
<p>Actualmente, existe un cuerpo de literatura que gira en torno a la pregunta recién planteada. Varios estudios han orientado sus esfuerzos a esclarecer cuáles son los factores que podrían estar influenciando una posible crisis de la ciencia. Al respecto, <span class="citation"><a href="#ref-baker_500_2016" role="doc-biblioref">Baker</a> (<a href="#ref-baker_500_2016" role="doc-biblioref">2016</a>)</span> hizo una encuesta donde preguntaba a investigadores de distintas disciplinas sobre la existencia de esta crisis y sus factores. En relación a sus resultados es posible plantear dos tipos de factores. El primero, de corte más estructural, está relacionado al modelo de producción científica actual, donde los incentivos por publicar <span class="citation">(<a href="#ref-angell_Publish_1986" role="doc-biblioref">Angell 1986</a>)</span>, los criterios de ascenso en la jerarquía académica <span class="citation">(<a href="#ref-flier_Faculty_2017" role="doc-biblioref">Flier 2017</a>)</span> y el formato de la revisión por pares contribuyen <span class="citation">(<a href="#ref-chambers_Registered_2013" role="doc-biblioref">Chambers 2013</a>)</span> a generar una cultura del <em>pública o perece</em> que fuerza a los investigadores a aumentar su productividad académica y facilita que caigan en <strong>prácticas cuestionables de investigación</strong> (QRP). El segundo factor, de corte más individual, está relacionado a la flexibilidad que tienen los investigadores a la hora de realizar investigaciones, generando la oportunidad para que también emerjan QRP <span class="citation">(<a href="#ref-christensen_Transparent_2019" role="doc-biblioref">G. S. Christensen, Freese, y Miguel 2019</a>; <a href="#ref-oboyle_Chrysalis_2017" role="doc-biblioref">O’Boyle, Banks, y Gonzalez-Mulé 2017</a>)</span>. En suma, ambos factores parecen tener algo en común: existen ciertas prácticas de investigación que afectan la credibilidad de la ciencia, por lo que la apuesta de muchos está en que identificandolas y proponiendo prácticas alternativas podemos mejorar la calidad de la ciencia <span class="citation">(e.g. <a href="#ref-miguel_Promoting_2014" role="doc-biblioref">Miguel et al. 2014</a>; <a href="#ref-chin_Questionable_2021" role="doc-biblioref">Chin et al. 2021</a>)</span>. En este capítulo buscamos contribuir a esa apuesta.</p>
<p>Para esquematizar de mejor manera qué es lo problemático de ciertas prácticas, es que utilizamos el esquema conceptual de <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span>. El esquema parte de una distinción básica entre la <em>ética en investigación</em> y la <em>integridad en investigación</em>, englobando ambas bajo el concepto de <em>Conducta Responsable de Investigación (RCR)</em> (ver Figura N° <a href="transparencia-de-qué.html#fig:rcr">2.2</a>. A grandes rasgos, la RCR se puede entender como “llevar a cabo la investigación de forma que se cumplan las responsabilidades profesionales de los investigadores, tal y como las definen sus organizaciones profesionales, las instituciones para las que trabajan y, en su caso, el gobierno y el público” <span class="citation">(<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck 2006</a>)</span>. En palabras simples, una conducta íntegra es atenerse a un conjunto de reglas sobre conducta científica.</p>
<div class="figure" style="text-align: center"><span id="fig:rcr"></span>
<img src="docs/images/rcr.png" alt="Conducta Responsable de Investigación. Imagen de @abrilruiz_Manzanas_2019 basada en @steneck_Fostering_2006" width="100%" />
<p class="caption">
Figura 2.2: Conducta Responsable de Investigación. Imagen de <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> basada en <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span>
</p>
</div>
<p>Dentro de la RCR, la ética de investigación está relacionada al comportamiento académico visto desde la óptica de los principios morales <span class="citation">(<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck 2006</a>)</span>, lo que se expresa en tópicos como el uso de datos, los consentimientos informados o el trato con los participantes de un estudio, por dar algunos ejemplos. La definición que ofrece <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span> señala que la ética de investigación se define como “el estudio crítico de los problemas morales asociados o que surgen en el curso de la investigación” (p.56) En cambio, la integridad en investigación se entiende como “poseer y adherirse firmemente a las normas profesionales, tal y como las señalan las organizaciones profesionales, las instituciones de investigación y, en su caso, el gobierno y el público” <span class="citation">(<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck 2006, 56</a>)</span>. Esto quiere decir que, a diferencia de la ética de investigación, el concepto de integridad está regido por estándares profesionales más que por principios morales, su función es plantear una guía clara para la conducta investigativa, de ahí que sea el concepto utilizado por distintos códigos de conducta <span class="citation">(ver <a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz 2019</a>)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:grad"></span>
<img src="docs/images/grad.png" alt="Gradación del comportamiento integro en investigación. Imagen de @abrilruiz_Manzanas_2019 basada en @steneck_Fostering_2006" width="100%" />
<p class="caption">
Figura 2.3: Gradación del comportamiento integro en investigación. Imagen de <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> basada en <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span>
</p>
</div>
<p>Habiéndonos situado dentro del concepto de integridad en la investigación, podemos pasar a delinear las principales prácticas que atentan contra él. Tanto <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span> como distintos códigos de conducta de universidades e instituciones de financiamiento <span class="citation">(ver <a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz 2019</a>)</span> evalúan las prácticas de investigación en un continuo, que representa cuánto adhieren los investigadores a los principios de integridad científica. La Figura N° <a href="transparencia-de-qué.html#fig:grad">2.3</a> esquematiza esta idea mostrando dos extremos, donde a la izquierda está el mejor comportamiento (RCR), y a la derecha el peor comportamiento (FFP). Las FPP son una abreviación en lengua inglesa para referirse a <em>Fabrication, Falsification, Plagiarism</em> (Invención, Falsificación y Plagio), también conocidas como <em>mala conducta académica</em>-. En el medio del continuo están las <em>prácticas cuestionables de investigación</em> (QRP, por sus siglas en inglés) las cuáles refieren a “acciones que violan los valores tradicionales de la empresa de investigación y que pueden ser perjudiciales para el proceso de investigación” <span class="citation">(<em>National Academies of Science</em> 1992 en <a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck 2006, 58</a>)</span>. Las QRP se caracterizan porque tienen el potencial de dañar la ciencia, en tanto las FFP la dañan directamente.</p>
<p>Comprendidos ambos conceptos, veamos las situaciones que podrían categorizarse como FFP y las que podrían concebirse como QRP.</p>
</div>
<div id="mala-conducta-académica-ffp" class="section level3 unnumbered">
<h3>Mala conducta académica (FFP)</h3>
<p>La mala conducta académica suelen ser situaciones polémicas y que muchas veces alcanzan gran cobertura mediática. El libro de <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> hace una revisión de una serie de situaciones, en distintas disciplinas y años, en las que investigadores han sido descubiertos cometiendo prácticas que atentan directamente a la ciencia (ver pp. 23-128). Las situaciones son variadas: existen casos de manipulación de imágenes, exageración de registros de laboratorio, o de plano la invención de conjuntos de datos enteros. En esta sección veremos el caso de Diderik Stapel como ejemplo de malas prácticas de investigación y plantearemos la relación que tiene con la transparencia en la investigación.</p>
<div id="el-caso-de-diderik-stapel" class="section level4 unnumbered">
<h4>El caso de Diderik Stapel</h4>
<p>Probablemente, el caso de Diderik Stapel sea uno de los más emblemáticos y representativos de este problema. Diderik Stapel era un investigador de la <em>Tilburg University</em> que se dedicaba al campo de la psicología social. Su carrera se caracterizó por una trayectoria ejemplar, obtuvo su M.A en psicología y comunicaciones el año 1991, se doctoró en psicología social el año 1997 y trabajó como profesor asociado primero en la <em>University of Gronigen</em> (2000-2006) y desde el 2006 en la Tilburg University. Fue fundador del <em>Tilburg Institute for Behavioral Economics Research</em>, del cuál el 2010 se convirtió en decano. Así también, fue galardonado con el premio a la trayectoria académica por la <em>Social of Experimental Social Psychology</em>. En breve, Stapel era una figura de alto estatus en el mundo académico, entre 1995 y 2015 publicó aproximadamente 150 artículos en revistas científicas, algunas de las más prestigiosas (e.g. <em>Science</em>). Sin embargo, el año 2011 se confirmó que gran parte de su trayectoria académica era una farsa.</p>
<p>Después de la verificación de su culpa ante acusaciones de malas prácticas, la carrera de Diderik Stapel acabó. Fue desvinculado de la Tilburg University, se le revocó su título de doctorado y toda su trayectoria académica fue investigada acusiociamente. El informe final de la investigación encontró que más de 50 de sus artículos eran fraudulentos. De hecho, Stapel está dentro de las diez figuras con más artículos retractados (58) en <em>Retraction Watch</em> (<a href="https://retractionwatch.com/" class="uri">https://retractionwatch.com/</a>), una plataforma que se dedica a sistematizar y mantener una lista actualizada sobre retracciones de artículos. A efectos de este escrito, lo que más destacable es que Stapel cometió conductas de mala conducta académica durante más de 15 años. La pregunta es ¿cómo fue esto posible? ¿cómo ninguno de sus colegas, alumnos o coautores se dio cuenta de sus malas prácticas? Pues, la respuesta tiene que ver con el concepto central de este capítulo: por la falta de <strong>transparencia</strong> durante el proceso de investigación.</p>
<p>Los artículos periodísticos que han profundizado en el caso han relatado parte del proceso investigativo de Stapel <span class="citation">(e.g. <a href="#ref-carey_Fraud_2011" role="doc-biblioref">Carey 2011</a>)</span>. Dentro de los procesos de producción y análisis de datos, Stapel se caracterizaba por hacer todo el trabajo solo y “a puertas cerradas”. Es decir, nadie más que él tenía acceso a los datos brutos, ni tampoco a la ejecución de las pruebas estadísticas. Generalmente, Stapel compartía con sus colegas y alumnos de doctorado la base de datos lista, con las pruebas estadísticas ya hechas y, claro está, con resultados estadísticamente significativos. Estas prácticas no causaron sospechas durante muchos años, es más, a muchos de sus estudiantes les parecía una práctica normal y eficiente. Además, con el estatus de Stapel ¿qué podría estar mal? Sin embargo, era a raíz de esta práctica que Stapel tenía la oportunidad de inventar y falsificar datos a su conveniencia. Esto explica en gran parte su trayectoria académica llena de grandes hallazgos y una producción académica increíble.</p>
<p>El caso de Stapel deja un punto importante sobre la mesa: la falta de transparencia en el proceso investigativo dio cabida a la mala conducta académica. Cómo nadie más colaboraba con el procesamiento de datos, ni tampoco parecía extraño que así fuera, las oportunidades para la falsificación de los datos estaban abiertas. Ahora, esta no es necesariamente una relación de causalidad, la falta de transparencia no tiene por qué terminar en conductas como fabricación o falsificación de datos. Sin embargo, tal y como lo argumentan <span class="citation"><a href="#ref-oboyle_Chrysalis_2017" role="doc-biblioref">O’Boyle, Banks, y Gonzalez-Mulé</a> (<a href="#ref-oboyle_Chrysalis_2017" role="doc-biblioref">2017</a>)</span> si son una oportunidad para violaciones a la integridad científica más sutiles, tales como las QRP.</p>
</div>
</div>
<div id="prácticas-cuestionables-de-investigación-qrp" class="section level3 unnumbered">
<h3>Prácticas cuestionables de investigación (QRP)</h3>
<p>Comencemos desde lo general. ¿Qué son las QRP? Como vimos anteriormente, son prácticas que están en el medio del continuo de integridad en la investigación, es decir, no son ni la mejor ni la peor conducta en lo que refiere a integridad en la investigación. El principal problema de las QRP es que alteran el proceso del método científico asociado al contraste de hipótesis <span class="citation">(<a href="#ref-bergkvist_Preregistration_2020" role="doc-biblioref">Bergkvist 2020</a>)</span>. En la literatura, existen una variedad de términos que se utilizan para describir las prácticas cuestionables (e.g. <em>salami slicing</em>, <em>p_hacking</em>), así como también distintas listas de prácticas que han emitido instituciones. <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> hace una recopilación y traducción de distintos códigos de conducta de distintas universidades y organismos, el cuál nosotros sistematizamos en la Tabla N° <a href="transparencia-de-qué.html#tab:tabqrp">2.2</a>. Esta tabla tiene por finalidad mostrar de una manera esquemática prácticas que alguna vez se han considerado cuestionables. Partiremos describiendo esta tabla con la finalidad de que el lector o lectora se vaya formando una idea de la amplia gama de prácticas cuestionables que existen, para luego pasar a especificar aquellas prácticas que tienen estrecha relación con la <strong>transparencia en los diseños de investigación</strong>.</p>
<p>La sistematización que hacemos de las prácticas presentadas por <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> se divide en cuatro dimensiones. La primera dimensión refiere a todas aquellas QRP que afectan el <strong>diseño de investigación y el procesamiento de los datos de un artículo</strong>, como por ejemplo eliminar datos después de las pruebas de hipótesis, suprimir o adicionar selectivamente variables o reformular las hipótesis para que calcen con los resultados. En cambio, la segunda dimensión apunta a los aspectos relacionados a la <strong>redacción, el reporte y la publicación de la investigación</strong>. En este apartado, podemos apreciar QRP que se relacionan al reporte selectivo de información (e.g. metodológica o de resultados) o a la forma en la que está escrito los resultados (e.g. exagerar la importancia de los resultados o tergiversar los logros de la investigación). La tercera dimensión se circunscribe al área de la <strong>propiedad intelectual y la autoría</strong>, donde las QRP tienen que ver con el uso de ideas sin dar crédito, o la asignación inapropiada de las autorías. Por último, identificamos una dimensión que tiene que ver con las <strong>relaciones con otros en el campo científico</strong>, donde las QRP se expresan en el incentivo o encubrimiento a cometer otro tipo de QRP, entre otros.</p>
<table class="table table-striped table-bordered" style="font-size: 10px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:tabqrp">Tabla 2.2: </span>Algunas situaciones de QRP
</caption>
<thead>
<tr>
<th style="text-align:left;">
Dimensión
</th>
<th style="text-align:left;">
Práctica
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="8">
Diseño y procesamiento
</td>
<td style="text-align:left;width: 5 cm; ">
Ignorar ciertos aspectos de los requerimientos de las personas participantes.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Pasar por alto el uso de datos cuestionables o de interpretaciones cuestionables que otros hacen.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Cambiar partes de un estudio como respuesta a la presión de una fuente de financiación
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Eliminar observaciones de los análisis basados en la intuición de que eran inexactos.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Redondear un valor p (por ejemplo, reportar que un p-value de 0,054 es menor a 0,05)
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Eliminación, adición o alteración de datos después de pruebas de hipótesis.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Supresión selectiva o adición de variables.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Invertir la dirección o reformular hipótesis para respaldar los datos
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="10">
Redacción, reporte y publicación
</td>
<td style="text-align:left;width: 5 cm; ">
Ampliar de manera innecesaria la bibliografía de un estudio.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Tergiversar los logros de la investigación.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Exagerar la importancia y la relevancia práctica de los resultados.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Retener detalles de la metodología de investigación (e.g. no reportar todas las variables dependientes de un estudio)
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Retener resultados de la investigación (e.g. no presentar datos que contradicen una propia investigación previa).
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Establecer publicaciones o brindar apoyo a publicaciones que no cumplen el proceso de control de calidad de la investigación
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Publicar los mismos datos o resultados en dos o más publicaciones.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Selectivamente reportar estudios que “funcionaron”.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Reportar hallazgos inesperados como previstos desde el principio.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Afirmar que los resultados no se ven afectados por variables demográficas cuando uno no está realmente seguro (o sabe que lo hacen).
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="6">
Citación y autoría
</td>
<td style="text-align:left;width: 5 cm; ">
Manipular la autoría o denigrar el papel de otros investigadores en las publicaciones.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Asignar inapropiadamente los crédios de autoría.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Volver a publicar partes sustanciales de publicaciones propias anteriores, incluidas las traducciones, sin citar debidamente el original (“autoplagio”).
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Citar de forma selectiva para mejorar los propios resultados o para complacer a los editores, los revisores o los colegas.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Usar ideas de otros sin obtener permiso o dar crédito
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Uso no autorizado de información confidencial en relación con la propia investigación.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; vertical-align: middle !important;" rowspan="7">
Relaciones con otros
</td>
<td style="text-align:left;width: 5 cm; ">
Permitir que los patrocinadores pongan en peligro la independencia en el proceso de investigación con el fin de introducir sesgos.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Acusar a un investigador de conducta indebida u otras infracciones de forma maliciosa.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Retrasar u obstaculizar inadecuadamente el trabajo de otros investigadores.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Emplear la experiencia profesional propia para alentar a que se incumpla la integridad de la investigación.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Ignorar supuestos incumplimientos de la integridad de la investigación cometidos por terceros o encubrir reacciones inadecuadas a conductas indebidas.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
No divulgar adecuadamente la participación en empresas cuyos productos se basan en la investigación de uno.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
Relaciones con estudiantes, sujetos de investigación o clientes que pueden ser interpretadas como cuestionables.
</td>
</tr>
<tr>
<td style="text-align:left;width: 5 cm; ">
</td>
<td style="text-align:left;width: 5 cm; ">
</td>
</tr>
</tbody>
</table>
<p>No es difícil imaginar que las QRP que engloba la Tabla N° <a href="transparencia-de-qué.html#tab:tabqrp">2.2</a> hayan ocurrido alguna vez. Probablemente, hemos escuchado la historia de algún conocido que se vio envuelto en cualquiera de estas situaciones o nosotros hemos sido parte de alguna de estas situaciones en algún momento de nuestra carrera académica. Sin embargo, más allá de la ocurrencia anecdótica de estas situaciones, es relevante preguntarse ¿son algo que ocurre seguido?</p>
<p>Existen una serie de estudios que han intentado medir directamente la prevalencia de estas prácticas a través de encuestas. <span class="citation"><a href="#ref-fanelli_How_2009" role="doc-biblioref">Fanelli</a> (<a href="#ref-fanelli_How_2009" role="doc-biblioref">2009</a>)</span> hizo un metaanálisis que tenía por objetivo sistematizar los resultados de estudios que hasta esa fecha habían abordado las prácticas de investigación desde encuestas cuantitativas. Los resultados mostraron que un 1.97% de investigadores había inventado datos al menos una vez (FFP) y que un 33.7% había realizado alguna vez una QRP como “borrar puntos de los datos basados en un sentimiento visceral”. Unos años más tarde, <span class="citation"><a href="#ref-john_Measuring_2012" role="doc-biblioref">John, Loewenstein, y Prelec</a> (<a href="#ref-john_Measuring_2012" role="doc-biblioref">2012</a>)</span> efectuaron otro estudio similar, demostrando que un 36.6% de quienes participaron alguna vez habían practicado alguna QRP. En detalle, analizando los porcentajes práctica a práctica se halló que el 50% de los psicólogos encuestados alguna vez reportaron selectivamente estudios que apoyaran su hipótesis; un 35% alguna vez reportaron resultados inesperados como esperados; y un 2% alguna vez reportó datos falsos. Este estudio ha sido replicado en Italia <span class="citation">(<a href="#ref-agnoli_Questionable_2017" role="doc-biblioref">Agnoli et al. 2017</a>)</span>, en Alemania <span class="citation">(<a href="#ref-fiedler_Questionable_2016" role="doc-biblioref">Fiedler y Schwarz 2016</a>)</span> y en Brasil <span class="citation">(<a href="#ref-rabelo_Questionable_2020" role="doc-biblioref">Rabelo et al. 2020</a>)</span>.</p>
<p>Más recientemente, han emergido estudios similares en otras áreas disciplinarias. Por ejemplo, a través de encuestas online <span class="citation"><a href="#ref-makel_Both_2021" role="doc-biblioref">Makel et al.</a> (<a href="#ref-makel_Both_2021" role="doc-biblioref">2021</a>)</span> logran constatar que existe una prevalencia considerable de las QRP en el campo de investigación educacional. De los participantes de la muestra un 10% admitió haber rellenado datos faltantes (NA’s) y un 67% señaló alguna vez haber omitido ciertos análisis de manera intencional. Siguiendo el mismo método, en el campo de la investigación comunicacional se ha encontrado evidencia parecida: 9% de los encuestados señala haber imputado datos faltantes sin reportarlo, un 34% declaró alguna vez haber excluido casos extremos de forma arbitraria y un 60% señala no haber reportado análisis con variables clave que no funcionaron. Del mismo modo, en los estudios cuantitativos sobre criminología existe un uso extendido de las QRP: un 87% de la muestra de <span class="citation"><a href="#ref-chin_Questionable_2021" role="doc-biblioref">Chin et al.</a> (<a href="#ref-chin_Questionable_2021" role="doc-biblioref">2021</a>)</span> ha utilizado múltiples QRP, siendo el reporte selectivo de resultados el más común (53%). Por último, fuera del área de las ciencias sociales, pero siguiendo la misma línea, <span class="citation"><a href="#ref-fraser_Questionable_2018" role="doc-biblioref">Fraser et al.</a> (<a href="#ref-fraser_Questionable_2018" role="doc-biblioref">2018</a>)</span> también hallan evidencia a favor de la existencia de distintas QRP en el campo de la ecología y evolución.</p>
<p>Los estudios mencionados arriba corresponden a la evidencia existente sobre la medición de QRP a través de encuestas. A modo de resumen, la Tabla N <a href="#tab:tabqrp2"><strong>??</strong></a> elaborada por <span class="citation"><a href="#ref-chin_Questionable_2021" role="doc-biblioref">Chin et al.</a> (<a href="#ref-chin_Questionable_2021" role="doc-biblioref">2021</a>)</span> agrupa la información que hemos revisado (y más), ordenándose por prácticas, campos de estudio y los artículos correspondientes a cada campo de estudio. Los números dentro de las casillas representan el porcentaje de prevalencia de cada práctica reportado por los participantes del estudio, y entre paréntesis el número de casos de la muestra. Los asteriscos significan que este estudio no incluyó esa práctica en el cuestionario.</p>
<table style="width:100%;">
<colgroup>
<col width="27%" />
<col width="9%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th>Práctica</th>
<th>Psicología</th>
<th>Psicología</th>
<th>Psicología</th>
<th>Ecología</th>
<th>Evolución</th>
<th>Educación</th>
<th>Comunicación</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td>John et al. (2012)</td>
<td>Agnoli et al. (2017)</td>
<td>Rabelo et al. (2020)</td>
<td>Fraser et al. (2018)</td>
<td>Fraser et al. (2018)</td>
<td>Makel et al. (2021)</td>
<td>Bakker et al. (2020)</td>
</tr>
<tr class="even">
<td>Omitir estudios o variables ni significativas</td>
<td>46 (485)</td>
<td>40 (217)</td>
<td>55 (232)</td>
<td>*</td>
<td>*</td>
<td>62 (783)</td>
<td>60</td>
</tr>
<tr class="odd">
<td>Subreportar resultados</td>
<td>63 (486)</td>
<td>48 (219)</td>
<td>22 (232)</td>
<td>64</td>
<td>64</td>
<td>67 (871)</td>
<td>64</td>
</tr>
<tr class="even">
<td>Subreportar condiciones</td>
<td>28 (484)</td>
<td>16 (219)</td>
<td>35 (232)</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
</tr>
<tr class="odd">
<td>Muestreo selectivo</td>
<td>56 (490)</td>
<td>53 (221)</td>
<td>22 (232)</td>
<td>37</td>
<td>51</td>
<td>29 (806)</td>
<td>23</td>
</tr>
<tr class="even">
<td>Excluir datos selectivamente</td>
<td>38 (484)</td>
<td>40 (219)</td>
<td>20 (232)</td>
<td>24</td>
<td>24</td>
<td>25 (806)</td>
<td>34</td>
</tr>
<tr class="odd">
<td>Excluir covariables selectivamente</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>42 (773)</td>
<td>46</td>
</tr>
<tr class="even">
<td>Cambiar análisis selectivamente</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>50 (811)</td>
<td>45</td>
</tr>
<tr class="odd">
<td>HARK</td>
<td>27 (489)</td>
<td>37 (219)</td>
<td>9 (232)</td>
<td>49</td>
<td>54</td>
<td>46 (880)</td>
<td>46</td>
</tr>
<tr class="even">
<td>Redondear valores p</td>
<td>22 (499)</td>
<td>22 (221)</td>
<td>18 (232)</td>
<td>27</td>
<td>18</td>
<td>29 (806)</td>
<td>24</td>
</tr>
<tr class="odd">
<td>Mal orientar respecto a los efectos de sociodemográficos</td>
<td>3 (499)</td>
<td>3 (223)</td>
<td>4 (232)</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
</tr>
<tr class="even">
<td>Esconder problemas</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>24 (889)</td>
<td>*</td>
</tr>
<tr class="odd">
<td>Esconder imputaciones</td>
<td>1 (495)</td>
<td>2 (220)</td>
<td>1 (232)</td>
<td>5</td>
<td>2</td>
<td>10 (898)</td>
<td>9</td>
</tr>
</tbody>
</table>
<p>Cómo hemos visto en la Tabla X y en la Tabla Y, existen muchas prácticas que podrían categorizarse cómo cuestionables. No obstante, hay algunas prácticas que queremos destacar aquí dado que están relacionadas con la transparencia en los diseños de investigación. Estas son: 1) los sesgos de publicación, 2) el <em>p hacking</em> y el 3) <em>HARKing</em>.</p>
<p><strong>Sesgo de publicación</strong></p>
<p>El <strong>sesgo de publicación</strong> significa que un artículo es publicado en base a sus resultados. Específicamente, el sesgo de publicación ocurre cuando el criterio determinante para que un artículo sea publicado es que sus resultados sean significativos, en desmedro de la publicación de resultados no significativos. Un ejemplo ilustrativo que usan <span class="citation"><a href="#ref-christensen_Transparent_2019" role="doc-biblioref">G. S. Christensen, Freese, y Miguel</a> (<a href="#ref-christensen_Transparent_2019" role="doc-biblioref">2019</a>)</span> para explicar esta práctica es el cómic <em>xkcd</em> títulado <em>Significant</em>. En el comic (Figura N <a href="transparencia-de-qué.html#fig:significant">2.4</a>) se puede observar que un personaje corre gritando que las gominolas (<em>jellybeans</em>) causan acné, a lo que el otro personaje llama a los científicos para que prueben esta hipótesis, resultando no significativa. Ante esto, nuevamente el personaje inicial plantea que podría depender del tipo de gominola, y es aquí donde se aprecia lo ilustrativo del cómic: aparecen 20 paneles, cada uno representando una prueba de hipótesis entre una gominola de determinado color y al acné. 19 de las pruebas resultan no significativas, y una (el color verde) resulta significativa. El cómic termina con una portada con el titular de la única prueba de hipótesis que arrojó resultados significativos.</p>
<div class="figure" style="text-align: center"><span id="fig:significant"></span>
<img src="http://es.xkcd.com/site_media/strips/significant.png" alt="Comic _Significant_ de xkcd" width="75%" />
<p class="caption">
Figura 2.4: Comic <em>Significant</em> de xkcd
</p>
</div>
<p>El cómic anterior muestra cómo es que un hallazgo de investigación sufre del sesgo de publicación. Al publicarse únicamente el resultado significativo e ignorándose los otros 19 no significativos, cualquier lector tendería a pensar que efectivamente las gominolas verdes causan acné, cuando probablemente sea una coincidencia. <span class="citation"><a href="#ref-rosenthal_file_1979" role="doc-biblioref">Rosenthal</a> (<a href="#ref-rosenthal_file_1979" role="doc-biblioref">1979</a>)</span> fue de los primeros trabajos en llamar la atención respecto de esta práctica, adjudicando el concepto de <em>file drawer problem</em> (en español: problema del cajón de archivos), el que hace alusión a los resultados que se pierden o quedan “archivados” dentro de un cuerpo de literatura. Desde ese estudio en adelante varios autores han contribuido con evidencia empírica sobre el sesgo de publicación. Por ejemplo, el estudio de <span class="citation"><a href="#ref-franco_Publication_2014" role="doc-biblioref">Franco, Malhotra, y Simonovits</a> (<a href="#ref-franco_Publication_2014" role="doc-biblioref">2014</a>)</span> logra cuantificar esta situación encontrando que los resultados nulos tienen un 40% menos de probabilidades de ser publicados en revistas científicas, en comparación a estudios con resultados significativos. Es más, muchas veces los resultados nulos ni siquiera llegan a ser escritos: más de un 60% de los experimentos que componen la muestra del estudio de <span class="citation"><a href="#ref-franco_Publication_2014" role="doc-biblioref">Franco, Malhotra, y Simonovits</a> (<a href="#ref-franco_Publication_2014" role="doc-biblioref">2014</a>)</span> nunca llegaron a ser escritos, en contraste al menos del 10% de resultados significativos.</p>
<p>El principal problema del sesgo de publicación es que puede impactar en la credibilidad de cuerpos enteros de literatura. Por ejemplo, en economía se han hecho varios metaanálisis que han buscado estimar el sesgo de publicación en distintos cuerpos de literatura <span class="citation">(e.g. <a href="#ref-brodeur_Star_2016" role="doc-biblioref">Brodeur et al. 2016</a>; <a href="#ref-vivalt_Heterogeneous_2015" role="doc-biblioref">Vivalt 2015</a>; <a href="#ref-viscusi_Role_2014" role="doc-biblioref">Viscusi 2014</a>)</span>. Uno de los trabajos más concluyentes es el de <span class="citation"><a href="#ref-doucouliagos_Are_2013" role="doc-biblioref">Doucouliagos y Stanley</a> (<a href="#ref-doucouliagos_Are_2013" role="doc-biblioref">2013</a>)</span>, quienes efectúan un meta análisis de 87 artículos de meta análisis en economía. En este trabajo encuentran que más de la mitad de cuerpos de literatura revisados sufren de un sesgo “sustancial” o “severo”. Sí bien en economía se ha avanzado mucho en este tipo de estudios, también a partir del desarrollo de distintos métodos de detección, se ha podido diagnosticar el sesgo de publicación en importantes revistas en sociología y ciencias políticas <span class="citation">(<a href="#ref-gerber_Publication_2008" role="doc-biblioref">A. S. Gerber y Malhotra 2008</a>; <a href="#ref-gerber_Statistical_2008" role="doc-biblioref">A. Gerber y Malhotra 2008</a>)</span>.</p>
<p><strong>P-hacking</strong></p>
<p>Otra práctica bastante cuestionada es el <em>p-hacking</em>. El p-hacking suele englobar muchas de las prácticas que vimos en un inicio, especialmente las que refieren al manejo de datos: excluir datos arbitrariamente, redondear un valor p, recolectar más datos posterior a hacer pruebas de hipótesis etc. Lo que tienen todas estas prácticas en común y lo que define el p-hacking es que se da cuando el procesamiento de los datos tiene por objetivo obtener resultados significativos. Si el sesgo de publicación afecta la credibilidad de un cuerpo de literatura, el <em>p-hacking</em> afecta a la credibilidad de los artículos mismos, ya que al forzar la significancia estadística la probabilidad de que en realidad estemos frente a un falso positivo aumenta. Un trabajo que da sustento a esta idea es el de <span class="citation"><a href="#ref-simmons_FalsePositive_2011" role="doc-biblioref">Simmons, Nelson, y Simonsohn</a> (<a href="#ref-simmons_FalsePositive_2011" role="doc-biblioref">2011</a>)</span>, quienes calculan la posibilidad de obtener un falso positivo (error Tipo I) de acuerdo con el nivel de <em>grados de libertad</em> que son utilizados por parte de los investigadores. El resultado principal es que a medida que aumenta el uso de grados de libertad, la posibilidad de obtener un falso positivo aumenta progresivamente.</p>
<p>El <em>p-hacking</em> también contribuye a sesgar cuerpos enteros de literatura. Para diagnosticar esto se ha utilizado una herramienta denominada <em>p-curve</em>, la cual “describe la densidad de los <em>p-values</em> reportados en una literatura, aprovechando el hecho de que si la hipótesis nula no se rechaza (es decir, sin efecto), los p-values deben distribuirse uniformemente entre 0 y 1” <span class="citation">(<a href="#ref-christensen_Transparent_2019" role="doc-biblioref">G. S. Christensen, Freese, y Miguel 2019, 67</a>.)</span>. De esta manera, en cuerpos de literatura que no sufran de p-hacking, la distribución de valors p debería estar cargada a la izquierda (siendo precisos, asimétrica a la derecha), en cambio, si existe sesgo por p-hacking la distribución de p-values estaría cargada a la derecha (asimetría a la izquierda). <span class="citation"><a href="#ref-simonsohn_Pcurve_2014" role="doc-biblioref">Simonsohn, Nelson, y Simmons</a> (<a href="#ref-simonsohn_Pcurve_2014" role="doc-biblioref">2014</a>)</span> proponen esta herramienta y la prueban en dos muestras de artículos de la <em>Journal of Personality and Social Psychology (JPSP)</em>. Las pruebas estadísticas consistieron en confirmar que la primera muestra de artículos (que presentaban signos de p-hacking) estaba sesgada, en cambio la segunda muestra (sin indicios de p-hacking), no lo estaba. Los resultados corroboraron las hipótesis, especificamente: los artículos que presentaban solamente resultados con covariables resultaron tener una p-curve cargada a la derecha (asimétrica a la izquierda).</p>
<p><strong>HARKing</strong></p>
<p>Por último, pero no menos importante existe la práctica del <em>HARKing</em>. El nombre es una nomenclatura en lengua inglesa: <em>Hypothesizing After the Results are Known</em>, que literalmente significa establecer las hipótesis del estudio una vez que se conocen los resultados <span class="citation">(<a href="#ref-kerr_HARKing_1998" role="doc-biblioref">Kerr 1998</a>)</span>. El principal problema de esta práctica es que confunde los dos tipos de razonamiento que están a la base de la ciencia: el exploratorio y el confirmatorio. El objetivo principal del razonamiento exploratorio es plantear hipótesis a partir del análisis de los datos, en cambio, el razonamiento confirmatorio busca plantear hipótesis basado en teoría y contrastar esas hipótesis con datos empíricos. Como señala <span class="citation"><a href="#ref-nosek_preregistration_2018" role="doc-biblioref">Brian A. Nosek et al.</a> (<a href="#ref-nosek_preregistration_2018" role="doc-biblioref">2018</a>)</span>, cuando se confunden ambos tipos de análisis y se hace pasar un razonamiento exploratorio como confirmatorio se está cometiendo un sesgo inherente, ya que se está generando un razonamiento circular: se plantean hipótesis a partir del comportamiento de los datos y se confirman las hipótesis con esos mismos datos.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-abrilruiz_Manzanas_2019" class="csl-entry">
Abril Ruiz, Angel. 2019. <em><span>Manzanas podridas: Malas prácticas de investigación y ciencia descuidada</span></em>.
</div>
<div id="ref-aczel_consensusbased_2020" class="csl-entry">
Aczel, Balazs, Barnabas Szaszi, Alexandra Sarafoglou, Zoltan Kekecs, Šimon Kucharský, Daniel Benjamin, Christopher D. Chambers, et al. 2020. <span>«A Consensus-Based Transparency Checklist»</span>. <em>Nature Human Behaviour</em> 4 (1): 4-6. <a href="https://doi.org/10.1038/s41562-019-0772-6">https://doi.org/10.1038/s41562-019-0772-6</a>.
</div>
<div id="ref-agnoli_Questionable_2017" class="csl-entry">
Agnoli, Franca, Jelte M. Wicherts, Coosje L. S. Veldkamp, Paolo Albiero, y Roberto Cubelli. 2017. <span>«Questionable Research Practices among Italian Research Psychologists»</span>. <em>PLOS ONE</em> 12 (3): e0172792. <a href="https://doi.org/10.1371/journal.pone.0172792">https://doi.org/10.1371/journal.pone.0172792</a>.
</div>
<div id="ref-an_Crisis_2018" class="csl-entry">
An, Gary. 2018. <span>«The <span>Crisis</span> of <span>Reproducibility</span>, the <span>Denominator Problem</span> and the <span>Scientific Role</span> of <span>Multi</span>-Scale <span>Modeling</span>»</span>. <em>Bulletin of Mathematical Biology</em> 80 (12): 3071-80. <a href="https://doi.org/10.1007/s11538-018-0497-0">https://doi.org/10.1007/s11538-018-0497-0</a>.
</div>
<div id="ref-angell_Publish_1986" class="csl-entry">
Angell, Marcia. 1986. <span>«Publish or <span>Perish</span>: <span>A Proposal</span>»</span>. <em>Annals of Internal Medicine</em> 104 (2): 261-62. <a href="https://doi.org/10.7326/0003-4819-104-2-261">https://doi.org/10.7326/0003-4819-104-2-261</a>.
</div>
<div id="ref-anvari_replicability_2018" class="csl-entry">
Anvari, Farid, y Daniël Lakens. 2018. <span>«The Replicability Crisis and Public Trust in Psychological Science»</span>. <em>Comprehensive Results in Social Psychology</em> 3 (3): 266-86. <a href="https://doi.org/10.1080/23743603.2019.1684822">https://doi.org/10.1080/23743603.2019.1684822</a>.
</div>
<div id="ref-baker_500_2016" class="csl-entry">
Baker, Monya. 2016. <span>«1,500 Scientists Lift the Lid on Reproducibility»</span>. <em>Nature</em> 533 (7604): 452-54. <a href="https://doi.org/10.1038/533452a">https://doi.org/10.1038/533452a</a>.
</div>
<div id="ref-bergkvist_Preregistration_2020" class="csl-entry">
Bergkvist, Lars. 2020. <span>«Preregistration as a Way to Limit Questionable Research Practice in Advertising Research»</span>. <em>International Journal of Advertising</em> 39 (7): 1172-80. <a href="https://doi.org/10.1080/02650487.2020.1753441">https://doi.org/10.1080/02650487.2020.1753441</a>.
</div>
<div id="ref-breznau_Does_2021" class="csl-entry">
Breznau, Nate. 2021. <span>«Does <span>Sociology Need Open Science</span>?»</span>. <em>Societies</em> 11 (1): 9. <a href="https://doi.org/10.3390/soc11010009">https://doi.org/10.3390/soc11010009</a>.
</div>
<div id="ref-brodeur_Star_2016" class="csl-entry">
Brodeur, Abel, Mathias Lé, Marc Sangnier, y Yanos Zylberberg. 2016. <span>«Star <span>Wars</span>: <span>The Empirics Strike Back</span>»</span>. <em>American Economic Journal: Applied Economics</em> 8 (1): 1-32. <a href="https://doi.org/10.1257/app.20150044">https://doi.org/10.1257/app.20150044</a>.
</div>
<div id="ref-byington_Solutions_2017" class="csl-entry">
Byington, Eliza, y Will Felps. 2017. <span>«Solutions to the <span>Credibility Crisis</span> in <span>Management Science</span>»</span>. <em>Academy of Management Learning and Education, The</em> 16 (marzo): 142-62. <a href="https://doi.org/10.5465/amle.2015.0035">https://doi.org/10.5465/amle.2015.0035</a>.
</div>
<div id="ref-carey_Fraud_2011" class="csl-entry">
Carey, Benedict. 2011. <span>«Fraud <span>Case Seen</span> as a <span>Red Flag</span> for <span>Psychology Research</span>»</span>. <em>The New York Times</em>, noviembre.
</div>
<div id="ref-chambers_Registered_2013" class="csl-entry">
Chambers, Christopher D. 2013. <span>«Registered <span>Reports</span>: <span>A</span> New Publishing Initiative at <span>Cortex</span>»</span>. <em>Cortex</em> 49 (3): 609-10. <a href="https://doi.org/10.1016/j.cortex.2012.12.016">https://doi.org/10.1016/j.cortex.2012.12.016</a>.
</div>
<div id="ref-chin_Questionable_2021" class="csl-entry">
Chin, Jason M., Justin T. Pickett, Simine Vazire, y Alex O. Holcombe. 2021. <span>«Questionable <span>Research Practices</span> and <span>Open Science</span> in <span>Quantitative Criminology</span>»</span>. <em>Journal of Quantitative Criminology</em>. <a href="https://doi.org/10.1007/s10940-021-09525-6">https://doi.org/10.1007/s10940-021-09525-6</a>.
</div>
<div id="ref-christensen_Transparent_2019" class="csl-entry">
Christensen, Garret S., Jeremy Freese, y Edward Miguel. 2019. <em>Transparent and Reproducible Social Science Research: How to Do Open Science</em>. <span>Oakland, California</span>: <span>University of California Press</span>.
</div>
<div id="ref-doucouliagos_Are_2013" class="csl-entry">
Doucouliagos, Chris, y T. D. Stanley. 2013. <span>«Are <span>All Economic Facts Greatly Exaggerated</span>? <span>Theory Competition</span> and <span>Selectivity</span>»</span>. <em>Journal of Economic Surveys</em> 27 (2): 316-39. <a href="https://doi.org/10.1111/j.1467-6419.2011.00706.x">https://doi.org/10.1111/j.1467-6419.2011.00706.x</a>.
</div>
<div id="ref-elliott_Taxonomy_2020" class="csl-entry">
Elliott, Kevin C. 2020. <span>«A <span>Taxonomy</span> of <span>Transparency</span> in <span>Science</span>»</span>. <em>Canadian Journal of Philosophy</em>, 1-14. <a href="https://doi.org/10.1017/can.2020.21">https://doi.org/10.1017/can.2020.21</a>.
</div>
<div id="ref-fanelli_How_2009" class="csl-entry">
Fanelli, Daniele. 2009. <span>«How <span>Many Scientists Fabricate</span> and <span>Falsify Research</span>? <span>A Systematic Review</span> and <span>Meta</span>-<span>Analysis</span> of <span>Survey Data</span>»</span>. <em>PLOS ONE</em> 4 (5): e5738. <a href="https://doi.org/10.1371/journal.pone.0005738">https://doi.org/10.1371/journal.pone.0005738</a>.
</div>
<div id="ref-fanelli_Opinion_2018" class="csl-entry">
———. 2018. <span>«Opinion: <span>Is</span> Science Really Facing a Reproducibility Crisis, and Do We Need It To?»</span>. <em>Proceedings of the National Academy of Sciences</em> 115 (11): 2628-31. <a href="https://doi.org/10.1073/pnas.1708272114">https://doi.org/10.1073/pnas.1708272114</a>.
</div>
<div id="ref-fiedler_Questionable_2016" class="csl-entry">
Fiedler, Klaus, y Norbert Schwarz. 2016. <span>«Questionable <span>Research Practices Revisited</span>»</span>. <em>Social Psychological and Personality Science</em> 7 (1): 45-52. <a href="https://doi.org/10.1177/1948550615612150">https://doi.org/10.1177/1948550615612150</a>.
</div>
<div id="ref-flier_Faculty_2017" class="csl-entry">
Flier, Jeffrey. 2017. <span>«Faculty Promotion Must Assess Reproducibility»</span>. <em>Nature</em> 549 (7671): 133-33. <a href="https://doi.org/10.1038/549133a">https://doi.org/10.1038/549133a</a>.
</div>
<div id="ref-franco_Publication_2014" class="csl-entry">
Franco, A., N. Malhotra, y G. Simonovits. 2014. <span>«Publication Bias in the Social Sciences: <span>Unlocking</span> the File Drawer»</span>. <em>Science</em> 345 (6203): 1502-5. <a href="https://doi.org/10.1126/science.1255484">https://doi.org/10.1126/science.1255484</a>.
</div>
<div id="ref-franca_Reproducibility_2019" class="csl-entry">
França, Thiago F. A., y José Maria Monserrat. 2019. <span>«Reproducibility Crisis, the Scientific Method, and the Quality of Published Studies: <span>Untangling</span> the Knot»</span>. <em>Learned Publishing</em> 32 (4): 406-8. <a href="https://doi.org/10.1002/leap.1250">https://doi.org/10.1002/leap.1250</a>.
</div>
<div id="ref-fraser_Questionable_2018" class="csl-entry">
Fraser, Hannah, Tim Parker, Shinichi Nakagawa, Ashley Barnett, y Fiona Fidler. 2018. <span>«Questionable Research Practices in Ecology and Evolution»</span>. <em>PLOS ONE</em> 13 (7): e0200303. <a href="https://doi.org/10.1371/journal.pone.0200303">https://doi.org/10.1371/journal.pone.0200303</a>.
</div>
<div id="ref-gall_credibility_2017" class="csl-entry">
Gall, Thomas, John P. A. Ioannidis, y Zacharias Maniadis. 2017. <span>«The Credibility Crisis in Research: <span>Can</span> Economics Tools Help?»</span>. <em>PLOS Biology</em> 15 (4): e2001846. <a href="https://doi.org/10.1371/journal.pbio.2001846">https://doi.org/10.1371/journal.pbio.2001846</a>.
</div>
<div id="ref-gerber_Publication_2008" class="csl-entry">
Gerber, Alan S., y Neil Malhotra. 2008. <span>«Publication <span>Bias</span> in <span>Empirical Sociological Research</span>: <span>Do Arbitrary Significance Levels Distort Published Results</span>?»</span>. <em>Sociological Methods &amp; Research</em> 37 (1): 3-30. <a href="https://doi.org/10.1177/0049124108318973">https://doi.org/10.1177/0049124108318973</a>.
</div>
<div id="ref-gerber_Statistical_2008" class="csl-entry">
Gerber, Alan, y Neil Malhotra. 2008. <span>«Do <span>Statistical Reporting Standards Affect What Is Published</span>? <span>Publication Bias</span> in <span>Two Leading Political Science Journals</span>»</span>. <em>Quarterly Journal of Political Science</em> 3 (3): 313-26. <a href="https://doi.org/10.1561/100.00008024">https://doi.org/10.1561/100.00008024</a>.
</div>
<div id="ref-john_Measuring_2012" class="csl-entry">
John, Leslie K., George Loewenstein, y Drazen Prelec. 2012. <span>«Measuring the <span>Prevalence</span> of <span>Questionable Research Practices With Incentives</span> for <span>Truth Telling</span>»</span>. <em>Psychological Science</em> 23 (5): 524-32. <a href="https://doi.org/10.1177/0956797611430953">https://doi.org/10.1177/0956797611430953</a>.
</div>
<div id="ref-kerr_HARKing_1998" class="csl-entry">
Kerr, Norbert L. 1998. <span>«<span>HARKing</span>: <span>Hypothesizing After</span> the <span>Results</span> Are <span>Known</span>»</span>. <em>Personality and Social Psychology Review</em> 2 (3): 196-217. <a href="https://doi.org/10.1207/s15327957pspr0203_4">https://doi.org/10.1207/s15327957pspr0203_4</a>.
</div>
<div id="ref-klein_Practical_2018" class="csl-entry">
Klein, Olivier, Tom E. Hardwicke, Frederik Aust, Johannes Breuer, Henrik Danielsson, Alicia Hofelich Mohr, Hans IJzerman, Gustav Nilsonne, Wolf Vanpaemel, y Michael C. Frank. 2018. <span>«A <span>Practical Guide</span> for <span>Transparency</span> in <span>Psychological Science</span>»</span>. Editado por Michéle Nuijten y Simine Vazire. <em>Collabra: Psychology</em> 4 (1). <a href="https://doi.org/10.1525/collabra.158">https://doi.org/10.1525/collabra.158</a>.
</div>
<div id="ref-makel_Both_2021" class="csl-entry">
Makel, Matthew C., Jaret Hodges, Bryan G. Cook, y Jonathan A. Plucker. 2021. <span>«Both <span>Questionable</span> and <span>Open Research Practices Are Prevalent</span> in <span>Education Research</span>»</span>. <em>Educational Researcher</em>, marzo, 0013189X211001356. <a href="https://doi.org/10.3102/0013189X211001356">https://doi.org/10.3102/0013189X211001356</a>.
</div>
<div id="ref-miguel_Promoting_2014" class="csl-entry">
Miguel, E., C. Camerer, K. Casey, J. Cohen, K. M. Esterling, A. Gerber, R. Glennerster, et al. 2014. <span>«Promoting <span>Transparency</span> in <span>Social Science Research</span>»</span>. <em>Science</em> 343 (6166): 30-31. <a href="https://doi.org/10.1126/science.1245317">https://doi.org/10.1126/science.1245317</a>.
</div>
<div id="ref-nosek_preregistration_2018" class="csl-entry">
Nosek, Brian A., Charles R. Ebersole, Alexander C. DeHaven, y David T. Mellor. 2018. <span>«The Preregistration Revolution»</span>. <em>Proceedings of the National Academy of Sciences</em> 115 (11): 2600-2606. <a href="https://doi.org/10.1073/pnas.1708274114">https://doi.org/10.1073/pnas.1708274114</a>.
</div>
<div id="ref-oboyle_Chrysalis_2017" class="csl-entry">
O’Boyle, Ernest Hugh, George Christopher Banks, y Erik Gonzalez-Mulé. 2017. <span>«The <span>Chrysalis Effect</span>: <span>How Ugly Initial Results Metamorphosize Into Beautiful Articles</span>»</span>. <em>Journal of Management</em> 43 (2): 376-99. <a href="https://doi.org/10.1177/0149206314527133">https://doi.org/10.1177/0149206314527133</a>.
</div>
<div id="ref-peng_reproducibility_2015" class="csl-entry">
Peng, Roger. 2015. <span>«The Reproducibility Crisis in Science: <span>A</span> Statistical Counterattack»</span>. <em>Significance</em> 12 (3): 30-32. <a href="https://doi.org/10.1111/j.1740-9713.2015.00827.x">https://doi.org/10.1111/j.1740-9713.2015.00827.x</a>.
</div>
<div id="ref-rabelo_Questionable_2020" class="csl-entry">
Rabelo, André L. A., Jéssica E. M. Farias, Maurício M. Sarmet, Teresa C. R. Joaquim, Raquel C. Hoersting, Luiz Victorino, João G. N. Modesto, y Ronaldo Pilati. 2020. <span>«Questionable Research Practices among <span>Brazilian</span> Psychological Researchers: <span>Results</span> from a Replication Study and an International Comparison»</span>. <em>International Journal of Psychology</em> 55 (4): 674-83. <a href="https://doi.org/10.1002/ijop.12632">https://doi.org/10.1002/ijop.12632</a>.
</div>
<div id="ref-rosenthal_file_1979" class="csl-entry">
Rosenthal, Robert. 1979. <span>«The File Drawer Problem and Tolerance for Null Results.»</span>. <em>Psychological Bulletin</em> 86 (3): 638-41. <a href="https://doi.org/10.1037/0033-2909.86.3.638">https://doi.org/10.1037/0033-2909.86.3.638</a>.
</div>
<div id="ref-simmons_FalsePositive_2011" class="csl-entry">
Simmons, Joseph P., Leif D. Nelson, y Uri Simonsohn. 2011. <span>«False-<span>Positive Psychology</span>: <span>Undisclosed Flexibility</span> in <span>Data Collection</span> and <span>Analysis Allows Presenting Anything</span> as <span>Significant</span>»</span>. <em>Psychological Science</em> 22 (11): 1359-66. <a href="https://doi.org/10.1177/0956797611417632">https://doi.org/10.1177/0956797611417632</a>.
</div>
<div id="ref-simonsohn_Pcurve_2014" class="csl-entry">
Simonsohn, Uri, Leif D. Nelson, y Joseph P. Simmons. 2014. <span>«P-Curve: <span>A</span> Key to the File-Drawer»</span>. <em>Journal of Experimental Psychology: General</em> 143 (2): 534-47. <a href="https://doi.org/10.1037/a0033242">https://doi.org/10.1037/a0033242</a>.
</div>
<div id="ref-steneck_Fostering_2006" class="csl-entry">
Steneck, Nicholas H. 2006. <span>«Fostering Integrity in Research: <span>Definitions</span>, Current Knowledge, and Future Directions»</span>. <em>Science and Engineering Ethics</em> 12 (1): 53-74. <a href="https://doi.org/10.1007/PL00022268">https://doi.org/10.1007/PL00022268</a>.
</div>
<div id="ref-viscusi_Role_2014" class="csl-entry">
Viscusi, W. Kip. 2014. <span>«The <span>Role</span> of <span>Publication Selection Bias</span> in <span>Estimates</span> of the <span>Value</span> of a <span>Statistical Life</span>»</span>. Working {{Paper}} 20116. Working <span>Paper Series</span>. <span>National Bureau of Economic Research</span>. <a href="https://doi.org/10.3386/w20116">https://doi.org/10.3386/w20116</a>.
</div>
<div id="ref-vivalt_Heterogeneous_2015" class="csl-entry">
Vivalt, Eva. 2015. <span>«Heterogeneous <span>Treatment Effects</span> in <span>Impact Evaluation</span>»</span>. <em>American Economic Review</em> 105 (5): 467-70. <a href="https://doi.org/10.1257/aer.p20151015">https://doi.org/10.1257/aer.p20151015</a>.
</div>
</div>
<hr>
<center> 
  <div class="footer">
      Laboratorio de Ciencia Social Abierta <a href="https://lisa-coes.netlify.app/"> (LISA) </a> del Centro de Estudios de Conflicto y Cohesión Social (COES) <a href="https://coes.cl/"></a>
      <br>
      Para más información puedes contactárnos <a href= "lisa@coes.cl"> en nuestro correo </a>
  </div>
</center>

            </section>

          </div>
        </div>
      </div>
<a href="transparencia.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="qué-se-ha-hecho.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["transparency.pdf", "transparency.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

</body>

</html>
