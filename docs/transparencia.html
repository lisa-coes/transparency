<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 2 Transparencia | Transparencia y Reproducibilidad en Investigación Social</title>
  <meta name="description" content="Un breve manual sobre la transparencia y reproducibilidad en las ciencias sociales." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 2 Transparencia | Transparencia y Reproducibilidad en Investigación Social" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Un breve manual sobre la transparencia y reproducibilidad en las ciencias sociales." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 2 Transparencia | Transparencia y Reproducibilidad en Investigación Social" />
  
  <meta name="twitter:description" content="Un breve manual sobre la transparencia y reproducibilidad en las ciencias sociales." />
  

<meta name="author" content="Julio Iturra y Martín Venegas" />


<meta name="date" content="2021-06-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="reproducibilidad.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Transparencia</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="transparencia.html"><a href="transparencia.html"><i class="fa fa-check"></i><b>2</b> Transparencia</a>
<ul>
<li class="chapter" data-level="2.1" data-path="transparencia.html"><a href="transparencia.html#qué-es-la-transparencia-un-concepto-multidimensional"><i class="fa fa-check"></i><b>2.1</b> ¿Qué es la transparencia? Un concepto multidimensional</a></li>
<li class="chapter" data-level="2.2" data-path="transparencia.html"><a href="transparencia.html#cuál-es-el-problema-la-crisis-de-credibilidad-en-las-ciencias-sociales"><i class="fa fa-check"></i><b>2.2</b> ¿Cuál es el problema? La crisis de credibilidad en las ciencias sociales</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="transparencia.html"><a href="transparencia.html#mala-conducta-académica-ffp"><i class="fa fa-check"></i><b>2.2.1</b> Mala conducta académica (FFP)</a></li>
<li class="chapter" data-level="2.2.2" data-path="transparencia.html"><a href="transparencia.html#prácticas-cuestionables-de-investigación-qrp"><i class="fa fa-check"></i><b>2.2.2</b> Prácticas cuestionables de investigación (QRP)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="transparencia.html"><a href="transparencia.html#qué-podemos-hacer-recomendaciones-prácticas-y-experiencias"><i class="fa fa-check"></i><b>2.3</b> ¿Qué podemos hacer? Recomendaciones, prácticas y experiencias</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="transparencia.html"><a href="transparencia.html#recomendaciones"><i class="fa fa-check"></i><b>2.3.1</b> Recomendaciones</a></li>
<li class="chapter" data-level="2.3.2" data-path="transparencia.html"><a href="transparencia.html#experiencias"><i class="fa fa-check"></i><b>2.3.2</b> Experiencias</a></li>
<li class="chapter" data-level="2.3.3" data-path="transparencia.html"><a href="transparencia.html#herramientas"><i class="fa fa-check"></i><b>2.3.3</b> Herramientas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="reproducibilidad.html"><a href="reproducibilidad.html"><i class="fa fa-check"></i><b>3</b> Reproducibilidad</a></li>
<li class="chapter" data-level="4" data-path="palabras-finales.html"><a href="palabras-finales.html"><i class="fa fa-check"></i><b>4</b> Palabras finales</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Transparencia y Reproducibilidad en Investigación Social</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transparencia" class="section level1" number="2">
<h1><span class="header-section-number">Capítulo 2</span> Transparencia</h1>
<p>La presente sección tiene trabaja en torno al concepto de <strong>transparencia</strong>, basandose en los siguientes objetivos:</p>
<ul>
<li>Entregar una definición precisa de qué se entiende por transparencia</li>
<li>Exponer las razones que llevan a considerar necesaria una promoción de la transparencia</li>
<li>Describir las formas en las que se puede promover la transparencia</li>
</ul>
<p>Al final de esta sección el lector tendrá un manejo del concepto de transparencia y su relevancia para promover una ciencia social abierta.</p>
<div id="qué-es-la-transparencia-un-concepto-multidimensional" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> ¿Qué es la transparencia? Un concepto multidimensional</h2>
<p>En términos generales, la transparencia se puede pensar cómo la cualidad de algo que se puede ver a través de él (RAE). Por ejemplo, el vidrio de una ventana hace posible ver con claridad lo que está del otro lado, sin embargo, si la ventana comienza a perder su transparencia la observación se torna más difícil. Más importante aun, la perdida de claridad causa una ambigüedad en la observación dando la posibilidad de emitir conclusiones erroneas sobre el otro lado de la ventana. Lo que antes era claramente un gato, una ventana poco transparente podría llevar hacer concluir que es un perro o cualquier otro tipo de animal. Este ejemplo entrega un punto de partida para reflexionar la transparencia en las ciencias sociales: a medida que aumentamos la transparencia podemos discernir con mayor claridad si estamos ante hallazgos científicos creíbles.</p>
<p>El concepto de transparencia ha tomado protagonismo en las ciencias sociales. Cada vez son más los autores que abogan por hacer una ciencia social transparente, así como también las iniciativas que buscan promover la adopción de principios transparentes. No obstante, cabe preguntarse ¿qué es exactamente lo que se está promoviendo? o más específicamente ¿cómo se está entendiendo la transparencia en la literatura? Por ejemplo, <span class="citation"><a href="#ref-breznau_Does_2021" role="doc-biblioref">Breznau</a> (<a href="#ref-breznau_Does_2021" role="doc-biblioref">2021</a>)</span> entiende la transparencia como una forma en los investigadores pueden revelar el proceso, ideas y materiales que sustentan un argumento con tal de contribuir a una comunidad científica más ética. También, <span class="citation"><a href="#ref-aczel_consensusbased_2020" role="doc-biblioref">Aczel et al.</a> (<a href="#ref-aczel_consensusbased_2020" role="doc-biblioref">2020</a>)</span> proponen la transparencia como un principio que permite evaluar y reproducir los hallazgos científicos, asi como también sintetizar investigaciones y contribuir a la ejecución de metanálisis. Estas formulaciones ayudan a ir comprendiendo lo que se dice actualmente en la literatura sobre transparencia, pero no son una presentación exhaustiva del concepto. En esta sección presentaremos dos fuentes que ayudan a comprender la complejidad que significa el concepto de transparencia. La primera será la taxonomía propuesta por <span class="citation"><a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">Elliott</a> (<a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">2020</a>)</span>, y la segunda serán las <em>TOP Guidelines</em> (Guías para la promoción de la apertura y la transparencia) <span class="citation">(<a href="#ref-nosek_Transparency_2014" role="doc-biblioref">Brian A. Nosek et al. 2014</a>)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:taxonomy"></span>
<img src="docs/images/taxonomy.png" alt="Taxonomía de Transparencia" width="100%" />
<p class="caption">
Figura 2.1: Taxonomía de Transparencia
</p>
</div>
<p>La taxonomía de <span class="citation"><a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">Elliott</a> (<a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">2020</a>)</span> tiene por objetivo sintetizar la complejidad del concepto de transparencia y las distintas formas que puede tomar. La taxonomía se basa en cuatro grandes preguntas: ¿por qué?, ¿quienes? ¿qué? y ¿cómo? Cada una de estas preguntas tiene al menos una dimensión asociada. La primera pregunta se refiere a los propósitos por los cuales adoptar la transparencia; la segunda pregunta apunta a la audiencia que se beneficia de la adopción de transparencia; la tercera pregunta hace alusión al contenido qué es transparentado y la cuarta pregunta hace alusión cuatro distintas dimensiones sobre cómo adoptar la transparencia: actores, mecanismos, tiempo y lugar. También, se añade una dimensión relacionada a los peligros de la transparencia en investigación. Una representación gráfica puede verse en la Figura N° <a href="transparencia.html#fig:taxonomy">2.1</a>.</p>
<p>En la Figura N° <a href="transparencia.html#fig:tabtax">2.2</a> se presenta una versión detallada de las dimensiones de la taxonomía en conjunto a una lista no exhaustiva de variaciones dentro de cada dimensión. Por ejemplo, la dimensión del propósito sintetiza varios de los puntos que ya se han señalado en la literatura sobre ciencia abierta. En general, los propósitos están orientados a mejorar la credibilidad de la ciencia, ya sea en si misma (e.g. facilitación del reanálisis de resultados) o ya sea en su rol instrumental (e.g. promover el desarrollo de política pública de calidad). Otra dimensión relevante a destacar es el contenido de lo que se hace transparente, donde según <span class="citation"><a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">Elliott</a> (<a href="#ref-elliott_Taxonomy_2020" role="doc-biblioref">2020</a>)</span> las variaciones van de lo más concreto como los datos, métodos y materiales hasta algo más complejo como los juicios de valor. Más adelante en el documento trataremos especialmente la transparencia de los datos, métodos y materiales.</p>
<div class="figure" style="text-align: center"><span id="fig:tabtax"></span>
<img src="docs/images/table_tax.png" alt="Variaciones por dimensión de transparencia" width="100%" />
<p class="caption">
Figura 2.2: Variaciones por dimensión de transparencia
</p>
</div>
<p>Otro tratamiento extensivo de la idea de transparencia se da en las TOP Guidelines. Estos son principios que buscan alcanzar un formato de investigación reproducible a tracés del aumento de la transparencia en el proceso y los productos de investigación <span class="citation">(<a href="#ref-nosek_Transparency_2014" role="doc-biblioref">Brian A. Nosek et al. 2014</a>)</span>. Estos principios sirven de guía a las revistas académicas para poder adherir progresivamente al ideal de transparencia en la ciencia. Son ocho principios:</p>
<ol style="list-style-type: decimal">
<li>Citación</li>
<li>Transparencia de datos</li>
<li>Transparencia de métodos análiticos (código)</li>
<li>Transparencia de los materiales</li>
<li>Transparencia del diseño y el análisis</li>
<li>Preregistro de estudios</li>
<li>Pre registro de planes de análisis</li>
<li>Replicación</li>
</ol>
<p>A grandes rasgos, el principio de citación propone que las normas de citado deben ampliarse también a los datos y códigos, permitiendo reconocer su autoría intelectual <span class="citation">(<a href="#ref-nosek_Promoting_2015" role="doc-biblioref">B. A. Nosek et al. 2015</a>)</span>. Los principios de transparencia de datos, métodos análiticos, materiales y diseño y análisis (2 a 5) refieren a la transparencia en su forma más concreta: la apertura del proceso de investigación para su evaluación. El detalle puesto a los principios responde a la generelabizilidad que se le busca dar a los principios. Por ejemplo, un estudio observacional cuantitativo no tiene material que transparentar, pero si datos y métodos análiticos. Asi también, un estudio cualitativo quizás no tenga código que transparentar, pero si un diseño y una bitacora detalla del proceso de análisis. En el caso de los principios relacionados al pre-registro, <span class="citation"><a href="#ref-nosek_Promoting_2015" role="doc-biblioref">B. A. Nosek et al.</a> (<a href="#ref-nosek_Promoting_2015" role="doc-biblioref">2015</a>)</span> argumenta que registrar los estudios los hace más descubribles, incluso si no son públicados. Asi también, los preregistros del plan de análisis contribuyen a distinguir entre los análisis confirmatorios y explotatorios (ver <span class="citation"><a href="#ref-nosek_preregistration_2018" role="doc-biblioref">Brian A. Nosek et al.</a> (<a href="#ref-nosek_preregistration_2018" role="doc-biblioref">2018</a>)</span> para un manejo detalado del tema). Por último, el principios de replicación fomenta las oportunidades para la corrección de artículos y redirecciona la investigación en vías más prometedoras <span class="citation">(<a href="#ref-nosek_Promoting_2015" role="doc-biblioref">B. A. Nosek et al. 2015</a>)</span>.</p>
<p>Cada uno de estos principios cuenta con tres niveles, que sirven para medir el grado de inclusión de la transparencia por parte de una revista cientifica. La Figura N° <a href="transparencia.html#fig:tabtop">2.3</a> muestra cada nivel en relación el principio correspondiente. Se añade un nivel 0 que no cumple los estandares de transparencia con la finalidad de tener una comparación. Por ejemplo, para los estandares de transparencia del método de análisis (código), el nivel 1 dicta que las revistas deben solicitar la existencia del código de análisis, en cambio, el nivel 3 es más estricto en plantear que el código de análisis debe estar almacenado en un repositorio confiable y que el análisis será reproducido durante el proceso de revisión. El mismo método se puede aplicar para la el preregistro del plan de análisis. En el nivel 1 las revistas promueven el uso de preregistros, en cambio, en el nivel 3 los preregistros son obligatorios y también reconocidos. En suma, las TOP Guideliness son una iniciativa que contribuye a la apertura en la ciencia cambiando los requerimientos de las revistas.</p>
<p>Los dos ejemplos entregados (la taxonomía y las TOP Guideliness) perimiten comprender con un poco más de detalle el concepto de transparencia. Sin embargo, aun queda la interrogante del por qué. ¿Cuál es la problemática que lleva a considerar necesaria la adopción de la transparencia? En breve, la respuesta es la crisis en las ciencias, y especificamente en las ciencia sociales.</p>
<div class="figure" style="text-align: center"><span id="fig:tabtop"></span>
<img src="docs/images/table_top.png" alt="Variaciones por dimensión de transparencia" width="100%" />
<p class="caption">
Figura 2.3: Variaciones por dimensión de transparencia
</p>
</div>
</div>
<div id="cuál-es-el-problema-la-crisis-de-credibilidad-en-las-ciencias-sociales" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> ¿Cuál es el problema? La crisis de credibilidad en las ciencias sociales</h2>
<p>En los últimos años, ha venido tomando fuerza la idea de que existe una crisis en la ciencia. Se han utilizado distintos nombres para aludir a esta idea. Algunos han hablado de una crisis de reproducibilidad <span class="citation">(e.g. <a href="#ref-baker_500_2016" role="doc-biblioref">Baker 2016</a>)</span>, siendo el principal problema que el formato actual de publicación cientifica no permite reproducir los hallazgos. Otros usan el término de crisis de “replicabilidad” <span class="citation">(e.g. <a href="#ref-swiatkowski_Replicability_2017" role="doc-biblioref">Świątkowski and Dompnier 2017</a>)</span>, donde se argumenta que el principal problema es la falta de replicaciones de estudios científicos. También se ha usado el término de crisis de credibilidad <span class="citation">(e.g. <a href="#ref-bergh_there_2017" role="doc-biblioref">Bergh et al. 2017</a>; <a href="#ref-carrier_Facing_2017" role="doc-biblioref">Carrier 2017</a>)</span>, aludiendo a que la evidencia científica ha ido perdiendo su carácter de fiable o veraz, ya sea por prácticas poco éticas, como también por acumulación de errores en los procesos inevstigativos. Sea el concepto que se use, la narrativa instalada es que existe una crisis y que hay que encaminar los esfuerzos en abordarla.</p>
<p>Una de las fuentes más comunes para dar cuenta de la existencia de una crisis de reproducibilidad es una encuesta online realizada por la revista <em>Nature</em>. En esta encuesta, <span class="citation"><a href="#ref-baker_500_2016" role="doc-biblioref">Baker</a> (<a href="#ref-baker_500_2016" role="doc-biblioref">2016</a>)</span> logró obtener las opiniones de 1,576 investigadores de discplinas de las ciencias naturales sobre tópicos relacionados a la reproducibilidad en las ciencias, tales como sus factores y soluciones. El resultado principal muestra que un 52% de los investigadores encuestados creen que hay una crisis significativa, y un 38% creen que hay una ligera crisis. Es decir, un <strong>90% de los encuestados está de acuerdo en la existencia una crisis</strong>. Cuándo se les pregunta por los factores que contribuyen a esta crisis, un 60% de los investigadores están de acuerdo en que la cultura del <em>pública o perece</em> y el reporte selectivo de resultados siempre o casi siempre contribuyen la crisis. Si bien la encuesta no es una muestra representativa de toda la comunidad cientifica, presenta una panoramica que lleva a, por lo menos, considerar la crisis de la ciencia como tema a investigar.</p>
<p>Actualmente, existe un cuerpo de literatura que se ha dedicado a debatir e intentar demostrar la existencia de una crisis de credibilidad en las ciencias -y en las ciencias sociales-. Esta literatura se ha centrado en buscar los factores que contribuirían a la crisis, así como también en desarrollar distintas herramientas, métodos y prácticas para revertirla. En esta sección señalaremos los factores que con mayor frecuencia se señalan en la literatura. Para esquematizar de mejor manera la presentación de estos factores y la evidencia existente, es que utilizaremos el esquema conceptual de <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span>. El esquema parte de una distinción básica entre la <em>ética en investigación</em> y la <em>integridad en investigación</em>, englobando ambas bajo el gran concepto de <em>Conducta Responsable de Investigación (RCR)</em> (ver Figura N° <a href="transparencia.html#fig:rcr">2.4</a>. A grandes rasgos, la RCR se puede entender como el “llevar a cabo la investigación de forma que se cumplan las responsabilidades profesionales de los investigadores, tal y como las definen sus organizaciones profesionales, las instituciones para las que trabajan y, en su caso, el gobierno y el público” <span class="citation">(<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck 2006</a>)</span><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<div class="figure" style="text-align: center"><span id="fig:rcr"></span>
<img src="docs/images/rcr.png" alt="Conducta Responsable de Investigación. Imagen de @abrilruiz_Manzanas_2019 basada en @steneck_Fostering_2006" width="100%" />
<p class="caption">
Figura 2.4: Conducta Responsable de Investigación. Imagen de <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> basada en <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span>
</p>
</div>
<p>Dentro de este concepto, la ética de investigación está relacionada al comportamiento académico visto desde la óptica de los principios morales <span class="citation">(<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck 2006</a>)</span>, lo que se expresa en tópicos sobre el uso de datos, los consentimientos informados, el trato con pacientes en el caso de las ciencias biomedicas, por dar algunos ejemplos. La definición que ofrece <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span> señala que la ética de investigación se define como “el estudio crítico de los problemas morales asociados o que surgen en el curso de la investigación” (p.56) En cámbio, la integridad en investiación se entiende como “poseer y adherirse firmemente a las normas profesionales, tal y como las señalan las organizaciones profesionales, las instituciones de investigación y, en su caso, el gobierno y el público” <span class="citation">(<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck 2006, 56</a>)</span>. A diferencia de la ética de investigación, el concepto de identidad está regido por los estándares profesionales más que por los principios morales, su función espantear una guía clara para la conducta inevestigativa, de ahí que sea el concepto utilizado por distintos códigos de conducta.</p>
<div class="figure" style="text-align: center"><span id="fig:grad"></span>
<img src="docs/images/grad.png" alt="Gradación del comportamiento integro en investigación. Imagen de @abrilruiz_Manzanas_2019 basada en @steneck_Fostering_2006" width="100%" />
<p class="caption">
Figura 2.5: Gradación del comportamiento integro en investigación. Imagen de <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> basada en <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span>
</p>
</div>
<p>Habiendonos situando dentro del concepto de integridad en la investigación, podemos pasar a delinear las principales prácticas que atentan contra él y que se han propuesto como factores que contribuyen a la crisis en la ciencia. Tanto <span class="citation"><a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck</a> (<a href="#ref-steneck_Fostering_2006" role="doc-biblioref">2006</a>)</span> como distintos códigos de conducta de universidades e instituciones de financiamiento [] plantean una gradación de prácticas de acuerdo en qué tanto atentan contra la integridad en investigación. La Figura N° <a href="transparencia.html#fig:grad">2.5</a> esquematiza esta idea mostrando dos extremos, donde a la izquierda está el mejor comportamiento (RCR), y a la derechaa el peor comportamiento (FFP). Las FPP son un abreviación en lengua inglésa para referirse a <em>Fabrication, Falsification, Plagiarism</em> (Invención, Falsificación y Plagio). Las tres prácticas que componen la abreviación también se conocen como <em>mala conducta académica</em>-. En el medio de la gradación están las <em>prácticas cuestionables de investigación</em> (QRP, por sus siglas en inglés) las cuáles refieren a “acciones que violan los valores tradicionales de la empresa de investigación y que pueden ser perjudiciales para el proceso de investigación” <span class="citation">(<em>National Academies of Science</em> 1992 en <a href="#ref-steneck_Fostering_2006" role="doc-biblioref">Steneck 2006, 58</a>)</span>. A diferencia de las FFP, las QRP son prácticas que tienen el potencial de dañar la ciencia y no que la dañan directamente (como las FFP).</p>
<p>En base a estas distinciones, presentaremos algunas situaciones y evidencia que dan cuenta de la FFP y las QRP.</p>
<div id="mala-conducta-académica-ffp" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Mala conducta académica (FFP)</h3>
<p>La mala conducta académica suelen ser situaciones que alcanzan gran conbertura mediatica y que no están exentas de polémicas. El libro de <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> presenta una serie de situaciones, en distintas discplinas y años, en las que investigadores han sido descubiertos cometiendo mala conducta. Las situaciones son variadas, existen casos de manipulación de imagenes [], exageración de lo registros de laboratorio, o de plano la invención de conjuntos de datos enteros. En esta sección veremos el caso de Diderik Stapel como ejemplo de malas prácticas de investigación y su relación con la transparencia en la investigación.</p>
<div id="diderik-stapel" class="section level4" number="2.2.1.1">
<h4><span class="header-section-number">2.2.1.1</span> Diderik Stapel</h4>
<p>Probablemente, el caso de Diderik Stapel sea uno de los más emblématicos y representativos de este problema. Diderik Stapel era un investigador de la <em>Tilburg University</em> que se dedicaba al campo de la psicologóa social. Su carrera se caracterizó por una trayectoria ejemplar, obtuvo su M.A en psicolgía y comunicaciones el año 1991, se doctoró en psicología social el año 1997 y trabajó como profesor asociado primero en la <em>University of Gronigen</em> (2000-2006) y dede el 2006 en la Tilburg University. Fue fundador del <em>Tilburg Institute for Behavioral Economics Research</em>, del cuál el 2010 se convirtió en decano. Así también, fue galardonado con el premio a la trayectoria académica por la <em>Social of Experimental Social Psychology</em>. En breve, Stapel era una figura de alto estatus en el mundo académico, entre 1995 y 2015 publicó aproximadamente 150 artículos en revistas cientificas, algunas de las más prestigiosas (e.g. <em>Science</em>). Sin embargo, el año 2011 se confirmó que gran parte de su trayectoria académica era una farsa.</p>
<p>Después de la verificación de su culpa ante acusaciones de malas prácticas, la carrera de Diderik Stapel acabó. Fue desvinculado de la Tilburg University, se le rebocó su título de doctorado y toda su trayectoria académica fue investigada acusiociamente. El informe final de la investigación encontró que más de 50 de sus artículos eran fraudulentos. De hecho, Stapel ocupa el tercer lugar en <em>Retraction Watch</em>, con 58 de sus artículos retractados. A efectos de este escrito, lo que más destacable es que Stapel cometió conductas de mala conducta académica durante más de 15 años. La pregunta es ¿cómo fue esto posible? ¿cómo ninguno de sus colegas, alumnos o co-autores se dio cuenta de sus malas prácticas? La respuesta breve es por la falta de transparencia durante el proceso de investigación.</p>
<p>Los artículos periodisticos que han profundizado en el caso <span class="citation">(e.g. <a href="#ref-carey_Fraud_2011" role="doc-biblioref">Carey 2011</a>)</span> han relatado parte del proceso investigativo de Stapel. Dentro de los procesos de producción y análisis de datos, Stapel se caracterizaba por hacer todo el trabajo solo y “a puertas cerradas.” Es decir, nadie más que él tenía acceso a los datos brutos, ni támpoco a la ejecución de las pruebas estadísticas. Generalmente, Stapel compartía con sus colegas y alumnos de doctorado la base de datos lista, con las pruebas estadísticas ya hechas y, claro está, con resultados significativos. Estas prácticas no causaron sospechas durante muchos años, es más, a muchos de sus estudiantes les parecía una práctica normal y eficiente. Además, con el estatus de Stapel ¿qué podría estar mal? Sin embargo, era a raíz de esta práctica que Stapel tenía la oportunidad de inventar y falsificar datos a su conveniencia. Esto explica en gran parte de su trayectoría académica llena de grandes hallazgos.</p>
<p>El caso de Stapel deja un punto base sobre la mesa: la falta de transparencia en el proceso investigativo dio cabida a la mala conducta académica. Cómo nadie más colaboraba con el procesamiento de datos, ni támpoco parecía extraño que asi fuera, las oportunidades para la falsificación de los datos estaba abierta. Ahora, esta no es necesariamente una relación de causalidad, la falta de transparencia no tiene porque terminar en conductas como fabricación o falsificación de datos. Sin embargo, tal y como lo argumentan <span class="citation">(<a href="#ref-oboyle_Chrysalis_2017" role="doc-biblioref">O’Boyle, Banks, and Gonzalez-Mulé 2017</a>)</span> si son una oportunidad para violaciones a la integridad cientifíca más sutiles, tales como las QRP.</p>
</div>
</div>
<div id="prácticas-cuestionables-de-investigación-qrp" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Prácticas cuestionables de investigación (QRP)</h3>
<p>Últimamente, el concepto de QRP ha sido bastante abordado en la literatura. Distintos autores han argumentado cómo y por qué las QRP pueden afectar a la credibilidad de la ciencia en su conjunto, y cómo la transparencia puede ser un principio que mitigue esta relación. Los aportes han ido desde opiniones, reflexiones o revisiones de literatura, hasta estudios empíricos que han buscado estimar la prevalencia de estas prácticas y su impacto. En esta sección haremos dos cosas. Primero, veremos algunas de las prácticas especificas que han sido categorizadas como QRP<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> y segundo, presentaremos la evidencia que ha buscado cuantificar o estimar estas prácticas y ver su impacto en la ciencia.</p>
<p>La primera lista de prácticas que presentamos proviene del <em>Código Europeo de Conducta para la Integridad en la Investigación</em> de la ALLEA. La lista, traducida y textual de <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span>, es la siguiente:</p>
<ol style="list-style-type: decimal">
<li><p>Manipular la autoría o denigrar el papel de otros investigadores en las publicaciones.</p></li>
<li><p>Volver a publicar partes sustanciales de publicaciones propias anteriores, incluidas las traducciones, sin reconocer o citar debidamente el original (“autoplagio”).</p></li>
<li><p>Citar de forma selectiva para mejorar los propios resultados o para complacer a los editores, los revisores o los colegas.</p></li>
<li><p>Retener resultados de la investigación.</p></li>
<li><p>Permitir que los patrocinadores pongan en peligro la independencia en el proceso de investigación o en la presentación de resultados con el fin de introducir sesgos.</p></li>
<li><p>Ampliar de manera innecesaria la bibliografía de un estudio.</p></li>
<li><p>Acusar a un investigador de conducta indebida u otras infracciones de forma maliciosa.</p></li>
<li><p>Tergiversar los logros de la investigación.</p></li>
<li><p>Exagerar la importancia y la relevancia práctica de los resultados.</p></li>
<li><p>Retrasar u obstaculizar inadecuadamente el trabajo de otros investigadores.</p></li>
<li><p>Emplear la experiencia profesional propia para alentar a que se incumpla la integridad de la investigación.</p></li>
<li><p>Ignorar supuestos incumplimientos de la integridad de la investigación cometidos por terceros o encubrir reacciones inadecuadas a conductas indebidas u otro tipo de incumplimientos por parte de las instituciones</p></li>
<li><p>Establecer publicaciones o brindar apoyo a publicaciones que no cumplen el proceso de control de calidad de la investigación (“publicaciones abusivas“)</p></li>
</ol>
<p>Otro trabajo que presenta una lista sistematizada de QRP es el de <span class="citation"><a href="#ref-martinson_Scientists_2005" role="doc-biblioref">Martinson, Anderson, and de Vries</a> (<a href="#ref-martinson_Scientists_2005" role="doc-biblioref">2005</a>)</span>. La lista, textual y traducida de <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> es la siguiente (incluye algunas FFP):</p>
<ol style="list-style-type: decimal">
<li><p>Falsificar o (“cocinar”) datos de investigación.</p></li>
<li><p>Ignorar los aspectos importantes de los requerimientos de las personas participantes.</p></li>
<li><p>No divulgar adecuadamente la participación en empresas cuyos productos se basan en la investigación de uno.</p></li>
<li><p>Relaciones con estudiantes, sujetos de investigación o clientes que pueden ser interpretadas como cuestionables.</p></li>
<li><p>Usar ideas de otros sin obtener permiso o dar crédito</p></li>
<li><p>Uso no autorizado de información confidencial en relación con la propia investigación.</p></li>
<li><p>No presentar datos que contradicen una propia investigación previa.</p></li>
<li><p>Eludir ciertos aspectos menores de los requerimientos de las personas participantes.</p></li>
<li><p>Pasar por alto el uso de datos cuestionables o de interpretaciones cuestionables que otros hacen.</p></li>
<li><p>Cambiar el diseño, la metodología o los resultados de un estudio como respuesta a la presión de una fuente de financiación.</p></li>
<li><p>Publicar los mismos datos o resultados en dos o más publicaciones.</p></li>
<li><p>Asignar inapropiadamente los crédios de autoría.</p></li>
<li><p>Retener detalles de la metodología o los resultados en artículos o propuestas</p></li>
<li><p>Usar diseños de investigación inadecuados o inapropiados.</p></li>
<li><p>Eliminar observaciones o puntos de datos de los análisis basados en la intuición de que eran inexactos.</p></li>
<li><p>Mantener registros inadecuados relacionados con los proyectos de investigación.</p></li>
</ol>
<p>Otro ejemplo es el de <span class="citation"><a href="#ref-john_Measuring_2012" role="doc-biblioref">John, Loewenstein, and Prelec</a> (<a href="#ref-john_Measuring_2012" role="doc-biblioref">2012</a>)</span>, quienes en su estudio sistematizan las prácticas en la siguiente lista -textual y traducida por <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span>-:</p>
<ol style="list-style-type: decimal">
<li><p>En un paper, no fueron reportadas todas las variables dependientes de un estudio.</p></li>
<li><p>Decidir si recopilar más datos después de ver si los resultados fueron significativos.</p></li>
<li><p>En un paper, no fueron reportadas todas las condiciones del estudio.</p></li>
<li><p>Dejar de recopilar datos antes de lo esperado porque se encontró el resultado que uno esperaba.</p></li>
<li><p>En un paper, redondear un valor p (por ejemplo, reportar que un p-value de 0,054 es menor a 0,05.</p></li>
<li><p>En un paper, selectivamente reportar estudios que (“funcionaron”).</p></li>
<li><p>En un paper, decidir si excluir datos después de analizar el impacto en los resultados.</p></li>
<li><p>En un paper, reportar un hallazgo inesperado como previsto desde el principio.</p></li>
<li><p>En un documento, afirmar que los resultados no se ven afectados por variables demográficas (por ejemplo, el género), cuando uno no está realmente seguro (o sabe que lo hacen).</p></li>
<li><p>Falsificar datos</p></li>
</ol>
<p>Otra lista, aunque menos extensiva, es la de <span class="citation"><a href="#ref-oboyle_Chrysalis_2017" role="doc-biblioref">O’Boyle, Banks, and Gonzalez-Mulé</a> (<a href="#ref-oboyle_Chrysalis_2017" role="doc-biblioref">2017</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Eliminación o adición de datos después de pruebas de hipótesis.</p></li>
<li><p>Alterando los datos después de la prueba de hipótesis.</p></li>
<li><p>Supresión selectiva o adición de variables.</p></li>
<li><p>Invertir la dirección o reformular hipótesis para respaldar los datos</p></li>
<li><p>Eliminación o adición post hoc de hipótesis</p></li>
</ol>
<p>La mayoría o la totalidad de las prácticas presentadas acá han sido nombradas de distintas formas a lo largo de los años. La lista presentada por <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> hace un buen resumen de los términos:</p>
<ul>
<li><em>Intentional bias in Research</em>: sesgos que investigadores introducen en la investigación.</li>
<li><em>Fudging, massaging o cooking</em>: procesar los datos con tal de obtener resultados desados (e.g. que se cumpla una hipotésis que originalmente no se cumpliría). Este concepto suele caer dentro de las FFP.</li>
<li><em>Drylabbing</em>: reportar experimentos que nunca se realizaron</li>
<li><em>Cherry picking data</em>: reportar resultados que confirman hipotésis e ignorar los que la contradicen</li>
<li><em>Salami publication o salami slicing</em>: dividir una invesigación en pequeñas partes y publicar las partes por separado, con tal de contar con más publicaciones.</li>
<li><em>Publication bias</em>: publicar una investigación solo si el resultado es significativo.</li>
<li><em>Verification bias</em>: repetir un experimento hasta obtener resultados deseados o extraer casos con el mismo objetivo.</li>
<li><em>Data fishing o p-hacking</em>: realización de multiples pruebas estadísticas para hallar patrones significativos.</li>
<li><em>HARKing</em><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>: plantear las hipotesis una vez habiendo analizado los resultados, y reportar el proceso inverso.</li>
</ul>
<p>Una vez contamos con un paneo general sobre qué situaciones particulares caben dentro del concepto de QRP (y también de FFP), podemos pasar a revisar la evidencia en turno a su prevalencia. Una de las fuentas más citadas para una primera aproximación a la cuantificación de las QRP es el estudio de <span class="citation"><a href="#ref-john_Measuring_2012" role="doc-biblioref">John, Loewenstein, and Prelec</a> (<a href="#ref-john_Measuring_2012" role="doc-biblioref">2012</a>)</span>. En este estudio se encuestaron a más de 2000 psicologos sobre su implicación con las QRP. El procedimiento consistió en preguntar sobre las 10 malas prácticas de investigación más comunes. Las preguntas sobre cada QRP estaban divididas en tres formatos: primero, relacionados a su propio comportamiento; segundo, relacionado al comportamiento de otros psicologos y tercero, el porcentaje de psicologos que creían que admitiría haber cometido QRP. Adicionalmente, a aquellos que admitieron haber prácticado alguna QRP se les preguntó acerca de las justificaciones para hacerlo. Los resultados mostraron que un 36.6% de quienes participaron alguna vez habían práctiado alguna QRP. En detalle, analizando los porcentajes práctica a práctica se halló que el 50% de los psicologos encuestados alguna vez reportaron selectivamente estudios que apoyaran su hipotesis; un 35% alguna vez reportaron resultados inesperados como esperados; y un 2% alguna vez reportó datos falsos.</p>
<p>Otro estudio, algo más antiguo en cuanto a fecha -en comparación a <span class="citation"><a href="#ref-john_Measuring_2012" role="doc-biblioref">John, Loewenstein, and Prelec</a> (<a href="#ref-john_Measuring_2012" role="doc-biblioref">2012</a>)</span>-, pero más potente en cuanto a resultados es el de <span class="citation"><a href="#ref-fanelli_How_2009" role="doc-biblioref">Fanelli</a> (<a href="#ref-fanelli_How_2009" role="doc-biblioref">2009</a>)</span>. En este estudio se hizo un metanálisis en varios idiomas sobre artículos que incluyeran palabras relacionadas a las malas prácticas en investigación (e.g. <em>research misconduct</em>) y que hubieran sido publicados entre 1988 y 2005. De un conjunto de referencias inicial de 3276, se seleccionaron 21 artículos. El criterio de inclusión era que fueran artículos cuantitativos de encuestas que preguntaran directamente sobre malas prácticas (como el de <span class="citation"><a href="#ref-john_Measuring_2012" role="doc-biblioref">John, Loewenstein, and Prelec</a> (<a href="#ref-john_Measuring_2012" role="doc-biblioref">2012</a>)</span>). Los principales resultados son los siguientes: en promedio, un 1.97% de investigadores admite haber fabricado datos al menos una vez; un 33.7% admite haber realizado QRP como “borrar puntos de los datos basados en un sentimiento visceral.” Cuando se les pregunta por los comportamientos de otros, un 14% consiera que sus colegas caen en FFP y un 72% en QRP.</p>
<p>Un estudio que se aleja de la encuesta directa sobre prácticas, pero que contribuye estimando cuál es el potencial de que una QRP afecte los resultados de un artículo es el de <span class="citation"><a href="#ref-simmons_FalsePositive_2011" role="doc-biblioref">Simmons, Nelson, and Simonsohn</a> (<a href="#ref-simmons_FalsePositive_2011" role="doc-biblioref">2011</a>)</span>. En detalle, los autores buscan calcular el <em>likelihood</em> de obtener un falso positivo (error Tipo I) de acuerdo a al nivel de manipulación intencionada de los datos. El procedimiento consistió en calcular 15.000 muestras de 20 observaciones por condición de tratamiento para un experimento hipotetico y determinaron cuatro posibles ajustes a los datos: 1) plantear dos variables dependientes correlacionadas, 2) incrementar la muestra entre 20 y 30 casos según condición de tratamiento, 3) controlar por una variable sociodemográfica (género) o plantear una interacción genero y condición de tratamiento y 4) flexibilidad en eliminar alguna condición de tratamiento. Como se puede ver en la Figura N° <a href="transparencia.html#fig:fp">2.6</a>, el resultado principal es que a medida que aumenta la cantidad de manipulación en los datos, el likelihood de obtener un falso positivo aumenta progresivamente. Una situación en donde se realicen los cuatro ajustes planteados tiene un 60.7% de likelihood de encontrar un falso positivo.</p>
<div class="figure" style="text-align: center"><span id="fig:fp"></span>
<img src="docs/images/fp.png" alt="Likekihood de obtener un falso positivo según nivel de manipulación de datos. Imagen de @simmons_FalsePositive_2011" width="100%" />
<p class="caption">
Figura 2.6: Likekihood de obtener un falso positivo según nivel de manipulación de datos. Imagen de <span class="citation"><a href="#ref-simmons_FalsePositive_2011" role="doc-biblioref">Simmons, Nelson, and Simonsohn</a> (<a href="#ref-simmons_FalsePositive_2011" role="doc-biblioref">2011</a>)</span>
</p>
</div>
<p>Una herramienta que se ha utilizado para estimar la existencia de p-hacking en los cuerpos de literatura es la <em>p-curve</em>. La p-curve “describe la densidad de los <em>p-values</em> reportados en una literatura, aprovechando el hecho de que si la hipótesis nula fuera verdadera (es decir, sin efecto), los p-values deben distribuirse uniformemente entre 0 y 1” <span class="citation">(<a href="#ref-christensen_Transparent_2019" role="doc-biblioref">Christensen, Freese, and Miguel 2019, 67</a>.)</span>. De esta manera, en cuerpos de literatura que no sufran de p-hacking, la distribución de p-values debería ser asimetrica a la derecha, en cambio, si existe sesgo por p-hacking la distribución de p-values estaría distribuida de forma asimetrica a la izquierda. <span class="citation"><a href="#ref-simonsohn_Pcurve_2014" role="doc-biblioref">Simonsohn, Nelson, and Simmons</a> (<a href="#ref-simonsohn_Pcurve_2014" role="doc-biblioref">2014</a>)</span> proponen esta herramienta y la prueban en dos muestras de artículos de la <em>Journal of Personality and Social Psychology (JPSP)</em>. Las pruebas estadísitcas consistieron en confirmar que la primera muestra de artículos (que presentaban signos de p-hacking) estaba sesgada, en cambio la segunda muestra (sin indicios de p-hacking), no lo estaba. Los resultados corroboraron las hipotesis, en detalle, los artículos que presentaban solamente resultados con covariables, resultaron tener una p-curve asimetrica a la izquierda.</p>
<p>Además del p-hacking, en la literatura también se habla del sesgo de publicación. Como se mencionó, el sesgo de publicación se refiere al reporte selectivo de resultados significativos, dejando sin reportar los no significativos. El estudio de <span class="citation"><a href="#ref-franco_Publication_2014" role="doc-biblioref">Franco, Malhotra, and Simonovits</a> (<a href="#ref-franco_Publication_2014" role="doc-biblioref">2014</a>)</span> busca cuantificar esta situación, especificamente en ciencias sociales. En su estudio encontraron un patrón interesante: “estudios donde la hipotesis principales arrojan resultados nulos son 40% menos probables de ser publicados en una revista cientifica, en contraste a estudios que arrojen resultados significativos” <span class="citation">(<a href="#ref-christensen_Transparent_2019" role="doc-biblioref">Christensen, Freese, and Miguel 2019, 41</a>)</span>. Los descriptivos se pueden ver en la Figura N° <a href="transparencia.html#fig:written">2.7</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:written"></span>
<img src="docs/images/fp.png" alt="Porcentajes de publicación de acuerdo a significancia de resultados. Imagen de @christensen_Transparent_2019." width="100%" />
<p class="caption">
Figura 2.7: Porcentajes de publicación de acuerdo a significancia de resultados. Imagen de <span class="citation"><a href="#ref-christensen_Transparent_2019" role="doc-biblioref">Christensen, Freese, and Miguel</a> (<a href="#ref-christensen_Transparent_2019" role="doc-biblioref">2019</a>)</span>.
</p>
</div>
<p>…</p>
<blockquote>
<p>Parrafo final de la seccion que vuelva a conectar los factores con la idea de crisis</p>
</blockquote>
</div>
</div>
<div id="qué-podemos-hacer-recomendaciones-prácticas-y-experiencias" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> ¿Qué podemos hacer? Recomendaciones, prácticas y experiencias</h2>
<div id="recomendaciones" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Recomendaciones</h3>
</div>
<div id="experiencias" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Experiencias</h3>
</div>
<div id="herramientas" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Herramientas</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-abrilruiz_Manzanas_2019" class="csl-entry">
Abril Ruiz, Angel. 2019. <em><span class="nocase">Manzanas podridas: Malas prácticas de investigación y ciencia descuidada</span></em>.
</div>
<div id="ref-aczel_consensusbased_2020" class="csl-entry">
Aczel, Balazs, Barnabas Szaszi, Alexandra Sarafoglou, Zoltan Kekecs, Šimon Kucharský, Daniel Benjamin, Christopher D. Chambers, et al. 2020. <span>“A Consensus-Based Transparency Checklist.”</span> <em>Nature Human Behaviour</em> 4 (1): 4–6. <a href="https://doi.org/10.1038/s41562-019-0772-6">https://doi.org/10.1038/s41562-019-0772-6</a>.
</div>
<div id="ref-baker_500_2016" class="csl-entry">
Baker, Monya. 2016. <span>“1,500 Scientists Lift the Lid on Reproducibility.”</span> <em>Nature</em> 533 (7604): 452–54. <a href="https://doi.org/10.1038/533452a">https://doi.org/10.1038/533452a</a>.
</div>
<div id="ref-bergh_there_2017" class="csl-entry">
Bergh, Donald D, Barton M Sharp, Herman Aguinis, and Ming Li. 2017. <span>“Is There a Credibility Crisis in Strategic Management Research? <span>Evidence</span> on the Reproducibility of Study Findings.”</span> <em>Strategic Organization</em> 15 (3): 423–36. <a href="https://doi.org/10.1177/1476127017701076">https://doi.org/10.1177/1476127017701076</a>.
</div>
<div id="ref-breznau_Does_2021" class="csl-entry">
Breznau, Nate. 2021. <span>“Does <span>Sociology Need Open Science</span>?”</span> <em>Societies</em> 11 (1): 9. <a href="https://doi.org/10.3390/soc11010009">https://doi.org/10.3390/soc11010009</a>.
</div>
<div id="ref-carey_Fraud_2011" class="csl-entry">
Carey, Benedict. 2011. <span>“Fraud <span>Case Seen</span> as a <span>Red Flag</span> for <span>Psychology Research</span>.”</span> <em>The New York Times</em>, November.
</div>
<div id="ref-carrier_Facing_2017" class="csl-entry">
Carrier, Martin. 2017. <span>“Facing the <span>Credibility Crisis</span> of <span>Science</span>: <span>On</span> the <span>Ambivalent Role</span> of <span>Pluralism</span> in <span>Establishing Relevance</span> and <span>Reliability</span>.”</span> <em>Perspectives on Science</em> 25 (4): 439–64. <a href="https://doi.org/10.1162/POSC_a_00249">https://doi.org/10.1162/POSC_a_00249</a>.
</div>
<div id="ref-christensen_Transparent_2019" class="csl-entry">
Christensen, Garret S., Jeremy Freese, and Edward Miguel. 2019. <em>Transparent and Reproducible Social Science Research: How to Do Open Science</em>. <span>Oakland, California</span>: <span>University of California Press</span>.
</div>
<div id="ref-elliott_Taxonomy_2020" class="csl-entry">
Elliott, Kevin C. 2020. <span>“A <span>Taxonomy</span> of <span>Transparency</span> in <span>Science</span>.”</span> <em>Canadian Journal of Philosophy</em>, 1–14. <a href="https://doi.org/10.1017/can.2020.21">https://doi.org/10.1017/can.2020.21</a>.
</div>
<div id="ref-fanelli_How_2009" class="csl-entry">
Fanelli, Daniele. 2009. <span>“How <span>Many Scientists Fabricate</span> and <span>Falsify Research</span>? <span>A Systematic Review</span> and <span>Meta</span>-<span>Analysis</span> of <span>Survey Data</span>.”</span> <em>PLOS ONE</em> 4 (5): e5738. <a href="https://doi.org/10.1371/journal.pone.0005738">https://doi.org/10.1371/journal.pone.0005738</a>.
</div>
<div id="ref-franco_Publication_2014" class="csl-entry">
Franco, A., N. Malhotra, and G. Simonovits. 2014. <span>“Publication Bias in the Social Sciences: <span>Unlocking</span> the File Drawer.”</span> <em>Science</em> 345 (6203): 1502–5. <a href="https://doi.org/10.1126/science.1255484">https://doi.org/10.1126/science.1255484</a>.
</div>
<div id="ref-john_Measuring_2012" class="csl-entry">
John, Leslie K., George Loewenstein, and Drazen Prelec. 2012. <span>“Measuring the <span>Prevalence</span> of <span>Questionable Research Practices With Incentives</span> for <span>Truth Telling</span>.”</span> <em>Psychological Science</em> 23 (5): 524–32. <a href="https://doi.org/10.1177/0956797611430953">https://doi.org/10.1177/0956797611430953</a>.
</div>
<div id="ref-martinson_Scientists_2005" class="csl-entry">
Martinson, Brian C., Melissa S. Anderson, and Raymond de Vries. 2005. <span>“Scientists Behaving Badly.”</span> <em>Nature</em> 435 (7043): 737–38. <a href="https://doi.org/10.1038/435737a">https://doi.org/10.1038/435737a</a>.
</div>
<div id="ref-nosek_Promoting_2015" class="csl-entry">
Nosek, B. A., G. Alter, G. C. Banks, D. Borsboom, S. D. Bowman, S. J. Breckler, S. Buck, et al. 2015. <span>“Promoting an Open Research Culture.”</span> <em>Science</em> 348 (6242): 1422–25. <a href="https://doi.org/10.1126/science.aab2374">https://doi.org/10.1126/science.aab2374</a>.
</div>
<div id="ref-nosek_Transparency_2014" class="csl-entry">
Nosek, Brian A., George Alter, George Christopher Banks, Denny Borsboom, Sara Bowman, Steven Breckler, Stuart Buck, Chris Chambers, Gilbert Chin, and Garret Christensen. 2014. <span>“Transparency and <span>Openness Promotion</span> (<span>TOP</span>) <span>Guidelines</span>,”</span> August.
</div>
<div id="ref-nosek_preregistration_2018" class="csl-entry">
Nosek, Brian A., Charles R. Ebersole, Alexander C. DeHaven, and David T. Mellor. 2018. <span>“The Preregistration Revolution.”</span> <em>Proceedings of the National Academy of Sciences</em> 115 (11): 2600–2606. <a href="https://doi.org/10.1073/pnas.1708274114">https://doi.org/10.1073/pnas.1708274114</a>.
</div>
<div id="ref-oboyle_Chrysalis_2017" class="csl-entry">
O’Boyle, Ernest Hugh, George Christopher Banks, and Erik Gonzalez-Mulé. 2017. <span>“The <span>Chrysalis Effect</span>: <span>How Ugly Initial Results Metamorphosize Into Beautiful Articles</span>.”</span> <em>Journal of Management</em> 43 (2): 376–99. <a href="https://doi.org/10.1177/0149206314527133">https://doi.org/10.1177/0149206314527133</a>.
</div>
<div id="ref-simmons_FalsePositive_2011" class="csl-entry">
Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. 2011. <span>“False-<span>Positive Psychology</span>: <span>Undisclosed Flexibility</span> in <span>Data Collection</span> and <span>Analysis Allows Presenting Anything</span> as <span>Significant</span>.”</span> <em>Psychological Science</em> 22 (11): 1359–66. <a href="https://doi.org/10.1177/0956797611417632">https://doi.org/10.1177/0956797611417632</a>.
</div>
<div id="ref-simonsohn_Pcurve_2014" class="csl-entry">
Simonsohn, Uri, Leif D. Nelson, and Joseph P. Simmons. 2014. <span>“P-Curve: <span>A</span> Key to the File-Drawer.”</span> <em>Journal of Experimental Psychology: General</em> 143 (2): 534–47. <a href="https://doi.org/10.1037/a0033242">https://doi.org/10.1037/a0033242</a>.
</div>
<div id="ref-steneck_Fostering_2006" class="csl-entry">
Steneck, Nicholas H. 2006. <span>“Fostering Integrity in Research: <span>Definitions</span>, Current Knowledge, and Future Directions.”</span> <em>Science and Engineering Ethics</em> 12 (1): 53–74. <a href="https://doi.org/10.1007/PL00022268">https://doi.org/10.1007/PL00022268</a>.
</div>
<div id="ref-swiatkowski_Replicability_2017" class="csl-entry">
Świątkowski, Wojciech, and Benoît Dompnier. 2017. <span>“Replicability <span>Crisis</span> in <span>Social Psychology</span>: <span>Looking</span> at the <span>Past</span> to <span>Find New Pathways</span> for the <span>Future</span>.”</span> <em>International Review of Social Psychology</em> 30 (1): 111–24. <a href="https://doi.org/10.5334/irsp.66">https://doi.org/10.5334/irsp.66</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>A raíz de esta definición, y también en base al código de conducta detallado por XX es que <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> proponen a la siguiente definición: “la RCR es el conjunto de normas, habitualmente plasmadas en códi- gos de conducta, que pretenden ser una guía para que nuestras acciones se mantengan dentro de la integridad investigadora” (p.144).<a href="transparencia.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Muchas de las fuentes que se presentan acá son parte de la revisión que hace <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span> sobre el concepto de QRP.<a href="transparencia.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Este concepto no está en la lista original de <span class="citation"><a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">Abril Ruiz</a> (<a href="#ref-abrilruiz_Manzanas_2019" role="doc-biblioref">2019</a>)</span>, pero lo agregamos dado su reciente uso.<a href="transparencia.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="reproducibilidad.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["transparency.pdf", "transparency.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
