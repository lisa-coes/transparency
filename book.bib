
@misc{_Berkeley_,
  title = {Berkeley {{Initiative}} for {{Transparency}} in the {{Social Sciences}}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\PI94UYMY\\www.bitss.org.html},
  howpublished = {https://www.bitss.org/},
  journal = {Berkeley Initiative for Transparency in the Social Sciences},
  keywords = {iniciativa},
  language = {en-US}
}

@misc{_FORRT_,
  title = {{{FORRT}} - {{Framework}} for {{Open}} and {{Reproducible Research Training}}},
  abstract = {Integrating open and reproducible science into higher education},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\NN4DHVQ9\\forrt.org.html},
  howpublished = {https://forrt.org/},
  journal = {FORRT - Framework for Open and Reproducible Research Training},
  keywords = {iniciativa},
  language = {en-us}
}

@misc{_Gates_,
  title = {Gates {{Open Research}} | {{Open Access Publishing Platform}} | {{Beyond}} a {{Research Journal}}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\REZHKL4P\\gatesopenresearch.org.html},
  howpublished = {https://gatesopenresearch.org/},
  keywords = {iniciativa}
}

@misc{_ILDA_,
  title = {{ILDA}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\8CMS3RCH\\idatosabiertos.org.html},
  howpublished = {https://idatosabiertos.org/},
  journal = {ILDA},
  keywords = {iniciativa},
  language = {es}
}

@misc{_Open_,
  title = {Open {{Science MOOC}}},
  howpublished = {https://github.com/OpenScienceMOOC/},
  keywords = {iniciativa}
}

@misc{_OSF_,
  title = {{{OSF}}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\3MRZHMNY\\osf.io.html},
  howpublished = {https://osf.io/},
  keywords = {iniciativa}
}

@misc{_Our_,
  title = {Our Definition of Science},
  abstract = {Science is the pursuit and application of knowledge and understanding of the natural and social world following a systematic methodology based on evidence.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\KV4SA7BA\\our-definition-of-science.html},
  howpublished = {https://sciencecouncil.org/about-science/our-definition-of-science/},
  journal = {The Science Council \textasciitilde},
  language = {en-GB}
}

@article{_Practices_2013,
  title = {Practices and {{Guidelines}}},
  year = {2013},
  month = nov,
  publisher = {{OSF}},
  abstract = {Hosted on the Open Science Framework},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\SNJRI3YX\\9h47z.html},
  language = {en}
}

@book{_Sobre_,
  title = {Sobre El Almacenamiento Abierto de Datos},
  abstract = {Sobre el almacenamiento abierto de datos},
  keywords = {revisado}
}

@misc{_SocArXiv_2016,
  title = {{{SocArXiv}}},
  year = {2016},
  month = jun,
  abstract = {SocArXiv, open archive of the social sciences, provides a free, non-profit, open access platform for social scientists to upload working papers, preprints, and published papers, with the option to \ldots},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\BF972M4F\\welcome.html},
  journal = {SocOpen: Home of SocArXiv},
  keywords = {iniciativa},
  language = {en}
}

@article{aczel_consensusbased_2020,
  title = {A Consensus-Based Transparency Checklist},
  author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharsk{\'y}, {\v S}imon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munaf{\`o}, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ron{\'a}n M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and {Giner-Sorolla}, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and {de la Guardia}, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
  year = {2020},
  month = jan,
  volume = {4},
  pages = {4--6},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0772-6},
  abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
  copyright = {2019 The Author(s)},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\Y6WBN7KP\\Aczel et al. - 2020 - A consensus-based transparency checklist.pdf;C\:\\Users\\Mar\\Zotero\\storage\\3UMG68GR\\s41562-019-0772-6.html},
  journal = {Nature Human Behaviour},
  keywords = {forrt},
  language = {en},
  number = {1}
}

@article{baker_500_2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  volume = {533},
  pages = {452--454},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  copyright = {2016 Nature Publishing Group},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\X39BPFAG\\Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf;C\:\\Users\\Mar\\Zotero\\storage\\AGYJWRMU\\533452a.html},
  journal = {Nature},
  language = {en},
  number = {7604}
}

@article{benjamin-chung_Internal_2020,
  title = {Internal Replication of Computational Workflows in Scientific Research},
  author = {{Benjamin-Chung}, Jade and Colford, Jr., John M. and Mertens, Andrew and Hubbard, Alan E. and Arnold, Benjamin F.},
  year = {2020},
  month = jun,
  volume = {4},
  pages = {17},
  issn = {2572-4754},
  doi = {10.12688/gatesopenres.13108.2},
  abstract = {Failures to reproduce research findings across scientific disciplines from psychology to physics have garnered increasing attention in recent years. External replication of published findings by outside investigators has emerged as a method to detect errors and bias in the published literature. However, some studies influence policy and practice before external replication efforts can confirm or challenge the original contributions. Uncovering and resolving errors before publication would increase the efficiency of the scientific process by increasing the accuracy of published evidence. Here we summarize the rationale and best practices for internal replication, a process in which multiple independent data analysts replicate an analysis and correct errors prior to publication. We explain how internal replication should reduce errors and bias that arise during data analyses and argue that it will be most effective when coupled with pre-specified hypotheses and analysis plans and performed with data analysts masked to experimental group assignments. By improving the reproducibility of published evidence, internal replication should contribute to more rapid scientific advances.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\6BFVNSDN\\Benjamin-Chung et al. - 2020 - Internal replication of computational workflows in.pdf;C\:\\Users\\Mar\\Zotero\\storage\\KFK58R93\\v2.html},
  journal = {Gates Open Research},
  language = {en}
}

@article{bishop_Rein_2019,
  title = {Rein in the Four Horsemen of Irreproducibility},
  author = {Bishop, Dorothy},
  year = {2019},
  month = apr,
  volume = {568},
  pages = {435--435},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-01307-2},
  abstract = {Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.},
  copyright = {2021 Nature},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\ZUUPWYUA\\Bishop - 2019 - Rein in the four horsemen of irreproducibility.pdf;C\:\\Users\\Mar\\Zotero\\storage\\GUKY52XH\\d41586-019-01307-2.html},
  journal = {Nature},
  keywords = {forrt},
  language = {en},
  number = {7753}
}

@article{breznau_Does_2021,
  title = {Does {{Sociology Need Open Science}}?},
  author = {Breznau, Nate},
  year = {2021},
  month = mar,
  volume = {11},
  pages = {9},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/soc11010009},
  abstract = {Reliability, transparency, and ethical crises pushed many social science disciplines toward dramatic changes, in particular psychology and more recently political science. This paper discusses why sociology should also change. It reviews sociology as a discipline through the lens of current practices, definitions of sociology, positions of sociological associations, and a brief consideration of the arguments of three highly influential yet epistemologically diverse sociologists: Weber, Merton, and Habermas. It is a general overview for students and sociologists to quickly familiarize themselves with the state of sociology or explore the idea of open science and its relevance to their discipline.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\KKRSKRK7\\Breznau - 2021 - Does Sociology Need Open Science.pdf;C\:\\Users\\Mar\\Zotero\\storage\\VY7BN6D9\\9.html},
  journal = {Societies},
  keywords = {crisis of science,Habermas,Merton,open science,p-hacking,publication bias,replication,research ethics,revisado,science community,sociology legitimation,transparency,Weber},
  language = {en},
  number = {1}
}

@article{button_Power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  volume = {14},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\U8DZPB7H\\Button et al. - 2013 - Power failure why small sample size undermines th.pdf;C\:\\Users\\Mar\\Zotero\\storage\\JB5Z5UJX\\nrn3475.html},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {5}
}

@article{camerer_Evaluating_2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  volume = {2},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash 36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\Q3HYDUQQ\\Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf;C\:\\Users\\Mar\\Zotero\\storage\\HQ8FTARY\\s41562-018-0399-z.html},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {9}
}

@article{chambers_Registered_2013,
  title = {Registered {{Reports}}: {{A}} New Publishing Initiative at~{{Cortex}}},
  shorttitle = {Registered {{Reports}}},
  author = {Chambers, Christopher D.},
  year = {2013},
  month = mar,
  volume = {49},
  pages = {609--610},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2012.12.016},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\WTBPKU2E\\Chambers - 2013 - Registered Reports A new publishing initiative at.pdf;C\:\\Users\\Mar\\Zotero\\storage\\GNCT39GD\\S0010945212003735.html},
  journal = {Cortex},
  keywords = {forrt},
  language = {en},
  number = {3}
}

@misc{chambers_Registered_2014,
  title = {Registered {{Reports}}: {{A}} Step Change in Scientific Publishing},
  author = {Chambers, Christopher D.},
  year = {2014},
  abstract = {Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept's founders, on how the initiative combats publication bias},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\FXFGPZSL\\registered-reports-a-step-change-in-scientific-publishing.html},
  howpublished = {https://www.elsevier.com/connect/reviewers-update/registered-reports-a-step-change-in-scientific-publishing},
  journal = {Reviewers' Update},
  keywords = {forrt},
  language = {en}
}

@article{chambers_Registered_2015,
  title = {Registered Reports: Realigning Incentives in Scientific Publishing},
  shorttitle = {Registered Reports},
  author = {Chambers, Christopher D. and Dienes, Zoltan and McIntosh, Robert D. and Rotshtein, Pia and Willmes, Klaus},
  year = {2015},
  month = may,
  volume = {66},
  pages = {A1-2},
  issn = {1973-8102},
  doi = {10.1016/j.cortex.2015.03.022},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\HDIEWQ4J\\Chambers et al. - 2015 - Registered reports realigning incentives in scien.pdf},
  journal = {Cortex; a Journal Devoted to the Study of the Nervous System and Behavior},
  keywords = {Biomedical Research,Editorial Policies,forrt,Humans,Motivation,Peer Review; Research,Publication Bias,Publishing,Reproducibility of Results},
  language = {eng},
  pmid = {25892410}
}

@article{cruwell_Easy_2018,
  title = {7 {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {7 {{Easy Steps}} to {{Open Science}}},
  author = {Cr{\"u}well, Sophia and van Doorn, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse and Orben, Amy and Parsons, Sam and {Schulte-Mecklenbeck}, Michael},
  year = {2018},
  month = nov,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/cfzyx},
  abstract = {The Open Science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz et al. (2018). Written for researchers and students - particularly in psychological science - it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\84V5RPB4\\Crüwell et al. - 2018 - 7 Easy Steps to Open Science An Annotated Reading.pdf},
  keywords = {Meta-science,Meta-Science,Open Access,Open Science,other,Psychology,Reproducibility,revisado,Social and Behavioral Sciences,Transparency}
}

@article{dal-re_Making_2014,
  title = {Making {{Prospective Registration}} of {{Observational Research}} a {{Reality}}},
  author = {{Dal-R{\'e}}, Rafael and Ioannidis, John P. and Bracken, Michael B. and Buffler, Patricia A. and Chan, An-Wen and Franco, Eduardo L. and Vecchia, Carlo La and Weiderpass, Elisabete},
  year = {2014},
  month = feb,
  volume = {6},
  pages = {224cm1-224cm1},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.3007513},
  abstract = {The vast majority of health-related observational studies are not prospectively registered and the advantages of registration have not been fully appreciated. Nonetheless, international standards require approval of study protocols by an independent ethics committee before the study can begin. We suggest that there is an ethical and scientific imperative to publicly preregister key information from newly approved protocols, which should be required by funders. Ultimately, more complete information may be publicly available by disclosing protocols, analysis plans, data sets, and raw data. Key information about human observational studies should be publicly available before the study is initiated. Key information about human observational studies should be publicly available before the study is initiated.},
  chapter = {Commentary},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\SYE5TDTX\\Dal-Ré et al. - 2014 - Making Prospective Registration of Observational R.pdf;C\:\\Users\\Mar\\Zotero\\storage\\JIKBW68S\\224cm1.html},
  journal = {Science Translational Medicine},
  language = {en},
  number = {224},
  pmid = {24553383}
}

@article{elliott_Taxonomy_undefined/ed,
  title = {A {{Taxonomy}} of {{Transparency}} in {{Science}}},
  author = {Elliott, Kevin C.},
  year = {undefined/ed},
  pages = {1--14},
  publisher = {{Cambridge University Press}},
  issn = {0045-5091, 1911-0820},
  doi = {10.1017/can.2020.21},
  abstract = {Both scientists and philosophers of science have recently emphasized the importance of promoting transparency in science. For scientists, transparency is a way to promote reproducibility, progress, and trust in research. For philosophers of science, transparency can help address the value-ladenness of scientific research in a responsible way. Nevertheless, the concept of transparency is a complex one. Scientists can be transparent about many different things, for many different reasons, on behalf of many different stakeholders. This paper proposes a taxonomy that clarifies the major dimensions along which approaches to transparency can vary. By doing so, it provides several insights that philosophers and other science studies scholars can pursue. In particular, it helps address common objections to pursuing transparency in science, it clarifies major forms of transparency, and it suggests avenues for further research on this topic.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\RPG3RTLM\\90136D2E9CE7F64650D05DECCD273D08.html},
  journal = {Canadian Journal of Philosophy},
  keywords = {open science,research ethics,science communication,transparency,value judgments,values and science},
  language = {en}
}

@article{fanelli_Opinion_2018,
  title = {Opinion: {{Is}} Science Really Facing a Reproducibility Crisis, and Do We Need It To?},
  shorttitle = {Opinion},
  author = {Fanelli, Daniele},
  year = {2018},
  month = mar,
  volume = {115},
  pages = {2628--2631},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708272114},
  abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\N4RBL4CV\\Fanelli - 2018 - Opinion Is science really facing a reproducibilit.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {forrt},
  language = {en},
  number = {11}
}

@incollection{fidler_Reproducibility_2021,
  title = {Reproducibility of {{Scientific Results}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Fidler, Fiona and Wilcox, John},
  editor = {Zalta, Edward N.},
  year = {2021},
  edition = {Summer 2021},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {The terms ``reproducibility crisis'' and ``replicationcrisis'' gained currency in conversation and in print over thelast decade (e.g., Pashler \& Wagenmakers 2012), as disappointingresults emerged from large scale reproducibility projects in variousmedical, life and behavioural sciences (e.g., Open ScienceCollaboration, OSC 2015). In 2016, a poll conducted by the journalNature reported that more than half (52\%) of scientistssurveyed believed science was facing a ``replicationcrisis'' (Baker 2016). More recently, some authors have moved tomore positive terms for describing this episode in science; forexample, Vazire (2018) refers instead to a ``credibilityrevolution'' highlighting the improved methods and open sciencepractices it has motivated., The crisis often refers collectively to at least the following things:, The associated open science reform movement aims to rectify conditionsthat led to the crisis. This is done by promoting activities such asdata sharing and public pre-registration of studies, and by advocatingstricter editorial policies around statistical reporting includingpublishing replication studies and statistically non-significantresults., This review consists of four distinct parts. First, we look at theterm ``reproducibility'' and related terms like``repeatability'' and ``replication'', presentingsome definitions and conceptual discussion about the epistemicfunction of different types of replication studies. Second, wedescribe the meta-science research that has established andcharacterised the reproducibility crisis, including large scalereplication projects and surveys of questionable research practices invarious scientific communities. Third, we look at attempts to addressepistemological questions about the limitations of replication, andwhat value it holds for scientific inquiry and the accumulation ofknowledge. The fourth and final part describes some of the manyinitiatives the open science reform movement has proposed (and in manycases implemented) to improve reproducibility in science. In addition,we reflect there on the values and norms which those reforms embody,noting their relevance to the debate about the role of values in thephilosophy of science.}
}

@article{flier_Faculty_2017,
  title = {Faculty Promotion Must Assess Reproducibility},
  author = {Flier, Jeffrey},
  year = {2017},
  month = sep,
  volume = {549},
  pages = {133--133},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/549133a},
  abstract = {Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.},
  copyright = {2017 Nature Publishing Group},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\4TIVKJT5\\549133a.html},
  journal = {Nature},
  keywords = {forrt},
  language = {en},
  number = {7671}
}

@article{franco_Publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, A. and Malhotra, N. and Simonovits, G.},
  year = {2014},
  month = sep,
  volume = {345},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  journal = {Science},
  language = {en},
  number = {6203}
}

@article{gerber_Statistical_2008,
  title = {Do {{Statistical Reporting Standards Affect What Is Published}}? {{Publication Bias}} in {{Two Leading Political Science Journals}}},
  shorttitle = {Do {{Statistical Reporting Standards Affect What Is Published}}?},
  author = {Gerber, Alan and Malhotra, Neil},
  year = {2008},
  month = oct,
  volume = {3},
  pages = {313--326},
  publisher = {{Now Publishers, Inc.}},
  issn = {1554-0626, 1554-0634},
  doi = {10.1561/100.00008024},
  abstract = {Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\KYDIY976\\QJPS-8024.html},
  journal = {Quarterly Journal of Political Science},
  language = {English},
  number = {3}
}

@article{ioannidis_Why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  volume = {2},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\MW6PY4QD\\Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf},
  journal = {PLOS Medicine},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia},
  language = {en},
  number = {8}
}

@article{kapiszewski_Transparency_2021,
  title = {Transparency in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2021},
  month = apr,
  volume = {54},
  pages = {285--291},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096520000955},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096520000955/resource/name/firstPage-S1049096520000955a.jpg},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\N5T4NDCJ\\Kapiszewski and Karcher - 2021 - Transparency in Practice in Qualitative Research.pdf;C\:\\Users\\Mar\\Zotero\\storage\\LGUNFDWV\\8B780E06FBF7F0837F39B2FD33900DD1.html},
  journal = {PS: Political Science \& Politics},
  language = {en},
  number = {2}
}

@article{klein_Practical_2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and IJzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
  editor = {Nuijten, Mich{\'e}le and Vazire, Simine},
  year = {2018},
  month = jun,
  volume = {4},
  issn = {2474-7394},
  doi = {10.1525/collabra.158},
  abstract = {The credibility of scientific claims depends upon the transparency of the research products upon which they are based (e.g., study protocols, data, materials, and analysis scripts). As psychology navigates a period of unprecedented introspection, user-friendly tools and services that support open science have flourished. However, the plethora of decisions and choices involved can be bewildering. Here we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research (e.g., choosing a repository, preparing their research products for sharing, structuring folders, etc.). Being an open scientist means adopting a few straightforward research management practices, which lead to less error prone, reproducible research workflows. Further, this adoption can be piecemeal \textendash{} each incremental step towards complete transparency adds positive value. Transparent research practices not only improve the efficiency of individual researchers, they enhance the credibility of the knowledge generated by the scientific community.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\N6UN8YAF\\Klein et al. - 2018 - A Practical Guide for Transparency in Psychologica.pdf;C\:\\Users\\Mar\\Zotero\\storage\\BR6TPSEM\\A-Practical-Guide-for-Transparency-in.html},
  journal = {Collabra: Psychology},
  number = {1}
}

@techreport{lareferencia.consejodirectivo_Comunicacion_2019,
  title = {{Comunicaci\'on Acad\'emica y Acceso Abierto: Acciones para un Pol\'itica P\'ublica en Am\'erica Latina}},
  shorttitle = {{Comunicaci\'on Acad\'emica y Acceso Abierto}},
  author = {LA Referencia. Consejo Directivo},
  year = {2019},
  month = may,
  institution = {{Zenodo}},
  doi = {10.5281/ZENODO.3229410},
  abstract = {Documento redactado como insumo  para las autoridades regionales que asistieron a la reuni\'on anual del Global Research Council con acuerdo del Consejo Directivo.   La publicaci\'on y difusi\'on del mismo se realiza con el fin de favorecer el di\'alogo y la construcci\'on de una visi\'on conjunta sobre la cual se debe profundizar y actualizar a la luz de los desaf\'ios del Acceso Abierto en la regi\'on en el corto y mediano plazo. La comunicaci\'on cient\'ifica y el cambio del modelo; la situaci\'on de Am\'erica Latina; el sistema de comunicaci\'on acad\'emica de la regi\'on, principios y acciones y recomendaciones para repositorios, consorcios y revistas son los ejes tem\'aticos que se abordan a lo largo de sus p\'aginas. El art\'iculo  refuerza la premisa de que se deben tomar acciones decididas para que los resultados financiados total o parcialmente con fondos p\'ublicos est\'en en Acceso Abierto y reafirma el rol central de los organismos de CyT para lograrlo. Basado en la realidad regional, propone principios generales y acciones para los repositorios de Acceso Abierto, consorcios y revistas con una mirada m\'as sist\'emica desde las pol\'iticas p\'ublicas. Concluye con la necesidad de un di\'alogo con iniciativas como el ``Plan S'' se\~nalando los puntos de acuerdo, as\'i como diferencias, dado el contexto regional, en temas como el APC o una valorizaci\'on del rol de los repositorios. Presentado en la reuni\'on  de   COAR. 2019.  Technical and Strategic Meeting of Repository Networks. Mayo 21, 2019 - Lyon, France. Alberto Cabezas Bullemore, Secretario Ejecutivo,   LA Referencia.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\R57VSJ8N\\LA Referencia. Consejo Directivo - 2019 - Comunicación Académica y Acceso Abierto Acciones .pdf},
  keywords = {Acceso Abierto,Ciencia Abierta,Financiadores,ONCYTs,Plan S,Repositorios,revisado},
  language = {es}
}

@article{lewandowsky_Research_2016,
  title = {Research Integrity: {{Don}}'t Let Transparency Damage Science},
  shorttitle = {Research Integrity},
  author = {Lewandowsky, Stephan and Bishop, Dorothy},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {459--461},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/529459a},
  abstract = {Stephan Lewandowsky and Dorothy Bishop explain how the research community should protect its members from harassment, while encouraging the openness that has become essential to science.},
  copyright = {2016 Nature Publishing Group},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\J5AWUCXL\\Lewandowsky and Bishop - 2016 - Research integrity Don't let transparency damage .pdf;C\:\\Users\\Mar\\Zotero\\storage\\TE4JAR2W\\529459a.html},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{lindsay_Seven_2020,
  title = {Seven Steps toward Transparency and Replicability in Psychological Science.},
  author = {Lindsay, D. Stephen},
  year = {2020},
  month = nov,
  volume = {61},
  pages = {310--317},
  issn = {1878-7304, 0708-5591},
  doi = {10.1037/cap0000222},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\7T4SURP8\\Lindsay - 2020 - Seven steps toward transparency and replicability .pdf},
  journal = {Canadian Psychology/Psychologie canadienne},
  keywords = {forrt},
  language = {en},
  number = {4}
}

@article{miguel_Promoting_2014,
  title = {Promoting {{Transparency}} in {{Social Science Research}}},
  author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and der Laan, M. Van},
  year = {2014},
  month = jan,
  volume = {343},
  pages = {30--31},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1245317},
  abstract = {There is growing appreciation for the advantages of experimentation in the social sciences. Policy-relevant claims that in the past were backed by theoretical arguments and inconclusive correlations are now being investigated using more credible methods. Changes have been particularly pronounced in development economics, where hundreds of randomized trials have been carried out over the last decade. When experimentation is difficult or impossible, researchers are using quasi-experimental designs. Governments and advocacy groups display a growing appetite for evidence-based policy-making. In 2005, Mexico established an independent government agency to rigorously evaluate social programs, and in 2012, the U.S. Office of Management and Budget advised federal agencies to present evidence from randomized program evaluations in budget requests (1, 2). Social scientists should adopt higher transparency standards to improve the quality and credibility of research. Social scientists should adopt higher transparency standards to improve the quality and credibility of research.},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\MWQWX2I6\\Miguel et al. - 2014 - Promoting Transparency in Social Science Research.pdf;C\:\\Users\\Mar\\Zotero\\storage\\J9V2M2ZG\\30.html},
  journal = {Science},
  language = {en},
  number = {6166},
  pmid = {24385620}
}

@article{naturehumanbehaviour_Tell_2020,
  title = {Tell It like It Is},
  author = {{Nature human behaviour}},
  year = {2020},
  month = jan,
  volume = {4},
  pages = {1--1},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-0818-9},
  abstract = {Every research paper tells a story, but the pressure to provide `clean' narratives is harmful for the scientific endeavour.},
  copyright = {2020 Springer Nature Limited},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\D9IJKHLU\\2020 - Tell it like it is.pdf;C\:\\Users\\Mar\\Zotero\\storage\\GGNSR5XK\\s41562-020-0818-9.html},
  journal = {Nature Human Behaviour},
  keywords = {forrt},
  language = {en},
  number = {1}
}

@article{nosek_preregistration_2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  volume = {115},
  pages = {2600--2606},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes\textemdash a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  chapter = {Colloquium Paper},
  copyright = {\textcopyright{} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\MMRA278K\\Nosek et al. - 2018 - The preregistration revolution.pdf;C\:\\Users\\Mar\\Zotero\\storage\\78Y3QQ72\\2600.html},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration,revisado},
  language = {en},
  number = {11},
  pmid = {29531091}
}

@article{nosek_Promoting_2015,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1422--1425},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\6AJFDEQF\\Nosek et al. - 2015 - Promoting an open research culture.pdf;C\:\\Users\\Mar\\Zotero\\storage\\C8E9YZMI\\1422.html},
  journal = {Science},
  keywords = {revisado},
  language = {en},
  number = {6242},
  pmid = {26113702}
}

@article{nosek_Registered_2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  month = may,
  volume = {45},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\E2QGMCPK\\Nosek and Lakens - 2014 - Registered Reports A Method to Increase the Credi.pdf},
  journal = {Social Psychology},
  keywords = {forrt},
  language = {en},
  number = {3}
}

@article{oboyle_Chrysalis_2017,
  title = {The {{Chrysalis Effect}}: {{How Ugly Initial Results Metamorphosize Into Beautiful Articles}}},
  shorttitle = {The {{Chrysalis Effect}}},
  author = {O'Boyle, Ernest Hugh and Banks, George Christopher and {Gonzalez-Mul{\'e}}, Erik},
  year = {2017},
  month = feb,
  volume = {43},
  pages = {376--399},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206314527133},
  abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the ``Chrysalis Effect.''},
  journal = {Journal of Management},
  keywords = {ethics,morality and moral behavior,philosophy of science,revisado,statistical methods},
  language = {en},
  number = {2}
}

@article{peng_reproducibility_2015,
  title = {The Reproducibility Crisis in Science: {{A}} Statistical Counterattack},
  shorttitle = {The Reproducibility Crisis in Science},
  author = {Peng, Roger},
  year = {2015},
  volume = {12},
  pages = {30--32},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2015.00827.x},
  abstract = {More people have more access to data than ever before. But a comparative lack of analytical skills has resulted in scientific findings that are neither replicable nor reproducible. It is time to invest in statistics education, says Roger Peng},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00827.x},
  copyright = {\textcopyright{} 2015 The Royal Statistical Society},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\P7P2WL5Y\\Peng - 2015 - The reproducibility crisis in science A statistic.pdf;C\:\\Users\\Mar\\Zotero\\storage\\U4FSW6GX\\j.1740-9713.2015.00827.html},
  journal = {Significance},
  language = {en},
  number = {3}
}

@article{price_Problem_2020,
  title = {Problem with p Values: Why p Values Do Not Tell You If Your Treatment Is Likely to Work},
  shorttitle = {Problem with p Values},
  author = {Price, Robert and Bethune, Rob and Massey, Lisa},
  year = {2020},
  month = jan,
  volume = {96},
  pages = {1--3},
  issn = {0032-5473, 1469-0756},
  doi = {10.1136/postgradmedj-2019-137079},
  journal = {Postgraduate Medical Journal},
  keywords = {forrt},
  language = {en},
  number = {1131}
}

@article{rodriguez-sanchez_Ciencia_2016,
  title = {{Ciencia reproducible: qu\'e, por qu\'e, c\'omo}},
  shorttitle = {{Ciencia reproducible}},
  author = {{Rodriguez-Sanchez}, Francisco and {P{\'e}rez-Luque}, Antonio Jes{\'u}s and Bartomeus, Ignasi and Varela, Sara},
  year = {2016},
  month = jul,
  volume = {25},
  pages = {83--92},
  issn = {1697-2473},
  doi = {10.7818/ECOS.2016.25-2.11},
  copyright = {Derechos de autor},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\33N3BNL7\\1178.html},
  journal = {Ecosistemas},
  keywords = {reproducibilidad,revisado},
  language = {es},
  number = {2}
}

@misc{rowe_Preview_2018,
  title = {Preview My New Book: {{Introduction}} to {{Reproducible Science}} in {{R}} | {{R}}-Bloggers},
  shorttitle = {Preview My New Book},
  author = {Rowe, Brian Lee Yung},
  year = {2018},
  month = nov,
  abstract = {I'm pleased to share Part I of my new book ``Introduction to Reproducible Science in R``. The purpose of this \ldots Continue reading \textrightarrow},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\LMSG3XTX\\preview-my-new-book-introduction-to-reproducible-science-in-r.html},
  language = {en-US}
}

@misc{science_Center_,
  title = {Center for {{Open Science}}},
  author = {Science, Center for Open},
  abstract = {At the Center for Open Science, our mission is to increase openness, integrity, and reproducibility of scholarly research. Promoting these practices within the research funding and publishing communities accelerates scientific progress},
  howpublished = {https://www.cos.io},
  keywords = {iniciativa},
  language = {en}
}

@article{simmons_FalsePositive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  volume = {22},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  journal = {Psychological Science},
  language = {en},
  number = {11}
}

@article{simonsohn_Pcurve_2014,
  title = {P-Curve: {{A}} Key to the File-Drawer},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  volume = {143},
  pages = {534--547},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/a0033242},
  abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that ``work,'' readers must ask, ``Are these effects true, or do they merely reflect selective reporting?'' We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps p-curves\textemdash containing more low (.01s) than high (.04s) significant p values\textemdash only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\SC7J46YH\\2013-25331-001.html},
  journal = {Journal of Experimental Psychology: General},
  keywords = {Hypothesis Testing,Psychology,Scientific Communication,Statistics},
  number = {2}
}

@article{stodden_Trust_2011,
  title = {Trust {{Your Science}}? {{Open Your Data}} and {{Code}}},
  shorttitle = {Trust {{Your Science}}?},
  author = {Stodden, Victoria C.},
  year = {2011},
  volume = {409},
  pages = {21--22},
  doi = {10.7916/D8CJ8Q0P},
  abstract = {This is a view on the reproducibility of computational sciences by Victoria Stodden. It contains information on the Reproducibility, Replicability, and Repeatability of code created by the other sciences. Stodden also talks about the rising prominence of computational sciences as we are in the digital age and what that means for the future of science and collecting data.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\XLHSABC7\\Stodden - 2011 - Trust Your Science Open Your Data and Code.pdf;C\:\\Users\\Mar\\Zotero\\storage\\62IABAV4\\D8CJ8Q0P.html},
  keywords = {forrt},
  language = {en}
}

@article{szollosi_Preregistration_2020,
  title = {Is {{Preregistration Worthwhile}}?},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard and {van Rooij}, Iris and Van Zandt, Trisha and Donkin, Chris},
  year = {2020},
  month = feb,
  volume = {24},
  pages = {94--95},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.11.009},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\9MCLXVCZ\\Szollosi et al. - 2020 - Is Preregistration Worthwhile.pdf},
  journal = {Trends in Cognitive Sciences},
  keywords = {forrt},
  language = {en},
  number = {2}
}

@article{vankov_Article_2014,
  title = {Article {{Commentary}}: {{On}} the {{Persistence}} of {{Low Power}} in {{Psychological Science}}},
  shorttitle = {Article {{Commentary}}},
  author = {Vankov, Ivan and Bowers, Jeffrey and Munaf{\`o}, Marcus R.},
  year = {2014},
  month = may,
  volume = {67},
  pages = {1037--1040},
  publisher = {{SAGE Publications}},
  issn = {1747-0218},
  doi = {10.1080/17470218.2014.885986},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\HRWCM3XB\\Vankov et al. - 2014 - Article Commentary On the Persistence of Low Powe.pdf},
  journal = {Quarterly Journal of Experimental Psychology},
  language = {en},
  number = {5}
}


