
@misc{__,
  type = {Misc}
}

@misc{_Berkeley_,
  title = {Berkeley {{Initiative}} for {{Transparency}} in the {{Social Sciences}}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\PI94UYMY\\www.bitss.org.html},
  howpublished = {https://www.bitss.org/},
  journal = {Berkeley Initiative for Transparency in the Social Sciences},
  keywords = {iniciativa},
  language = {en-US}
}

@misc{_evolucion_,
  title = {La Evoluci\'on Del {{Open Access}} | {{Derechos Digitales}}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\VWNNDRIZ\\la-evolucion-del-open-access-en-chile.html},
  howpublished = {https://www.derechosdigitales.org/8067/la-evolucion-del-open-access-en-chile/}
}

@misc{_FORRT_,
  title = {{{FORRT}} - {{Framework}} for {{Open}} and {{Reproducible Research Training}}},
  abstract = {Integrating open and reproducible science into higher education},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\NN4DHVQ9\\forrt.org.html},
  howpublished = {https://forrt.org/},
  journal = {FORRT - Framework for Open and Reproducible Research Training},
  keywords = {iniciativa},
  language = {en-us}
}

@misc{_Gates_,
  title = {Gates {{Open Research}} | {{Open Access Publishing Platform}} | {{Beyond}} a {{Research Journal}}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\REZHKL4P\\gatesopenresearch.org.html},
  howpublished = {https://gatesopenresearch.org/},
  keywords = {iniciativa}
}

@misc{_Gates_a,
  title = {Gates {{Open Research}} | {{Open Access Publishing Platform}} | {{Beyond}} a {{Research Journal}}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\36V4C227\\gatesopenresearch.org.html},
  howpublished = {https://gatesopenresearch.org/},
  keywords = {iniciativa}
}

@misc{_ILDA_,
  title = {{ILDA}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\8CMS3RCH\\idatosabiertos.org.html},
  howpublished = {https://idatosabiertos.org/},
  journal = {ILDA},
  keywords = {iniciativa},
  language = {es}
}

@misc{_initial_,
  title = {Initial Steps toward Reproducible Research},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\GIC88MTP\\steps2rr.html},
  howpublished = {https://kbroman.org/steps2rr/},
  keywords = {iniciativa}
}

@misc{_Open_,
  title = {Open {{Science MOOC}}},
  howpublished = {https://github.com/OpenScienceMOOC/},
  keywords = {iniciativa}
}

@misc{_OSF_,
  title = {{{OSF}}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\3MRZHMNY\\osf.io.html},
  howpublished = {https://osf.io/},
  keywords = {iniciativa}
}

@misc{_Our_,
  title = {Our Definition of Science},
  abstract = {Science is the pursuit and application of knowledge and understanding of the natural and social world following a systematic methodology based on evidence.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\KV4SA7BA\\our-definition-of-science.html},
  howpublished = {https://sciencecouncil.org/about-science/our-definition-of-science/},
  journal = {The Science Council \textasciitilde},
  language = {en-GB}
}

@article{_Practices_2013,
  title = {Practices and {{Guidelines}}},
  year = {2013},
  month = nov,
  publisher = {{OSF}},
  abstract = {Hosted on the Open Science Framework},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\SNJRI3YX\\9h47z.html},
  language = {en}
}

@misc{_Project_,
  title = {Project {{TIER}} | {{Project TIER}} | {{Teaching Integrity}} in {{Empirical Research}}},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\UQLGTKM2\\www.projecttier.org.html},
  howpublished = {https://www.projecttier.org/},
  keywords = {iniciativa}
}

@book{_Sobre_,
  title = {Sobre El Almacenamiento Abierto de Datos},
  abstract = {Sobre el almacenamiento abierto de datos},
  keywords = {revisado}
}

@misc{_SocArXiv_2016,
  title = {{{SocArXiv}}},
  year = {2016},
  month = jun,
  abstract = {SocArXiv, open archive of the social sciences, provides a free, non-profit, open access platform for social scientists to upload working papers, preprints, and published papers, with the option to \ldots},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\BF972M4F\\welcome.html},
  journal = {SocOpen: Home of SocArXiv},
  keywords = {iniciativa},
  language = {en}
}

@misc{_sociologists_2015,
  title = {Sociologists Need to Be Better at Replication \textendash{} a Guest Post by Cristobal Young},
  year = {2015},
  month = aug,
  abstract = {Cristobal Young is an assistant professor at Stanford's Department of Sociology. He works on quantitative methods, stratification, and economic sociology. In this post co-authored with Aaron \ldots},
  journal = {orgtheory.net},
  language = {en}
}

@book{abrilruiz_Manzanas_2019,
  title = {{Manzanas podridas: Malas pr\'acticas de investigaci\'on y ciencia descuidada}},
  shorttitle = {{Manzanas podridas}},
  author = {Abril Ruiz, Angel},
  year = {2019},
  annotation = {OCLC: 1120499121},
  isbn = {978-1-07-075536-6},
  language = {Spanish}
}

@article{aczel_consensusbased_2020,
  title = {A Consensus-Based Transparency Checklist},
  author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharsk{\'y}, {\v S}imon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munaf{\`o}, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ron{\'a}n M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and {Giner-Sorolla}, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and {de la Guardia}, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
  year = {2020},
  month = jan,
  volume = {4},
  pages = {4--6},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0772-6},
  abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
  copyright = {2019 The Author(s)},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\Y6WBN7KP\\Aczel et al. - 2020 - A consensus-based transparency checklist.pdf;C\:\\Users\\Mar\\Zotero\\storage\\3UMG68GR\\s41562-019-0772-6.html},
  journal = {Nature Human Behaviour},
  keywords = {forrt},
  language = {en},
  number = {1}
}

@article{baker_500_2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  volume = {533},
  pages = {452--454},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  copyright = {2016 Nature Publishing Group},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\X39BPFAG\\Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf;C\:\\Users\\Mar\\Zotero\\storage\\AGYJWRMU\\533452a.html},
  journal = {Nature},
  language = {en},
  number = {7604}
}

@article{benjamin-chung_Internal_2020,
  title = {Internal Replication of Computational Workflows in Scientific Research},
  author = {{Benjamin-Chung}, Jade and Colford, Jr., John M. and Mertens, Andrew and Hubbard, Alan E. and Arnold, Benjamin F.},
  year = {2020},
  month = jun,
  volume = {4},
  pages = {17},
  issn = {2572-4754},
  doi = {10.12688/gatesopenres.13108.2},
  abstract = {Failures to reproduce research findings across scientific disciplines from psychology to physics have garnered increasing attention in recent years. External replication of published findings by outside investigators has emerged as a method to detect errors and bias in the published literature. However, some studies influence policy and practice before external replication efforts can confirm or challenge the original contributions. Uncovering and resolving errors before publication would increase the efficiency of the scientific process by increasing the accuracy of published evidence. Here we summarize the rationale and best practices for internal replication, a process in which multiple independent data analysts replicate an analysis and correct errors prior to publication. We explain how internal replication should reduce errors and bias that arise during data analyses and argue that it will be most effective when coupled with pre-specified hypotheses and analysis plans and performed with data analysts masked to experimental group assignments. By improving the reproducibility of published evidence, internal replication should contribute to more rapid scientific advances.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\6BFVNSDN\\Benjamin-Chung et al. - 2020 - Internal replication of computational workflows in.pdf;C\:\\Users\\Mar\\Zotero\\storage\\KFK58R93\\v2.html},
  journal = {Gates Open Research},
  language = {en}
}

@article{bishop_Rein_2019,
  title = {Rein in the Four Horsemen of Irreproducibility},
  author = {Bishop, Dorothy},
  year = {2019},
  month = apr,
  volume = {568},
  pages = {435--435},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-01307-2},
  abstract = {Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.},
  copyright = {2021 Nature},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\ZUUPWYUA\\Bishop - 2019 - Rein in the four horsemen of irreproducibility.pdf;C\:\\Users\\Mar\\Zotero\\storage\\GUKY52XH\\d41586-019-01307-2.html},
  journal = {Nature},
  keywords = {forrt},
  language = {en},
  number = {7753}
}

@article{bowers_How_2016,
  title = {How to Improve Your Relationship with Your Future Self},
  author = {Bowers, Jake and Voors, Maarten},
  year = {2016},
  month = dec,
  volume = {36},
  pages = {829--848},
  issn = {0718-090X},
  doi = {10.4067/S0718-090X2016000300011},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\EC5YIFBM\\Bowers and Voors - 2016 - How to improve your relationship with your future .pdf;C\:\\Users\\Mar\\Zotero\\storage\\DL4R8Y78\\scielo.html},
  journal = {Revista de ciencia pol\'itica (Santiago)},
  number = {3}
}

@article{breznau_Does_2021,
  title = {Does {{Sociology Need Open Science}}?},
  author = {Breznau, Nate},
  year = {2021},
  month = mar,
  volume = {11},
  pages = {9},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/soc11010009},
  abstract = {Reliability, transparency, and ethical crises pushed many social science disciplines toward dramatic changes, in particular psychology and more recently political science. This paper discusses why sociology should also change. It reviews sociology as a discipline through the lens of current practices, definitions of sociology, positions of sociological associations, and a brief consideration of the arguments of three highly influential yet epistemologically diverse sociologists: Weber, Merton, and Habermas. It is a general overview for students and sociologists to quickly familiarize themselves with the state of sociology or explore the idea of open science and its relevance to their discipline.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\KKRSKRK7\\Breznau - 2021 - Does Sociology Need Open Science.pdf;C\:\\Users\\Mar\\Zotero\\storage\\VY7BN6D9\\9.html},
  journal = {Societies},
  keywords = {crisis of science,Habermas,Merton,open science,p-hacking,publication bias,replication,research ethics,revisado,science community,sociology legitimation,transparency,Weber},
  language = {en},
  number = {1}
}

@misc{breznau_Open_,
  title = {Open Science in Sociology. {{What}}, Why and Now.},
  author = {Breznau, Nate},
  abstract = {WHAT By now you've heard the term ``open science''. Although it has no global definition, its advocates tend toward certain agreements. Most definitions focus on the practical aspects of accessibility. ``\ldots the practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely \ldots{} Continue reading Open science in sociology. What, why and now.},
  journal = {Crowdid},
  language = {en-US},
  type = {Billet}
}

@techreport{brodeur_Methods_2018,
  title = {Methods {{Matter}}: {{P}}-{{Hacking}} and {{Causal Inference}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = {2018},
  month = aug,
  institution = {{Institute of Labor Economics (IZA)}},
  abstract = {The economics 'credibility revolution' has promoted the identification of causal relationships using difference-in-differences (DID), instrumental variables (IV), randomized control trials (RCT) and regression discontinuity design (RDD) methods. The extent to which a reader should trust claims about the statistical significance of results proves very sensitive to method. Applying multiple methods to 13,440 hypothesis tests reported in 25 top economics journals in 2015, we show that selective publication and p-hacking is a substantial problem in research employing DID and (in particular) IV. RCT and RDD are much less problematic. Almost 25\% of claims of marginally significant results in IV papers are misleading.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\KHSHNKXQ\\Brodeur et al. - 2018 - Methods Matter P-Hacking and Causal Inference in .pdf},
  keywords = {causal inference,p-curves,p-hacking,publication bias,research methods},
  number = {11796},
  type = {{{IZA Discussion Paper}}}
}

@article{button_Power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  volume = {14},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\U8DZPB7H\\Button et al. - 2013 - Power failure why small sample size undermines th.pdf;C\:\\Users\\Mar\\Zotero\\storage\\JB5Z5UJX\\nrn3475.html},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {5}
}

@article{camerer_Evaluating_2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  volume = {2},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash 36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\Q3HYDUQQ\\Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf;C\:\\Users\\Mar\\Zotero\\storage\\HQ8FTARY\\s41562-018-0399-z.html},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {9}
}

@article{carrier_Facing_2017,
  title = {Facing the {{Credibility Crisis}} of {{Science}}: {{On}} the {{Ambivalent Role}} of {{Pluralism}} in {{Establishing Relevance}} and {{Reliability}}},
  shorttitle = {Facing the {{Credibility Crisis}} of {{Science}}},
  author = {Carrier, Martin},
  year = {2017},
  month = may,
  volume = {25},
  pages = {439--464},
  issn = {1063-6145},
  doi = {10.1162/POSC_a_00249},
  abstract = {Science at the interface with society is regarded with mistrust among parts of the public. Scientific judgments on matters of practical concern are not infrequently suspected of being incompetent and biased. I discuss two proposals for remedying this deficiency. The first aims at strengthening the independence of science and suggests increasing the distance to political and economic powers. The drawback is that this runs the risk of locking science in an academic ivory tower. The second proposal favors ``counter-politicization'' in that research is strongly focused on projects ``in the public interest,'' that is, on projects whose expected results will benefit all those concerned by these results. The disadvantage is that the future use of research findings cannot be delineated reliably in advance. I argue that the underlying problem is the perceived lack of relevance and reliability and that pluralism is an important step toward its solution. Pluralism serves to stimulate a more inclusive research agenda and strengthens the well-testedness of scientific approaches. However, pluralism also prevents the emergence of clear-cut practical suggestions. Accordingly, pluralism is part of the solution to the credibility crisis of science, but also part of the problem. In order for science to be suitable as a guide for practice, the leeway of scientific options needs to be narrowed \textendash{} in spite of uncertainty in epistemic respect. This reduction can be achieved by appeal to criteria that do not focus on the epistemic credentials of the suggestions but on their appropriateness in practical respect.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\W2QFPQUP\\Carrier - 2017 - Facing the Credibility Crisis of Science On the A.pdf},
  journal = {Perspectives on Science},
  number = {4}
}

@article{chambers_Registered_2013,
  title = {Registered {{Reports}}: {{A}} New Publishing Initiative at~{{Cortex}}},
  shorttitle = {Registered {{Reports}}},
  author = {Chambers, Christopher D.},
  year = {2013},
  month = mar,
  volume = {49},
  pages = {609--610},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2012.12.016},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\WTBPKU2E\\Chambers - 2013 - Registered Reports A new publishing initiative at.pdf;C\:\\Users\\Mar\\Zotero\\storage\\GNCT39GD\\S0010945212003735.html},
  journal = {Cortex},
  keywords = {forrt},
  language = {en},
  number = {3}
}

@misc{chambers_Registered_2014,
  title = {Registered {{Reports}}: {{A}} Step Change in Scientific Publishing},
  author = {Chambers, Christopher D.},
  year = {2014},
  abstract = {Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept's founders, on how the initiative combats publication bias},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\FXFGPZSL\\registered-reports-a-step-change-in-scientific-publishing.html},
  howpublished = {https://www.elsevier.com/connect/reviewers-update/registered-reports-a-step-change-in-scientific-publishing},
  journal = {Reviewers' Update},
  keywords = {forrt},
  language = {en}
}

@article{chambers_Registered_2015,
  title = {Registered Reports: Realigning Incentives in Scientific Publishing},
  shorttitle = {Registered Reports},
  author = {Chambers, Christopher D. and Dienes, Zoltan and McIntosh, Robert D. and Rotshtein, Pia and Willmes, Klaus},
  year = {2015},
  month = may,
  volume = {66},
  pages = {A1-2},
  issn = {1973-8102},
  doi = {10.1016/j.cortex.2015.03.022},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\HDIEWQ4J\\Chambers et al. - 2015 - Registered reports realigning incentives in scien.pdf},
  journal = {Cortex; a Journal Devoted to the Study of the Nervous System and Behavior},
  keywords = {Biomedical Research,Editorial Policies,forrt,Humans,Motivation,Peer Review; Research,Publication Bias,Publishing,Reproducibility of Results},
  language = {eng},
  pmid = {25892410}
}

@book{christensen_Transparent_2019,
  title = {Transparent and Reproducible Social Science Research: How to Do Open Science},
  shorttitle = {Transparent and Reproducible Social Science Research},
  author = {Christensen, Garret S. and Freese, Jeremy and Miguel, Edward},
  year = {2019},
  publisher = {{University of California Press}},
  address = {{Oakland, California}},
  abstract = {"Social science practitioners have recently witnessed numerous episodes of influential research that fell apart upon close scrutiny. These instances have spurred suspicions that other published results may contain errors or may at least be less robust than they appear. In response, an influential movement has emerged across the social sciences for greater research transparency, openness, and reproducibility. Transparent and Reproducible Social Science Research crystallizes the new insights, practices, and methods of this rising interdisciplinary field"--Provided by publisher},
  isbn = {978-0-520-96923-0},
  keywords = {Reproducible research,Research,Social sciences},
  lccn = {Q180.55.S7}
}

@article{chubin_Open_1985,
  title = {Open {{Science}} and {{Closed Science}}: {{Tradeoffs}} in a {{Democracy}}},
  shorttitle = {Open {{Science}} and {{Closed Science}}},
  author = {Chubin, Daryl E.},
  year = {1985},
  month = apr,
  volume = {10},
  pages = {73--80},
  publisher = {{SAGE Publications Inc}},
  issn = {0162-2439},
  doi = {10.1177/016224398501000211},
  journal = {Science, Technology, \& Human Values},
  language = {en},
  number = {2}
}

@article{cruwell_Easy_2018,
  title = {7 {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {7 {{Easy Steps}} to {{Open Science}}},
  author = {Cr{\"u}well, Sophia and van Doorn, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse and Orben, Amy and Parsons, Sam and {Schulte-Mecklenbeck}, Michael},
  year = {2018},
  month = nov,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/cfzyx},
  abstract = {The Open Science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz et al. (2018). Written for researchers and students - particularly in psychological science - it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\84V5RPB4\\Crüwell et al. - 2018 - 7 Easy Steps to Open Science An Annotated Reading.pdf},
  keywords = {Meta-science,Meta-Science,Open Access,Open Science,other,Psychology,Reproducibility,revisado,Social and Behavioral Sciences,Transparency}
}

@article{dal-re_Making_2014,
  title = {Making {{Prospective Registration}} of {{Observational Research}} a {{Reality}}},
  author = {{Dal-R{\'e}}, Rafael and Ioannidis, John P. and Bracken, Michael B. and Buffler, Patricia A. and Chan, An-Wen and Franco, Eduardo L. and Vecchia, Carlo La and Weiderpass, Elisabete},
  year = {2014},
  month = feb,
  volume = {6},
  pages = {224cm1-224cm1},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.3007513},
  abstract = {The vast majority of health-related observational studies are not prospectively registered and the advantages of registration have not been fully appreciated. Nonetheless, international standards require approval of study protocols by an independent ethics committee before the study can begin. We suggest that there is an ethical and scientific imperative to publicly preregister key information from newly approved protocols, which should be required by funders. Ultimately, more complete information may be publicly available by disclosing protocols, analysis plans, data sets, and raw data. Key information about human observational studies should be publicly available before the study is initiated. Key information about human observational studies should be publicly available before the study is initiated.},
  chapter = {Commentary},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\SYE5TDTX\\Dal-Ré et al. - 2014 - Making Prospective Registration of Observational R.pdf;C\:\\Users\\Mar\\Zotero\\storage\\JIKBW68S\\224cm1.html},
  journal = {Science Translational Medicine},
  language = {en},
  number = {224},
  pmid = {24553383}
}

@article{elliott_Taxonomy_2020,
  title = {A {{Taxonomy}} of {{Transparency}} in {{Science}}},
  author = {Elliott, Kevin C.},
  year = {2020},
  pages = {1--14},
  publisher = {{Cambridge University Press}},
  issn = {0045-5091, 1911-0820},
  doi = {10.1017/can.2020.21},
  abstract = {Both scientists and philosophers of science have recently emphasized the importance of promoting transparency in science. For scientists, transparency is a way to promote reproducibility, progress, and trust in research. For philosophers of science, transparency can help address the value-ladenness of scientific research in a responsible way. Nevertheless, the concept of transparency is a complex one. Scientists can be transparent about many different things, for many different reasons, on behalf of many different stakeholders. This paper proposes a taxonomy that clarifies the major dimensions along which approaches to transparency can vary. By doing so, it provides several insights that philosophers and other science studies scholars can pursue. In particular, it helps address common objections to pursuing transparency in science, it clarifies major forms of transparency, and it suggests avenues for further research on this topic.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\RPG3RTLM\\90136D2E9CE7F64650D05DECCD273D08.html},
  journal = {Canadian Journal of Philosophy},
  keywords = {open science,research ethics,science communication,transparency,value judgments,values and science},
  language = {en}
}

@article{engzell_Improving_2020,
  title = {Improving {{Social Science}}: {{Lessons}} from the {{Open Science Movement}}},
  shorttitle = {Improving {{Social Science}}},
  author = {Engzell, Per and Rohrer, Julia M.},
  year = {2020},
  month = apr,
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/6whjt},
  abstract = {The transdisciplinary movement towards greater research transparency opens the door for a meta-scientific exchange between different social sciences. In the spirit of such an exchange, we offer some lessons inspired by ongoing debates in psychology, highlighting the broad benefits of open science but also potential pitfalls, as well as practical challenges in the implementation that have not yet been fully resolved. Our discussion is aimed towards political scientists but relevant for population sciences more broadly.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\4J4R8QGD\\Engzell and Rohrer - 2020 - Improving Social Science Lessons from the Open Sc.pdf},
  keywords = {credibility,meta-science,open science,replication,reproducibility,Social and Behavioral Sciences,transparency}
}

@article{fanelli_Opinion_2018,
  title = {Opinion: {{Is}} Science Really Facing a Reproducibility Crisis, and Do We Need It To?},
  shorttitle = {Opinion},
  author = {Fanelli, Daniele},
  year = {2018},
  month = mar,
  volume = {115},
  pages = {2628--2631},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708272114},
  abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\N4RBL4CV\\Fanelli - 2018 - Opinion Is science really facing a reproducibilit.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {forrt},
  language = {en},
  number = {11}
}

@incollection{fidler_Reproducibility_2021,
  title = {Reproducibility of {{Scientific Results}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Fidler, Fiona and Wilcox, John},
  editor = {Zalta, Edward N.},
  year = {2021},
  edition = {Summer 2021},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {The terms ``reproducibility crisis'' and ``replicationcrisis'' gained currency in conversation and in print over thelast decade (e.g., Pashler \& Wagenmakers 2012), as disappointingresults emerged from large scale reproducibility projects in variousmedical, life and behavioural sciences (e.g., Open ScienceCollaboration, OSC 2015). In 2016, a poll conducted by the journalNature reported that more than half (52\%) of scientistssurveyed believed science was facing a ``replicationcrisis'' (Baker 2016). More recently, some authors have moved tomore positive terms for describing this episode in science; forexample, Vazire (2018) refers instead to a ``credibilityrevolution'' highlighting the improved methods and open sciencepractices it has motivated., The crisis often refers collectively to at least the following things:, The associated open science reform movement aims to rectify conditionsthat led to the crisis. This is done by promoting activities such asdata sharing and public pre-registration of studies, and by advocatingstricter editorial policies around statistical reporting includingpublishing replication studies and statistically non-significantresults., This review consists of four distinct parts. First, we look at theterm ``reproducibility'' and related terms like``repeatability'' and ``replication'', presentingsome definitions and conceptual discussion about the epistemicfunction of different types of replication studies. Second, wedescribe the meta-science research that has established andcharacterised the reproducibility crisis, including large scalereplication projects and surveys of questionable research practices invarious scientific communities. Third, we look at attempts to addressepistemological questions about the limitations of replication, andwhat value it holds for scientific inquiry and the accumulation ofknowledge. The fourth and final part describes some of the manyinitiatives the open science reform movement has proposed (and in manycases implemented) to improve reproducibility in science. In addition,we reflect there on the values and norms which those reforms embody,noting their relevance to the debate about the role of values in thephilosophy of science.}
}

@article{flier_Faculty_2017,
  title = {Faculty Promotion Must Assess Reproducibility},
  author = {Flier, Jeffrey},
  year = {2017},
  month = sep,
  volume = {549},
  pages = {133--133},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/549133a},
  abstract = {Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.},
  copyright = {2017 Nature Publishing Group},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\4TIVKJT5\\549133a.html},
  journal = {Nature},
  keywords = {forrt},
  language = {en},
  number = {7671}
}

@article{franco_Publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, A. and Malhotra, N. and Simonovits, G.},
  year = {2014},
  month = sep,
  volume = {345},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  journal = {Science},
  language = {en},
  number = {6203}
}

@article{freese_Replication_2017,
  title = {Replication in {{Social Science}}},
  author = {Freese, Jeremy and Peterson, David},
  year = {2017},
  month = jul,
  volume = {43},
  pages = {147--165},
  publisher = {{Annual Reviews}},
  issn = {0360-0572},
  doi = {10.1146/annurev-soc-060116-053450},
  abstract = {Across the medical and social sciences, new discussions about replication have led to transformations in research practice. Sociologists, however, have been largely absent from these discussions. The goals of this review are to introduce sociologists to these developments, synthesize insights from science studies about replication in general, and detail the specific issues regarding replication that occur in sociology. The first half of the article argues that a sociologically sophisticated understanding of replication must address both the ways that replication rules and conventions evolved within an epistemic culture and how those cultures are shaped by specific research challenges. The second half outlines the four main dimensions of replicability in quantitative sociology\textemdash verifiability, robustness, repeatability, and generalizability\textemdash and discusses the specific ambiguities of interpretation that can arise in each. We conclude by advocating some commonsense changes to promote replication while acknowledging the epistemic diversity of our field.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\LE6LM93T\\Freese and Peterson - 2017 - Replication in Social Science.pdf},
  journal = {Annual Review of Sociology},
  number = {1}
}

@article{frey_Publishing_2003,
  title = {Publishing as {{Prostitution}}? \textendash{} {{Choosing Between One}}'s {{Own Ideas}} and {{Academic Success}}},
  shorttitle = {Publishing as {{Prostitution}}?},
  author = {Frey, Bruno S.},
  year = {2003},
  month = jul,
  volume = {116},
  pages = {205--223},
  issn = {1573-7101},
  doi = {10.1023/A:1024208701874},
  abstract = {Survival in academia depends on publications in refereedjournals. Authors only get their papers accepted if theyintellectually prostitute themselves by slavishly followingthe demands made by anonymous referees who have no propertyrights to the journals they advise. Intellectual prostitutionis neither beneficial to suppliers nor consumers. But it isavoidable. The editor (with property rights to the journal)should make the basic decision of whether a paper is worthpublishing or not. The referees should only offer suggestionsfor improvement. The author may disregard this advice. Thisreduces intellectual prostitution and produces more originalpublications.},
  journal = {Public Choice},
  language = {en},
  number = {1}
}

@article{gerber_Publication_2008,
  title = {Publication {{Bias}} in {{Empirical Sociological Research}}: {{Do Arbitrary Significance Levels Distort Published Results}}?},
  shorttitle = {Publication {{Bias}} in {{Empirical Sociological Research}}},
  author = {Gerber, Alan S. and Malhotra, Neil},
  year = {2008},
  month = aug,
  volume = {37},
  pages = {3--30},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124108318973},
  abstract = {Despite great attention to the quality of research methods in individual studies, if publication decisions of journals are a function of the statistical significance of research findings, the published literature as a whole may not produce accurate measures of true effects. This article examines the two most prominent sociology journals (the American Sociological Review and the American Journal of Sociology) and another important though less influential journal (The Sociological Quarterly) for evidence of publication bias. The effect of the .05 significance level on the pattern of published findings is examined using a ``caliper'' test, and the hypothesis of no publication bias can be rejected at approximately the 1 in 10 million level. Findings suggest that some of the results reported in leading sociology journals may be misleading and inaccurate due to publication bias. Some reasons for publication bias and proposed reforms to reduce its impact on research are also discussed.},
  journal = {Sociological Methods \& Research},
  keywords = {caliper test,hypothesis testing,meta-analysis,publication bias},
  language = {en},
  number = {1}
}

@article{gerber_Statistical_2008,
  title = {Do {{Statistical Reporting Standards Affect What Is Published}}? {{Publication Bias}} in {{Two Leading Political Science Journals}}},
  shorttitle = {Do {{Statistical Reporting Standards Affect What Is Published}}?},
  author = {Gerber, Alan and Malhotra, Neil},
  year = {2008},
  month = oct,
  volume = {3},
  pages = {313--326},
  publisher = {{Now Publishers, Inc.}},
  issn = {1554-0626, 1554-0634},
  doi = {10.1561/100.00008024},
  abstract = {Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\KYDIY976\\QJPS-8024.html},
  journal = {Quarterly Journal of Political Science},
  language = {English},
  number = {3}
}

@article{haven_Preregistering_2019,
  title = {Preregistering Qualitative Research},
  author = {Haven, Tamarinde L. and Grootel, Dr Leonie Van},
  year = {2019},
  month = apr,
  volume = {26},
  pages = {229--244},
  publisher = {{Taylor \& Francis}},
  issn = {0898-9621},
  doi = {10.1080/08989621.2019.1580147},
  abstract = {The threat to reproducibility and awareness of current rates of research misbehavior sparked initiatives to better academic science. One initiative is preregistration of quantitative research. We investigate whether the preregistration format could also be used to boost the credibility of qualitative research. A crucial distinction underlying preregistration is that between prediction and postdiction. In qualitative research, data are used to decide which way interpretation should move forward, using data to generate hypotheses and new research questions. Qualitative research is thus a real-life example of postdiction research. Some may object to the idea of preregistering qualitative studies because qualitative research generally does not test hypotheses, and because qualitative research design is typically flexible and subjective. We rebut these objections, arguing that making hypotheses explicit is just one feature of preregistration, that flexibility can be tracked using preregistration, and that preregistration would provide a check on subjectivity. We then contextualize preregistrations alongside another initiative to enhance credibility in qualitative research: the confirmability audit. Besides, preregistering qualitative studies is practically useful to combating dissemination bias and could incentivize qualitative researchers to report constantly on their study's development. We conclude with suggested modifications to the Open Science Framework preregistration form to tailor it for qualitative studies.},
  annotation = {\_eprint: https://doi.org/10.1080/08989621.2019.1580147},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\7LP2K5V6\\Haven and Grootel - 2019 - Preregistering qualitative research.pdf},
  journal = {Accountability in Research},
  keywords = {Preregistration,qualitative research,transparency},
  number = {3},
  pmid = {30741570}
}

@article{head_Extent_2015,
  title = {The {{Extent}} and {{Consequences}} of {{P}}-{{Hacking}} in {{Science}}},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  month = mar,
  volume = {13},
  pages = {e1002106},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002106},
  abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as ``p-hacking,'' occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\75FVECDR\\Head et al. - 2015 - The Extent and Consequences of P-Hacking in Scienc.pdf},
  journal = {PLOS Biology},
  keywords = {Bibliometrics,Binomials,Medicine and health sciences,Metaanalysis,Publication ethics,Reproducibility,Statistical data,Test statistics},
  language = {en},
  number = {3}
}

@article{ioannidis_Why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  volume = {2},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\MW6PY4QD\\Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf},
  journal = {PLOS Medicine},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia},
  language = {en},
  number = {8}
}

@article{jerolmack_Ethical_2019,
  title = {The {{Ethical Dilemmas}} and {{Social Scientific Trade}}-Offs of {{Masking}} in {{Ethnography}}},
  author = {Jerolmack, Colin and Murphy, Alexandra K.},
  year = {2019},
  month = nov,
  volume = {48},
  pages = {801--827},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124117701483},
  abstract = {Masking, the practice of hiding or distorting identifying information about people, places, and organizations, is usually considered a requisite feature of ethnographic research and writing. This is justified both as an ethical obligation to one's subjects and as a scientifically neutral position (as readers are enjoined to treat a case's idiosyncrasies as sociologically insignificant). We question both justifications, highlighting potential ethical dilemmas and obstacles to constructing cumulative social science that can arise through masking. Regarding ethics, we show, on the one hand, how masking may give subjects a false sense of security because it implies a promise of confidentiality that it often cannot guarantee and, on the other hand, how naming may sometimes be what subjects want and expect. Regarding scientific tradeoffs, we argue that masking can reify ethnographic authority, exaggerate the universality of the case (e.g., ``Middletown''), and inhibit replicability (or ``revisits'') and sociological comparison. While some degree of masking is ethically and practically warranted in many cases and the value of disclosure varies across ethnographies, we conclude that masking should no longer be the default option that ethnographers unquestioningly choose.},
  journal = {Sociological Methods \& Research},
  keywords = {disclosure,ethics,ethnography,generalizability,masking,pseudonyms},
  language = {en},
  number = {4}
}

@article{john_Measuring_2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  volume = {23},
  pages = {524--532},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  journal = {Psychological Science},
  keywords = {disclosure,judgment,methodology,professional standards},
  language = {en},
  number = {5}
}

@article{kapiszewski_Openness_2019,
  title = {Openness in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2019},
  month = jun,
  doi = {10.33774/apsa-2019-if2he},
  abstract = {The discipline of political science has been engaged in discussion about when, why, and how to make scholarship more open for at least three decades.This piece argues that the best way to resolve our differences and develop appropriate norms and guidelines for making different types of qualitative research more open is to move from ``if'' to ``how'' \textendash{} for individual political scientists to make their work more open \textendash{} generating examples from which we can learn and on which we can build. We begin by articulating a series of principles that underlie our views on openness. We then consider the ``state of the debate,'' briefly outlining the contours of the scholarship on openness in political and other social sciences, highlighting the fractured nature of the discussion. The heart of the piece considers various strategies, illustrated by exemplary applications, for making qualitative research more open.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\ZYP473R7\\Kapiszewski and Karcher - 2019 - Openness in Practice in Qualitative Research.pdf},
  language = {en}
}

@article{kapiszewski_Transparency_2021,
  title = {Transparency in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2021},
  month = apr,
  volume = {54},
  pages = {285--291},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096520000955},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096520000955/resource/name/firstPage-S1049096520000955a.jpg},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\N5T4NDCJ\\Kapiszewski and Karcher - 2021 - Transparency in Practice in Qualitative Research.pdf;C\:\\Users\\Mar\\Zotero\\storage\\LGUNFDWV\\8B780E06FBF7F0837F39B2FD33900DD1.html},
  journal = {PS: Political Science \& Politics},
  language = {en},
  number = {2}
}

@article{klein_Practical_2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and IJzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
  editor = {Nuijten, Mich{\'e}le and Vazire, Simine},
  year = {2018},
  month = jun,
  volume = {4},
  issn = {2474-7394},
  doi = {10.1525/collabra.158},
  abstract = {The credibility of scientific claims depends upon the transparency of the research products upon which they are based (e.g., study protocols, data, materials, and analysis scripts). As psychology navigates a period of unprecedented introspection, user-friendly tools and services that support open science have flourished. However, the plethora of decisions and choices involved can be bewildering. Here we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research (e.g., choosing a repository, preparing their research products for sharing, structuring folders, etc.). Being an open scientist means adopting a few straightforward research management practices, which lead to less error prone, reproducible research workflows. Further, this adoption can be piecemeal \textendash{} each incremental step towards complete transparency adds positive value. Transparent research practices not only improve the efficiency of individual researchers, they enhance the credibility of the knowledge generated by the scientific community.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\N6UN8YAF\\Klein et al. - 2018 - A Practical Guide for Transparency in Psychologica.pdf;C\:\\Users\\Mar\\Zotero\\storage\\BR6TPSEM\\A-Practical-Guide-for-Transparency-in.html},
  journal = {Collabra: Psychology},
  number = {1}
}

@misc{knowledge_Together_,
  title = {Together We Can Fix Academia},
  author = {Knowledge, Free Our},
  abstract = {We're helping researchers organise collective action in support of open and reproducible research practices},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\WPAHPJR8\\freeourknowledge.org.html},
  howpublished = {https://freeourknowledge.org/},
  journal = {Free Our Knowledge},
  keywords = {iniciativa},
  language = {en}
}

@techreport{lareferencia.consejodirectivo_Comunicacion_2019,
  title = {{Comunicaci\'on Acad\'emica y Acceso Abierto: Acciones para un Pol\'itica P\'ublica en Am\'erica Latina}},
  shorttitle = {{Comunicaci\'on Acad\'emica y Acceso Abierto}},
  author = {LA Referencia. Consejo Directivo},
  year = {2019},
  month = may,
  institution = {{Zenodo}},
  doi = {10.5281/ZENODO.3229410},
  abstract = {Documento redactado como insumo  para las autoridades regionales que asistieron a la reuni\'on anual del Global Research Council con acuerdo del Consejo Directivo.   La publicaci\'on y difusi\'on del mismo se realiza con el fin de favorecer el di\'alogo y la construcci\'on de una visi\'on conjunta sobre la cual se debe profundizar y actualizar a la luz de los desaf\'ios del Acceso Abierto en la regi\'on en el corto y mediano plazo. La comunicaci\'on cient\'ifica y el cambio del modelo; la situaci\'on de Am\'erica Latina; el sistema de comunicaci\'on acad\'emica de la regi\'on, principios y acciones y recomendaciones para repositorios, consorcios y revistas son los ejes tem\'aticos que se abordan a lo largo de sus p\'aginas. El art\'iculo  refuerza la premisa de que se deben tomar acciones decididas para que los resultados financiados total o parcialmente con fondos p\'ublicos est\'en en Acceso Abierto y reafirma el rol central de los organismos de CyT para lograrlo. Basado en la realidad regional, propone principios generales y acciones para los repositorios de Acceso Abierto, consorcios y revistas con una mirada m\'as sist\'emica desde las pol\'iticas p\'ublicas. Concluye con la necesidad de un di\'alogo con iniciativas como el ``Plan S'' se\~nalando los puntos de acuerdo, as\'i como diferencias, dado el contexto regional, en temas como el APC o una valorizaci\'on del rol de los repositorios. Presentado en la reuni\'on  de   COAR. 2019.  Technical and Strategic Meeting of Repository Networks. Mayo 21, 2019 - Lyon, France. Alberto Cabezas Bullemore, Secretario Ejecutivo,   LA Referencia.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\R57VSJ8N\\LA Referencia. Consejo Directivo - 2019 - Comunicación Académica y Acceso Abierto Acciones .pdf},
  keywords = {Acceso Abierto,Ciencia Abierta,Financiadores,ONCYTs,Plan S,Repositorios,revisado},
  language = {es}
}

@article{lewandowsky_Research_2016,
  title = {Research Integrity: {{Don}}'t Let Transparency Damage Science},
  shorttitle = {Research Integrity},
  author = {Lewandowsky, Stephan and Bishop, Dorothy},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {459--461},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/529459a},
  abstract = {Stephan Lewandowsky and Dorothy Bishop explain how the research community should protect its members from harassment, while encouraging the openness that has become essential to science.},
  copyright = {2016 Nature Publishing Group},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\J5AWUCXL\\Lewandowsky and Bishop - 2016 - Research integrity Don't let transparency damage .pdf;C\:\\Users\\Mar\\Zotero\\storage\\TE4JAR2W\\529459a.html},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{lindsay_Seven_2020,
  title = {Seven Steps toward Transparency and Replicability in Psychological Science.},
  author = {Lindsay, D. Stephen},
  year = {2020},
  month = nov,
  volume = {61},
  pages = {310--317},
  issn = {1878-7304, 0708-5591},
  doi = {10.1037/cap0000222},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\7T4SURP8\\Lindsay - 2020 - Seven steps toward transparency and replicability .pdf},
  journal = {Canadian Psychology/Psychologie canadienne},
  keywords = {forrt},
  language = {en},
  number = {4}
}

@techreport{luke_Epistemological_2019,
  title = {Epistemological and {{Ontological Priors}}: {{Explicating}} the {{Perils}} of {{Transparency}}},
  shorttitle = {Epistemological and {{Ontological Priors}}},
  author = {Luke, Timothy W. and {V{\'a}zquez-Arroyo}, Antonio and Hawkesworth, Mary},
  year = {2019},
  month = feb,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3332878},
  abstract = {The discipline of political science encompasses multiple research communities, which have grown out of and rely upon different epistemological and ontological presuppositions.  Recent debates about transparency raise important questions about which of these research communities will be accredited within the discipline, whose values, norms, and methods of knowledge production will gain ascendency and whose will be marginalized.  Although the language of "transparency" makes it appear that these debates are apolitical, simply elaborating standards that all political scientists share, the intensity and content of recent contestations about DA-RT, JETS, and QTD attest to the profoundly political nature of these methodological discussions. This report traces the epistemological and ontological assumptions that have shaped diverse research communities within the discipline, situating "transparency" in relation to classical (Aristotelian), modern (Baconian) and twentieth-century (positivist, critical rationalist, and postpositivist) versions of empiricism.  It shows how recent discussions of transparency accredit certain empirical approaches by collapsing the scope of empirical investigation and the parameters of the knowable.  And it argues that "transparency" is inappropriate as a regulative ideal for political science because it misconstrues the roles of theory, social values, and critique in scholarly investigation.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\CSJB78BF\\papers.html},
  keywords = {epistemology,ontology,philosophy of science,qualitative methods,Qualitative Transparency Deliberations,research transparency},
  language = {en},
  number = {ID 3332878},
  type = {{{SSRN Scholarly Paper}}}
}

@article{miguel_Promoting_2014,
  title = {Promoting {{Transparency}} in {{Social Science Research}}},
  author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and der Laan, M. Van},
  year = {2014},
  month = jan,
  volume = {343},
  pages = {30--31},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1245317},
  abstract = {There is growing appreciation for the advantages of experimentation in the social sciences. Policy-relevant claims that in the past were backed by theoretical arguments and inconclusive correlations are now being investigated using more credible methods. Changes have been particularly pronounced in development economics, where hundreds of randomized trials have been carried out over the last decade. When experimentation is difficult or impossible, researchers are using quasi-experimental designs. Governments and advocacy groups display a growing appetite for evidence-based policy-making. In 2005, Mexico established an independent government agency to rigorously evaluate social programs, and in 2012, the U.S. Office of Management and Budget advised federal agencies to present evidence from randomized program evaluations in budget requests (1, 2). Social scientists should adopt higher transparency standards to improve the quality and credibility of research. Social scientists should adopt higher transparency standards to improve the quality and credibility of research.},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\MWQWX2I6\\Miguel et al. - 2014 - Promoting Transparency in Social Science Research.pdf;C\:\\Users\\Mar\\Zotero\\storage\\J9V2M2ZG\\30.html},
  journal = {Science},
  language = {en},
  number = {6166},
  pmid = {24385620}
}

@article{motta_Dynamics_2018,
  title = {The {{Dynamics}} and {{Political Implications}} of {{Anti}}-{{Intellectualism}} in the {{United States}}},
  author = {Motta, Matthew},
  year = {2018},
  month = may,
  volume = {46},
  pages = {465--498},
  publisher = {{SAGE Publications Inc}},
  issn = {1532-673X},
  doi = {10.1177/1532673X17719507},
  abstract = {Recently, Americans have become increasingly likely to hold anti-intellectual attitudes (i.e., negative affect toward scientists and other experts). However, few have investigated the political implications of anti-intellectualism, and much empirical uncertainty surrounds whether or not these attitudes can be mitigated. Drawing on cross-sectional General Social Survey (GSS) data and a national election panel in 2016, I find that anti-intellectualism is associated with not only the rejection of policy-relevant matters of scientific consensus but support for political movements (e.g., ``Brexit'') and politicians (e.g., George Wallace, Donald Trump) who are skeptical of experts. Critically, though, I show that these effects can be mitigated. Verbal intelligence plays a strong role in mitigating anti-intellectual sympathies, compared with previously studied potential mitigators. I conclude by discussing how scholars might build on this research to study the political consequences of anti-intellectualism in the future.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\56UDHV9E\\Motta - 2018 - The Dynamics and Political Implications of Anti-In.pdf},
  journal = {American Politics Research},
  keywords = {anti-intellectualism,antiscience attitudes,political psychology,public opinion,verbal intelligence},
  language = {en},
  number = {3}
}

@misc{nassi-calo_Open_2013,
  title = {Open {{Access}} and a Call to Prevent the Looming Crisis in Science | {{SciELO}} in {{Perspective}}},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2013},
  month = jul,
  abstract = {The number of retracted articles has recently been on the rise. Bj\"orn Brembs identifies this tendency as a reflection of an imminent crisis in science whose},
  language = {en-US}
}

@misc{nassi-calo_reproducibilidad_2014,
  title = {La Reproducibilidad En Los Resultados de Investigaci\'on: La Mirada Subjetiva | {{SciELO}} En {{Perspectiva}}},
  shorttitle = {La Reproducibilidad En Los Resultados de Investigaci\'on},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2014},
  month = feb,
  abstract = {En una \'epoca en que las discusiones sobre \'etica en la experimentaci\'on y la publicaci\'on cient\'ifica traspasan los laboratorios y ambientes acad\'emicos para},
  language = {en-US}
}

@article{naturehumanbehaviour_Tell_2020,
  title = {Tell It like It Is},
  author = {{Nature human behaviour}},
  year = {2020},
  month = jan,
  volume = {4},
  pages = {1--1},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-0818-9},
  abstract = {Every research paper tells a story, but the pressure to provide `clean' narratives is harmful for the scientific endeavour.},
  copyright = {2020 Springer Nature Limited},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\D9IJKHLU\\2020 - Tell it like it is.pdf;C\:\\Users\\Mar\\Zotero\\storage\\GGNSR5XK\\s41562-020-0818-9.html},
  journal = {Nature Human Behaviour},
  keywords = {forrt},
  language = {en},
  number = {1}
}

@article{nosek_preregistration_2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  volume = {115},
  pages = {2600--2606},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes\textemdash a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  chapter = {Colloquium Paper},
  copyright = {\textcopyright{} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\MMRA278K\\Nosek et al. - 2018 - The preregistration revolution.pdf;C\:\\Users\\Mar\\Zotero\\storage\\78Y3QQ72\\2600.html},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration,revisado},
  language = {en},
  number = {11},
  pmid = {29531091}
}

@article{nosek_Promoting_2015,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1422--1425},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\6AJFDEQF\\Nosek et al. - 2015 - Promoting an open research culture.pdf;C\:\\Users\\Mar\\Zotero\\storage\\C8E9YZMI\\1422.html},
  journal = {Science},
  keywords = {revisado},
  language = {en},
  number = {6242},
  pmid = {26113702}
}

@article{nosek_Registered_2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  month = may,
  volume = {45},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\E2QGMCPK\\Nosek and Lakens - 2014 - Registered Reports A Method to Increase the Credi.pdf},
  journal = {Social Psychology},
  keywords = {forrt},
  language = {en},
  number = {3}
}

@article{nosek_Transparency_2014,
  title = {Transparency and {{Openness Promotion}} ({{TOP}}) {{Guidelines}}},
  author = {Nosek, Brian A. and Alter, George and Banks, George Christopher and Borsboom, Denny and Bowman, Sara and Breckler, Steven and Buck, Stuart and Chambers, Chris and Chin, Gilbert and Christensen, Garret},
  year = {2014},
  month = aug,
  publisher = {{OSF}},
  abstract = {The Transparency and Openness Promotion (TOP) Committee met in November 2014 to address one important element of the incentive systems - journals' procedures and  policies for publication. The outcome of the effort is the TOP Guidelines. There are eight standards in the TOP guidelines; each move scientific communication toward greater openness.  These standards are modular, facilitating adoption in whole or in part. However, they also complement each other, in that commitment to one standard may facilitate adoption of others. Moreover, the guidelines are sensitive to barriers to openness by articulating, for example, a process for exceptions to sharing because of ethical issues, intellectual property concerns, or availability of necessary resources.      Hosted on the Open Science Framework},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\T7WX3SMW\\9f6gx.html},
  language = {en}
}

@misc{nw_Trust_2019,
  title = {Trust and {{Mistrust}} in {{Americans}}' {{Views}} of {{Scientific Experts}}},
  author = {NW, 1615 L. St and Suite 800Washington and Inquiries, DC 20036USA202-419-4300 | Main202-857-8562 | Fax202-419-4372 | Media},
  year = {2019},
  month = aug,
  abstract = {Public confidence in scientists is on the upswing, and six-in-ten Americans say scientists should play an active role in policy debates about scientific issues, according to a new Pew Research Center survey.},
  journal = {Pew Research Center Science \& Society},
  language = {en-US}
}

@article{oboyle_Chrysalis_2017,
  title = {The {{Chrysalis Effect}}: {{How Ugly Initial Results Metamorphosize Into Beautiful Articles}}},
  shorttitle = {The {{Chrysalis Effect}}},
  author = {O'Boyle, Ernest Hugh and Banks, George Christopher and {Gonzalez-Mul{\'e}}, Erik},
  year = {2017},
  month = feb,
  volume = {43},
  pages = {376--399},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206314527133},
  abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the ``Chrysalis Effect.''},
  journal = {Journal of Management},
  keywords = {ethics,morality and moral behavior,philosophy of science,revisado,statistical methods},
  language = {en},
  number = {2}
}

@misc{pages_Sociological_,
  title = {`{{Sociological Gobbledygook}}' and {{Public Distrust}} of {{Social Science Experts}} - {{There}}'s {{Research}} on {{That}}},
  author = {Pages, The Society},
  abstract = {The Society Pages (TSP) is an open-access social science project headquartered in the Department of Sociology at the University of Minnesota},
  language = {en}
}

@article{peng_reproducibility_2015,
  title = {The Reproducibility Crisis in Science: {{A}} Statistical Counterattack},
  shorttitle = {The Reproducibility Crisis in Science},
  author = {Peng, Roger},
  year = {2015},
  volume = {12},
  pages = {30--32},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2015.00827.x},
  abstract = {More people have more access to data than ever before. But a comparative lack of analytical skills has resulted in scientific findings that are neither replicable nor reproducible. It is time to invest in statistics education, says Roger Peng},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00827.x},
  copyright = {\textcopyright{} 2015 The Royal Statistical Society},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\P7P2WL5Y\\Peng - 2015 - The reproducibility crisis in science A statistic.pdf;C\:\\Users\\Mar\\Zotero\\storage\\U4FSW6GX\\j.1740-9713.2015.00827.html},
  journal = {Significance},
  language = {en},
  number = {3}
}

@article{price_Problem_2020,
  title = {Problem with p Values: Why p Values Do Not Tell You If Your Treatment Is Likely to Work},
  shorttitle = {Problem with p Values},
  author = {Price, Robert and Bethune, Rob and Massey, Lisa},
  year = {2020},
  month = jan,
  volume = {96},
  pages = {1--3},
  issn = {0032-5473, 1469-0756},
  doi = {10.1136/postgradmedj-2019-137079},
  journal = {Postgraduate Medical Journal},
  keywords = {forrt},
  language = {en},
  number = {1131}
}

@article{rinke_Probabilistic_2018,
  title = {Probabilistic {{Misconceptions Are Pervasive Among Communication Researchers}}},
  author = {Rinke, Eike Mark and Schneider, Frank M.},
  year = {2018},
  month = sep,
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/h8zbe},
  abstract = {Across all areas of communication research, the most popular approach to generating insights about communication is the classical significance test (also called null hypothesis significance testing, NHST). The predominance of NHST in communication research is in spite of serious concerns about the ability of researchers to properly interpret its results. We draw on data from a survey of the ICA membership to assess the evidential basis of these concerns. The vast majority of communication researchers misinterpreted NHST (91\%) and the most prominent alternative, confidence intervals (96\%), while overestimating their competence. Academic seniority and statistical experience did not predict better interpretation outcomes. These findings indicate major problems regarding the generation of knowledge in the field of communication research.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\I8PXB338\\Rinke and Schneider - 2018 - Probabilistic Misconceptions Are Pervasive Among C.pdf},
  keywords = {Communication,confidence intervals,data analysis,misconceptions,significance testing,Social and Behavioral Sciences,statistical inference,statistics}
}

@article{rodriguez-sanchez_Ciencia_2016,
  title = {{Ciencia reproducible: qu\'e, por qu\'e, c\'omo}},
  shorttitle = {{Ciencia reproducible}},
  author = {{Rodriguez-Sanchez}, Francisco and {P{\'e}rez-Luque}, Antonio Jes{\'u}s and Bartomeus, Ignasi and Varela, Sara},
  year = {2016},
  month = jul,
  volume = {25},
  pages = {83--92},
  issn = {1697-2473},
  doi = {10.7818/ECOS.2016.25-2.11},
  copyright = {Derechos de autor},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\33N3BNL7\\1178.html},
  journal = {Ecosistemas},
  keywords = {reproducibilidad,revisado},
  language = {es},
  number = {2}
}

@misc{rowe_Preview_2018,
  title = {Preview My New Book: {{Introduction}} to {{Reproducible Science}} in {{R}} | {{R}}-Bloggers},
  shorttitle = {Preview My New Book},
  author = {Rowe, Brian Lee Yung},
  year = {2018},
  month = nov,
  abstract = {I'm pleased to share Part I of my new book ``Introduction to Reproducible Science in R``. The purpose of this \ldots Continue reading \textrightarrow},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\LMSG3XTX\\preview-my-new-book-introduction-to-reproducible-science-in-r.html},
  language = {en-US}
}

@misc{science_Center_,
  title = {Center for {{Open Science}}},
  author = {Science, Center for Open},
  abstract = {At the Center for Open Science, our mission is to increase openness, integrity, and reproducibility of scholarly research. Promoting these practices within the research funding and publishing communities accelerates scientific progress},
  howpublished = {https://www.cos.io},
  keywords = {iniciativa},
  language = {en}
}

@article{simmons_FalsePositive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  volume = {22},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  journal = {Psychological Science},
  language = {en},
  number = {11}
}

@article{simmons_FalsePositive_2011a,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  volume = {22},
  pages = {1359--1366},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\KA4WVZJW\\Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf},
  journal = {Psychological Science},
  keywords = {disclosure,methodology,motivated reasoning,publication},
  language = {en},
  number = {11}
}

@article{simonsohn_Pcurve_2014,
  title = {P-Curve: {{A}} Key to the File-Drawer},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  volume = {143},
  pages = {534--547},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/a0033242},
  abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that ``work,'' readers must ask, ``Are these effects true, or do they merely reflect selective reporting?'' We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps p-curves\textemdash containing more low (.01s) than high (.04s) significant p values\textemdash only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\SC7J46YH\\2013-25331-001.html},
  journal = {Journal of Experimental Psychology: General},
  keywords = {Hypothesis Testing,Psychology,Scientific Communication,Statistics},
  number = {2}
}

@article{stodden_Trust_2011,
  title = {Trust {{Your Science}}? {{Open Your Data}} and {{Code}}},
  shorttitle = {Trust {{Your Science}}?},
  author = {Stodden, Victoria C.},
  year = {2011},
  volume = {409},
  pages = {21--22},
  doi = {10.7916/D8CJ8Q0P},
  abstract = {This is a view on the reproducibility of computational sciences by Victoria Stodden. It contains information on the Reproducibility, Replicability, and Repeatability of code created by the other sciences. Stodden also talks about the rising prominence of computational sciences as we are in the digital age and what that means for the future of science and collecting data.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\XLHSABC7\\Stodden - 2011 - Trust Your Science Open Your Data and Code.pdf;C\:\\Users\\Mar\\Zotero\\storage\\62IABAV4\\D8CJ8Q0P.html},
  keywords = {forrt},
  language = {en}
}

@article{szollosi_Preregistration_2020,
  title = {Is {{Preregistration Worthwhile}}?},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard and {van Rooij}, Iris and Van Zandt, Trisha and Donkin, Chris},
  year = {2020},
  month = feb,
  volume = {24},
  pages = {94--95},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.11.009},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\9MCLXVCZ\\Szollosi et al. - 2020 - Is Preregistration Worthwhile.pdf},
  journal = {Trends in Cognitive Sciences},
  keywords = {forrt},
  language = {en},
  number = {2}
}

@article{thibodeaux_Production_2016,
  title = {Production as Social Change: {{Policy}} Sociology as a Public Good},
  shorttitle = {Production as Social Change},
  author = {Thibodeaux, Jarrett},
  year = {2016},
  month = may,
  volume = {36},
  pages = {183--190},
  publisher = {{Routledge}},
  issn = {0273-2173},
  doi = {10.1080/02732173.2015.1102666},
  abstract = {Burawoy described two ways sociology can aid the public, through: (1) instrumental (policy) sociology and (2) reflexive (public) sociology. This article elaborates the different assumptions of how social change occurs according to policy and public sociology (and how sociology effects social change). Policy sociology assumes social change occurs through the scientific elaboration of the best means to achieve goals. However, policy sociology largely takes the public as an object of power rather than subjects who can utilize scientific knowledge. Public sociology assumes that social change occurs through the exposure of contradictions in goals, which elaborates better goals. However, the elaboration of contradictions assumes that there is a fundamental thesis/antithesis in society. If there are multiple goals/theses, public sociology fails in at least three ways. Policy sociology, when reflexively selecting its public, provides the best way sociology can aid the public.},
  annotation = {\_eprint: https://doi.org/10.1080/02732173.2015.1102666},
  journal = {Sociological Spectrum},
  number = {3}
}

@article{vankov_Article_2014,
  title = {Article {{Commentary}}: {{On}} the {{Persistence}} of {{Low Power}} in {{Psychological Science}}},
  shorttitle = {Article {{Commentary}}},
  author = {Vankov, Ivan and Bowers, Jeffrey and Munaf{\`o}, Marcus R.},
  year = {2014},
  month = may,
  volume = {67},
  pages = {1037--1040},
  publisher = {{SAGE Publications}},
  issn = {1747-0218},
  doi = {10.1080/17470218.2014.885986},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\HRWCM3XB\\Vankov et al. - 2014 - Article Commentary On the Persistence of Low Powe.pdf},
  journal = {Quarterly Journal of Experimental Psychology},
  language = {en},
  number = {5}
}

@article{warren_How_2019,
  title = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}? {{Or}} to {{Get Tenure}}? {{Trends}} over a {{Generation}}},
  shorttitle = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}?},
  author = {Warren, John Robert},
  year = {2019},
  month = feb,
  volume = {6},
  pages = {172--196},
  issn = {2330-6696},
  doi = {10.15195/v6.a7},
  abstract = {Many sociologists suspect that publication expectations have risen over time\textemdash that how much graduate students have published to get assistant professor jobs and how much assistant professors have published to be promoted have gone up. Using information about faculty in 21 top sociology departments from the American Sociological Association's Guide to Graduate Departments of Sociology, online curricula vitae, and other public records, I provide empirical evidence to support this suspicion. On the day they start their first jobs, new assistant professors in recent years have already published roughly twice as much as their counterparts did in the early 1990s. Trends for promotion to associate professor are not as dramatic but are still remarkable. I evaluate several potential explanations for these trends and conclude that they are driven mainly by changes over time in the fiscal and organizational realities of universities and departments.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\BL77TN79\\Warren - 2019 - How Much Do You Have to Publish to Get a Job in a .pdf;C\:\\Users\\Mar\\Zotero\\storage\\W7KBP3KB\\articles-v6-7-172.html},
  journal = {Sociological Science},
  language = {en-US}
}

@article{wilson_Replication_1973,
  title = {The {{Replication Problem}} in {{Sociology}}: {{A Report}} and a {{Suggestion}}*},
  shorttitle = {The {{Replication Problem}} in {{Sociology}}},
  author = {Wilson, Franklin D. and Smoke, Gale L. and Martin, J. David},
  year = {1973},
  volume = {43},
  pages = {141--149},
  issn = {1475-682X},
  doi = {10.1111/j.1475-682X.1973.tb00711.x},
  abstract = {The deleterious effects of joint bias in favor of statistical inference and against replication are becoming well known. The acceptance of numerous Type I errors into the literature is by far the most serious of these. Data on the contents of three major journals support the contention that a joint bias for statistical significance tests, for rejections, and against replication exists in modern sociology. This finding replicates that of Sterling (1959) for psychology. A speculative analysis of the dynamics of publication decisions suggests that a compact format for reporting replications might make their publication more attractive to editors, and thus increase their frequency in the literature. A possible format for briefly reporting replication studies is suggested.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-682X.1973.tb00711.x},
  journal = {Sociological Inquiry},
  language = {en},
  number = {2}
}

@article{zenk-moltgen_Factors_2018,
  title = {Factors Influencing the Data Sharing Behavior of Researchers in Sociology and Political Science},
  author = {{Zenk-M{\"o}ltgen}, Wolfgang and Akdeniz, Esra and Katsanidou, Alexia and Na{\ss}hoven, Verena and Balaban, Ebru},
  year = {2018},
  month = jan,
  volume = {74},
  pages = {1053--1073},
  publisher = {{Emerald Publishing Limited}},
  issn = {0022-0418},
  doi = {10.1108/JD-09-2017-0126},
  abstract = {Purpose Open data and data sharing should improve transparency of research. The purpose of this paper is to investigate how different institutional and individual factors affect the data sharing behavior of authors of research articles in sociology and political science. Design/methodology/approach Desktop research analyzed attributes of sociology and political science journals (n=262) from their websites. A second data set of articles (n=1,011; published 2012-2014) was derived from ten of the main journals (five from each discipline) and stated data sharing was examined. A survey of the authors used the Theory of Planned Behavior to examine motivations, behavioral control, and perceived norms for sharing data. Statistical tests (Spearman's {$\rho$}, {$\chi$}2) examined correlations and associations. Findings Although many journals have a data policy for their authors (78 percent in sociology, 44 percent in political science), only around half of the empirical articles stated that the data were available, and for only 37 percent of the articles could the data be accessed. Journals with higher impact factors, those with a stated data policy, and younger journals were more likely to offer data availability. Of the authors surveyed, 446 responded (44 percent). Statistical analysis indicated that authors' attitudes, reported past behavior, social norms, and perceived behavioral control affected their intentions to share data. Research limitations/implications Less than 50 percent of the authors contacted provided responses to the survey. Results indicate that data sharing would improve if journals had explicit data sharing policies but authors also need support from other institutions (their universities, funding councils, and professional associations) to improve data management skills and infrastructures. Originality/value This paper builds on previous similar research in sociology and political science and explains some of the barriers to data sharing in social sciences by combining journal policies, published articles, and authors' responses to a survey.},
  file = {C\:\\Users\\Mar\\Zotero\\storage\\5FMYESJH\\html.html},
  journal = {Journal of Documentation},
  keywords = {Data availability,Data policy,Data sharing,Political science,Replication,Research data management,Research transparency,Sociology,Theory of Planned Behaviour},
  number = {5}
}


