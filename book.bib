
@misc{_Figueiredo_2021,
  title = {Figueiredo, {{Martinovic}}, {{Rees}}, and {{Licata}}: {{Collective Memories}} and {{Present}}-{{Day Intergroup Relations}}: {{Introduction}} to the {{Special Thematic Section}}},
  shorttitle = {Figueiredo, {{Martinovic}}, {{Rees}}, and {{Licata}}},
  year = {2021},
  month = jun,
  doi = {10.5964/jspp.v5i2.895},
  howpublished = {https://jspp.psychopen.eu/index.php/jspp/article/download/4995/4995.html?inline=1}
}

@misc{_impact_2021,
  title = {The Impact of a Multicultural Exchange between Indigenous and Non-Indigenous History Teachers for Students' Attitudes: Preliminary Evidence from a Pilot Study in {{Chile}}: {{Multicultural Education Review}}: {{Vol}} 12, {{No}} 3},
  year = {2021},
  month = jun,
  howpublished = {https://www.tandfonline.com/doi/abs/10.1080/2005615X.2020.1808927}
}

@misc{_Retraction_,
  title = {Retraction {{Watch}}},
  abstract = {Tracking retractions as a window into the scientific process},
  howpublished = {https://retractionwatch.com/},
  journal = {Retraction Watch},
  language = {en-US}
}

@book{abrilruiz_Manzanas_2019,
  title = {{Manzanas podridas: Malas pr\'acticas de investigaci\'on y ciencia descuidada}},
  shorttitle = {{Manzanas podridas}},
  author = {Abril Ruiz, Angel},
  year = {2019},
  annotation = {OCLC: 1120499121},
  isbn = {978-1-07-075536-6},
  language = {Spanish}
}

@article{aczel_consensusbased_2020,
  title = {A Consensus-Based Transparency Checklist},
  author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharsk{\'y}, {\v S}imon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munaf{\`o}, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ron{\'a}n M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and {Giner-Sorolla}, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and {de la Guardia}, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
  year = {2020},
  month = jan,
  volume = {4},
  pages = {4--6},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0772-6},
  abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
  copyright = {2019 The Author(s)},
  journal = {Nature Human Behaviour},
  keywords = {forrt,herramienta},
  language = {en},
  number = {1}
}

@article{allen_Open_2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  month = may,
  volume = {17},
  pages = {e3000246},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
  journal = {PLOS Biology},
  keywords = {Careers,Experimental design,Neuroimaging,Open data,Open science,Peer review,Reproducibility,Statistical data},
  language = {en},
  number = {5}
}

@article{allison_Reproducibility_2018,
  title = {Reproducibility of Research: {{Issues}} and Proposed Remedies},
  shorttitle = {Reproducibility of Research},
  author = {Allison, David B. and Shiffrin, Richard M. and Stodden, Victoria},
  year = {2018},
  month = mar,
  volume = {115},
  pages = {2561--2562},
  journal = {Proceedings of the National Academy of Sciences},
  number = {11}
}

@article{an_Crisis_2018,
  title = {The {{Crisis}} of {{Reproducibility}}, the {{Denominator Problem}} and the {{Scientific Role}} of {{Multi}}-Scale {{Modeling}}},
  author = {An, Gary},
  year = {2018},
  month = dec,
  volume = {80},
  pages = {3071--3080},
  issn = {1522-9602},
  doi = {10.1007/s11538-018-0497-0},
  abstract = {The ``Crisis of Reproducibility'' has received considerable attention both within the scientific community and without. While factors associated with scientific culture and practical practice are most often invoked, I propose that the Crisis of Reproducibility is ultimately a failure of generalization with a fundamental scientific basis in the methods used for biomedical research. The Denominator Problem describes how limitations intrinsic to the two primary approaches of biomedical research, clinical studies and preclinical experimental biology, lead to an inability to effectively characterize the full extent of biological heterogeneity, which compromises the task of generalizing acquired knowledge. Drawing on the example of the unifying role of theory in the physical sciences, I propose that multi-scale mathematical and dynamic computational models, when mapped to the modular structure of biological systems, can serve a unifying role as formal representations of what is conserved and similar from one biological context to another. This ability to explicitly describe the generation of heterogeneity from similarity addresses the Denominator Problem and provides a scientific response to the Crisis of Reproducibility.},
  journal = {Bulletin of Mathematical Biology},
  keywords = {crisis},
  language = {en},
  number = {12}
}

@article{andrea_Why_2018,
  title = {Why Science's Crisis Should Not Become a Political Battling Ground},
  author = {Andrea, Saltelli},
  year = {2018},
  month = dec,
  volume = {104},
  pages = {85--90},
  issn = {0016-3287},
  doi = {10.1016/j.futures.2018.07.006},
  abstract = {A science war is in full swing which has taken science's reproducibility crisis as a battleground. While conservatives and corporate interests use the crisis to weaken regulations, their opponent deny the existence of a science's crisis altogether. Thus, for the conservative National Association of Scholars NAS the crisis is real and due to the progressive assault on higher education with ideologies such as ``neo-Marxism, radical feminism, historicism, post-colonialism, deconstructionism, post-modernism, liberation theology''. In the opposite field, some commentators claim that there is no crisis in science and that saying the opposite is irresponsible. These positions are to be seen in the context of the ongoing battle against regulation, of which the new rules proposed at the US Environmental Protection Agency (EPA) are but the last chapter. In this optic, Naomi Oreskes writes on Nature that what constitutes the crisis is the conservatives' attack on science. This evident right-left divide in the reading of the crisis is unhelpful and dangerous to the survival of science itself. An alternative reading ignored by the contendents would suggest that structural contradictions have emerged in modern science, and that addressing these should be the focus of our attention.},
  journal = {Futures},
  keywords = {crisis,Evidence-based policy,History and philosophy of science,Post-normal science,Science and technology studies,Science’s crisis,Science’s reproducibility,Science’s war,Scientism},
  language = {en}
}

@article{angell_Publish_1986,
  title = {Publish or {{Perish}}: {{A Proposal}}},
  shorttitle = {Publish or {{Perish}}},
  author = {Angell, Marcia},
  year = {1986},
  month = feb,
  volume = {104},
  pages = {261--262},
  publisher = {{American College of Physicians}},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-104-2-261},
  journal = {Annals of Internal Medicine},
  keywords = {institutional},
  number = {2}
}

@article{anvari_replicability_2018,
  title = {The Replicability Crisis and Public Trust in Psychological Science},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = {2018},
  month = sep,
  volume = {3},
  pages = {266--286},
  publisher = {{Routledge}},
  issn = {2374-3603},
  doi = {10.1080/23743603.2019.1684822},
  abstract = {Replication failures of past findings in several scientific disciplines, including psychology, medicine, and experimental economics, have created a ``crisis of confidence'' among scientists. Psychological science has been at the forefront of tackling these issues, with discussions about replication failures and scientific self-criticisms of questionable research practices (QRPs) increasingly taking place in public forums. How this replicability crisis impacts the public's trust is a question yet to be answered by research. Whereas some researchers believe that the public's trust will be positively impacted or maintained, others believe trust will be diminished. Because it is our field of expertise, we focus on trust in psychological science. We performed a study testing how public trust in past and future psychological research would be impacted by being informed about (i) replication failures (replications group), (ii) replication failures and criticisms of QRPs (QRPs group), and (iii) replication failures, criticisms of QRPs, and proposed reforms (reforms group). Results from a mostly European sample (N = 1129) showed that, compared to a control group, people in the replications, QRPs, and reforms groups self-reported less trust in past research. Regarding trust in future research, the replications and QRPs groups did not significantly differ from the control group. Surprisingly, the reforms group had less trust in future research than the control group. Nevertheless, people in the replications, QRPs, and reforms groups did not significantly differ from the control group in how much they believed future research in psychological science should be supported by public funding. Potential explanations are discussed.},
  annotation = {\_eprint: https://doi.org/10.1080/23743603.2019.1684822},
  journal = {Comprehensive Results in Social Psychology},
  keywords = {crisis of confidence,open science,Replicability crisis,reproducibility crisis,trust in science},
  number = {3}
}

@article{armeni_widescale_2021,
  title = {Towards Wide-Scale Adoption of Open Science Practices: {{The}} Role of Open Science Communities},
  shorttitle = {Towards Wide-Scale Adoption of Open Science Practices},
  author = {Armeni, Kristijan and Brinkman, Loek and Carlsson, Rickard and Eerland, Anita and Fijten, Rianne and Fondberg, Robin and Heininga, Vera E and Heunis, Stephan and Koh, Wei Qi and Masselink, Maurits and Moran, Niall and Baoill, Andrew {\'O} and Sarafoglou, Alexandra and Schettino, Antonio and Schwamm, Hardy and Sjoerds, Zsuzsika and Teperek, Marta and {van den Akker}, Olmo R and {van't Veer}, Anna and {Zurita-Milla}, Raul},
  year = {2021},
  month = jul,
  issn = {0302-3427},
  doi = {10.1093/scipol/scab039},
  abstract = {Despite the increasing availability of Open Science (OS) infrastructure and the rise in policies to change behaviour, OS practices are not yet the norm. While pioneering researchers are developing OS practices, the majority sticks to status quo. To transition to common practice, we must engage a critical proportion of the academic community. In this transition, OS Communities (OSCs) play a key role. OSCs are bottom-up learning groups of scholars that discuss OS within and across disciplines. They make OS knowledge more accessible and facilitate communication among scholars and policymakers. Over the past two years, eleven OSCs were founded at several Dutch university cities. In other countries, similar OSCs are starting up. In this article, we discuss the pivotal role OSCs play in the large-scale transition to OS. We emphasize that, despite the grassroot character of OSCs, support from universities is critical for OSCs to be viable, effective, and sustainable.},
  journal = {Science and Public Policy},
  number = {scab039}
}

@article{baker_500_2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  volume = {533},
  pages = {452--454},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  copyright = {2016 Nature Publishing Group},
  journal = {Nature},
  keywords = {crisis},
  language = {en},
  number = {7604}
}

@article{bakker_Ensuring_2018,
  title = {Ensuring the Quality and Specificity of Preregistrations},
  author = {Bakker, Marjan and Veldkamp, Coosje Lisabet Sterre and van Assen, Marcel A. L. M. and Crompvoets, Elise Anne Victoire and Ong, How Hwee and Nosek, Brian A. and Soderberg, Courtney K. and Mellor, David Thomas and Wicherts, Jelte},
  year = {2018},
  month = sep,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/cdgyh},
  abstract = {Researchers face many, often seemingly arbitrary choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. Opportunistic use of `researcher degrees of freedom' aimed at obtaining statistical significance increases the likelihood of obtaining and publishing false positive results and overestimated effect sizes. Preregistration is a mechanism for reducing such degrees of freedom by specifying designs and analysis plans before observing the research outcomes. The effectiveness of preregistration may depend, in part, on whether the process facilitates sufficiently specific articulation of such plans. In this preregistered study, we compared two formats of preregistration available on the OSF: Standard Pre-Data Collection Registration and Prereg Challenge registration (now called ``OSF Preregistration'', http://osf.io/prereg/). The Prereg Challenge format was a structured workflow with detailed instructions, and an independent review to confirm completeness; the ``Standard'' format was unstructured with minimal direct guidance to give researchers flexibility for what to pre-specify. Results of comparing random samples of 53 preregistrations from each format indicate that the structured format restricted the opportunistic use of researcher degrees of freedom better (Cliff's Delta = 0.49) than the unstructured format, but neither eliminated all researcher degrees of freedom. We also observed very low concordance among coders about the number of hypotheses (14\%), indicating that they are often not clearly stated. We conclude that effective preregistration is challenging, and registration formats that provide effective guidance may improve the quality of research.},
  keywords = {Meta-science,preregistration,Quantitative Methods,Questionable research practices,researcher degrees of freedom,Social and Behavioral Sciences,Statistical Methods}
}

@article{barba_Terminologies_2018,
  title = {Terminologies for {{Reproducible Research}}},
  author = {Barba, Lorena A.},
  year = {2018},
  month = feb,
  abstract = {Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called "crisis" of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, A---make no distinction between the words reproduce and replicate, or B---use them distinctly. If B, then they are commonly divided in two camps. In a spectrum of concerns that starts at a minimum standard of "same data+same methods=same results," to "new data and/or new methods in an independent study=same findings," group 1 calls the minimum standard reproduce, while group 2 calls it replicate. This direct swap of the two terms aggravates an already weighty issue. By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.},
  archiveprefix = {arXiv},
  eprint = {1802.03311},
  eprinttype = {arxiv},
  journal = {arXiv:1802.03311 [cs]},
  keywords = {Computer Science - Digital Libraries},
  primaryclass = {cs}
}

@article{benjamin-chung_Internal_2020,
  title = {Internal Replication of Computational Workflows in Scientific Research},
  author = {{Benjamin-Chung}, Jade and Colford, Jr., John M. and Mertens, Andrew and Hubbard, Alan E. and Arnold, Benjamin F.},
  year = {2020},
  month = jun,
  volume = {4},
  pages = {17},
  issn = {2572-4754},
  doi = {10.12688/gatesopenres.13108.2},
  abstract = {Failures to reproduce research findings across scientific disciplines from psychology to physics have garnered increasing attention in recent years. External replication of published findings by outside investigators has emerged as a method to detect errors and bias in the published literature. However, some studies influence policy and practice before external replication efforts can confirm or challenge the original contributions. Uncovering and resolving errors before publication would increase the efficiency of the scientific process by increasing the accuracy of published evidence. Here we summarize the rationale and best practices for internal replication, a process in which multiple independent data analysts replicate an analysis and correct errors prior to publication. We explain how internal replication should reduce errors and bias that arise during data analyses and argue that it will be most effective when coupled with pre-specified hypotheses and analysis plans and performed with data analysts masked to experimental group assignments. By improving the reproducibility of published evidence, internal replication should contribute to more rapid scientific advances.},
  journal = {Gates Open Research},
  language = {en}
}

@article{bergh_there_2017,
  title = {Is There a Credibility Crisis in Strategic Management Research? {{Evidence}} on the Reproducibility of Study Findings},
  shorttitle = {Is There a Credibility Crisis in Strategic Management Research?},
  author = {Bergh, Donald D and Sharp, Barton M and Aguinis, Herman and Li, Ming},
  year = {2017},
  month = aug,
  volume = {15},
  pages = {423--436},
  publisher = {{SAGE Publications}},
  issn = {1476-1270},
  doi = {10.1177/1476127017701076},
  abstract = {Recent studies report an inability to replicate previously published research, leading some to suggest that scientific knowledge is facing a credibility crisis. In this essay, we provide evidence on whether strategic management research may itself be vulnerable to these concerns. We conducted a study whereby we attempted to reproduce the empirical findings of 88 articles appearing in the Strategic Management Journal using data reported in the articles themselves. About 70\% of the studies did not disclose enough data to permit independent tests of reproducibility of their findings. Of those that could be retested, almost one-third reported hypotheses as statistically significant which were no longer so and far more significant results were found to be non-significant in the reproductions than in the opposite direction. Collectively, incomplete reporting practices, disclosure errors, and possible opportunism limit the reproducibility of most studies. Until disclosure standards and requirements change to include more complete reporting and facilitate tests of reproducibility, the strategic management field appears vulnerable to a credibility crisis.},
  journal = {Strategic Organization},
  keywords = {crisis,knowledge credibility,replication,reproducibility},
  language = {en},
  number = {3}
}

@article{bishop_Rein_2019,
  title = {Rein in the Four Horsemen of Irreproducibility},
  author = {Bishop, Dorothy},
  year = {2019},
  month = apr,
  volume = {568},
  pages = {435--435},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-01307-2},
  abstract = {Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.},
  copyright = {2021 Nature},
  journal = {Nature},
  keywords = {forrt},
  language = {en},
  number = {7753}
}

@article{bowers_How_2016,
  title = {How to Improve Your Relationship with Your Future Self},
  author = {Bowers, Jake and Voors, Maarten},
  year = {2016},
  month = dec,
  volume = {36},
  pages = {829--848},
  issn = {0718-090X},
  doi = {10.4067/S0718-090X2016000300011},
  journal = {Revista de ciencia pol\'itica (Santiago)},
  number = {3}
}

@article{breznau_Does_2021,
  title = {Does {{Sociology Need Open Science}}?},
  author = {Breznau, Nate},
  year = {2021},
  month = mar,
  volume = {11},
  pages = {9},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/soc11010009},
  abstract = {Reliability, transparency, and ethical crises pushed many social science disciplines toward dramatic changes, in particular psychology and more recently political science. This paper discusses why sociology should also change. It reviews sociology as a discipline through the lens of current practices, definitions of sociology, positions of sociological associations, and a brief consideration of the arguments of three highly influential yet epistemologically diverse sociologists: Weber, Merton, and Habermas. It is a general overview for students and sociologists to quickly familiarize themselves with the state of sociology or explore the idea of open science and its relevance to their discipline.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  journal = {Societies},
  keywords = {crisis of science,Habermas,Merton,open science,p-hacking,publication bias,replication,research ethics,revisado,science community,sociology legitimation,transparency,Weber},
  language = {en},
  number = {1}
}

@article{breznau_Observing_2021,
  title = {Observing {{Many Researchers Using}} the {{Same Data}} and {{Hypothesis Reveals}} a {{Hidden Universe}} of {{Uncertainty}}},
  author = {Breznau, Nate and Rinke, Eike Mark and Wuttke, Alexander and Adem, Muna and Adriaans, Jule and {Alvarez-Benjumea}, Amalia and Andersen, Henrik Kenneth and Auer, Daniel and Azevedo, Flavio and Bahnsen, Oke and Balzer, Dave and Bauer, Gerrit and Bauer, Paul C. and Baumann, Markus and Baute, Sharon and Benoit, Verena and Bernauer, Julian and Berning, Carl and Berthold, Anna and Bethke, Felix and Biegert, Thomas and Blinzler, Katharina and Blumenberg, Johannes and Bobzien, Licia and Bohman, Andrea and Bol, Thijs and Bostic, Amie and Brzozowska, Zuzanna and Burgdorf, Katharina and Burger, Kaspar and Busch, Kathrin and Castillo, Juan Carlos and Chan, Nathan and Christmann, Pablo and Connelly, Roxanne and Czymara, Christian S. and Damian, Elena and Ecker, Alejandro and Edelmann, Achim and Eger, Maureen A. and Ellerbrock, Simon and Forke, Anna and Forster, Andrea and Gaasendam, Chris and Gavras, Konstantin and Gayle, Vernon and Gessler, Theresa and Gnambs, Timo and Godefroidt, Am{\'e}lie and Gr{\"o}mping, Max and Gro{\ss}, Martin and Gruber, Stefan and Gummer, Tobias and Hadjar, Andreas and Heisig, Jan Paul and Hellmeier, Sebastian and Heyne, Stefanie and Hirsch, Magdalena and Hjerm, Mikael and Hochman, Oshrat and H{\"o}vermann, Andreas and Hunger, Sophia and Hunkler, Christian and Huth, Nora and Ignacz, Zsofia and Jacobs, Laura and Jacobsen, Jannes and Jaeger, Bastian and Jungkunz, Sebastian and Jungmann, Nils and Kauff, Mathias and Kleinert, Manuel and Klinger, Julia and Kolb, Jan-Philipp and Ko{\l}czy{\'n}ska, Marta and Kuk, John Seungmin and Kuni{\ss}en, Katharina and Sinatra, Dafina Kurti and Greinert, Alexander and Lersch, Philipp M. and L{\"o}bel, Lea-Maria and Lutscher, Philipp and Mader, Matthias and Madia, Joan and Malancu, Natalia and Maldonado, Luis and Marahrens, Helge and Martin, Nicole and Martinez, Paul and Mayerl, Jochen and Mayorga, Oscar Jose and McManus, Patricia and Wagner, Kyle and Meeusen, Cecil and Meierrieks, Daniel and Mellon, Jonathan and Merhout, Friedolin and Merk, Samuel and Meyer, Daniel and Micheli, Leticia and Mijs, Jonathan J. B. and Moya, Crist{\'o}bal and Neunhoeffer, Marcel and N{\"u}st, Daniel and Nyg{\aa}rd, Olav and Ochsenfeld, Fabian and Otte, Gunnar and Pechenkina, Anna and Prosser, Christopher and Raes, Louis and Ralston, Kevin and Ramos, Miguel and Roets, Arne and Rogers, Jonathan and Ropers, Guido and Samuel, Robin and Sand, Gregor and Schachter, Ariela and Schaeffer, Merlin and Schieferdecker, David and Schlueter, Elmar and Schmidt, Katja M. and Schmidt, Regine and {Schmidt-Catran}, Alexander and Schmiedeberg, Claudia and Schneider, J{\"u}rgen and Schoonvelde, Martijn and {Schulte-Cloos}, Julia and Schumann, Sandy and Schunck, Reinhard and Schupp, J{\"u}rgen and Seuring, Julian and Silber, Henning and Sleegers, Willem and Sonntag, Nico and Staudt, Alexander and Steiber, Nadia and Steiner, Nils and Sternberg, Sebastian and Stiers, Dieter and Stojmenovska, Dragana and Storz, Nora and Striessnig, Erich and Stroppe, Anne-Kathrin and Teltemann, Janna and Tibajev, Andrey and Tung, Brian B. and Vagni, Giacomo and Assche, Jasper Van and van der Linden, Meta and van der Noll, Jolanda and Hootegem, Arno Van and Vogtenhuber, Stefan and Voicu, Bogdan and Wagemans, Fieke and Wehl, Nadja and Werner, Hannah and Wiernik, Brenton M. and Winter, Fabian and Wolf, Christof and Yamada, Yuki and Zhang, Nan and Ziller, Conrad and Zins, Stefan and {\.Z}{\'o}{\l}tak, Tomasz and Nguyen, Hung H. V.},
  year = {2021},
  month = mar,
  publisher = {{MetaArXiv}},
  doi = {10.31222/osf.io/cd5j9},
  abstract = {How does noise generated by researcher decisions undermine the credibility of science? We test this by observing all decisions made among 73 research teams as they independently conduct studies on the same hypothesis with identical starting data. We find excessive variation of outcomes. When combined, the 107 observed research decisions taken across teams explained at most 2.6\% of the total variance in effect sizes and 10\% of the deviance in subjective conclusions. Expertise, prior beliefs and attitudes of the researchers explain even less. Each model deployed to test the hypothesis was unique, which highlights a vast universe of research design variability that is normally hidden from view and suggests humility when presenting and interpreting scientific findings.},
  keywords = {Analytical Flexibility,Crowdsourced Replication Initiative,Crowdsourcing,Economics,Garden of Forking Paths,Immigration,Many Analysts,Meta-Science,Noise,Other Social and Behavioral Sciences,Political Science,Psychology,Researcher Degrees of Freedom,Researcher Variability,Social and Behavioral Sciences,Social Policy,Sociology}
}

@misc{breznau_Open_,
  title = {Open Science in Sociology. {{What}}, Why and Now.},
  author = {Breznau, Nate},
  abstract = {WHAT By now you've heard the term ``open science''. Although it has no global definition, its advocates tend toward certain agreements. Most definitions focus on the practical aspects of accessibility. ``\ldots the practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely \ldots{} Continue reading Open science in sociology. What, why and now.},
  journal = {Crowdid},
  language = {en-US},
  type = {Billet}
}

@techreport{brodeur_Methods_2018,
  title = {Methods {{Matter}}: {{P}}-{{Hacking}} and {{Causal Inference}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = {2018},
  month = aug,
  institution = {{Institute of Labor Economics (IZA)}},
  abstract = {The economics 'credibility revolution' has promoted the identification of causal relationships using difference-in-differences (DID), instrumental variables (IV), randomized control trials (RCT) and regression discontinuity design (RDD) methods. The extent to which a reader should trust claims about the statistical significance of results proves very sensitive to method. Applying multiple methods to 13,440 hypothesis tests reported in 25 top economics journals in 2015, we show that selective publication and p-hacking is a substantial problem in research employing DID and (in particular) IV. RCT and RDD are much less problematic. Almost 25\% of claims of marginally significant results in IV papers are misleading.},
  keywords = {causal inference,p-curves,p-hacking,practices,publication bias,research methods},
  number = {11796},
  type = {{{IZA Discussion Paper}}}
}

@article{brodeur_Star_2016,
  title = {Star {{Wars}}: {{The Empirics Strike Back}}},
  shorttitle = {Star {{Wars}}},
  author = {Brodeur, Abel and L{\'e}, Mathias and Sangnier, Marc and Zylberberg, Yanos},
  year = {2016},
  month = jan,
  volume = {8},
  pages = {1--32},
  issn = {1945-7782},
  doi = {10.1257/app.20150044},
  abstract = {Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing "significant" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)},
  journal = {American Economic Journal: Applied Economics},
  keywords = {Market for Economists; Estimation: General,Role of Economics,Role of Economists},
  language = {en},
  number = {1}
}

@article{burlig_Improving_2018,
  title = {Improving Transparency in Observational Social Science Research: {{A}} Pre-Analysis Plan Approach},
  shorttitle = {Improving Transparency in Observational Social Science Research},
  author = {Burlig, Fiona},
  year = {2018},
  month = jul,
  volume = {168},
  pages = {56--60},
  issn = {0165-1765},
  doi = {10.1016/j.econlet.2018.03.036},
  abstract = {Social science research has undergone a credibility revolution, but these gains are at risk due to problematic research practices. Existing research on transparency has centered around randomized controlled trials, which constitute only a small fraction of research in economics. In this paper, I highlight three scenarios in which study preregistration can be credibly applied in non-experimental settings: cases where researchers collect their own data; prospective studies; and research using restricted-access data.},
  journal = {Economics Letters},
  keywords = {Confidential data,Observational research,Pre-registration,Transparency},
  language = {en}
}

@article{button_Power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  volume = {14},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  journal = {Nature Reviews Neuroscience},
  keywords = {practices},
  language = {en},
  number = {5}
}

@article{byington_Solutions_2017,
  title = {Solutions to the {{Credibility Crisis}} in {{Management Science}}},
  author = {Byington, Eliza and Felps, Will},
  year = {2017},
  month = mar,
  volume = {16},
  pages = {142--162},
  doi = {10.5465/amle.2015.0035},
  abstract = {This article argues much academic misconduct can be explained as the result of social dilemmas occurring at two levels of Management science. First, the career benefits associated with engaging in Noncredible Research Practices (NCRPs) (e.g. data manipulation, fabricating results, data hoarding, undisclosed HARKing) result in many academics choosing self-interest over collective welfare. These perverse incentives derive from journal gatekeepers who are pressed into a similar social dilemma. Namely, an individual journal's status (i.e. its ``impact factor'') is likely to suffer from unilaterally implementing practices that help ensure the credibility of Management science claims (e.g. dedicating journal space to strict replications, crowd-sourcing replications, data submission requirements, in-house analysis checks, registered reports, Open Practice badges). Fortunately, research on social dilemmas and collective action offers solutions. For example, journal editors could pledge to publish a certain number of credibility boosting articles contingent on a proportion of their ``peer'' journals doing the same. Details for successful implementation of conditional pledges, other social dilemma solutions \textendash{} including actions for Management academics who support changes in journal practices (e.g. reviewer boycotts / buycotts), and insights on credibility supportive journal practices from other fields are provided.},
  journal = {Academy of Management Learning and Education, The},
  keywords = {crisis}
}

@article{caldwell_Moving_2020,
  title = {Moving {{Sport}} and {{Exercise Science Forward}}: {{A Call}} for the {{Adoption}} of {{More Transparent Research Practices}}},
  shorttitle = {Moving {{Sport}} and {{Exercise Science Forward}}},
  author = {Caldwell, Aaron R. and Vigotsky, Andrew D. and Tenan, Matthew S. and Radel, R{\'e}mi and Mellor, David T. and Kreutzer, Andreas and Lahart, Ian M. and Mills, John P. and Boisgontier, Matthieu P. and Boardley, Ian and Bouza, Brooke and Cheval, Boris and Chow, Zad Rafi and Contreras, Bret and Dieter, Brad and Halperin, Israel and Haun, Cody and Knudson, Duane and Lahti, Johan and Miller, Matthew and Morin, Jean-Benoit and Naughton, Mitchell and Neva, Jason and Nuckols, Greg and Peters, Sue and Roberts, Brandon and {Rosa-Caldwell}, Megan and Schmidt, Julia and Schoenfeld, Brad J. and Severin, Richard and Skarabot, Jakob and Steele, James and Twomey, Rosie and Zenko, Zachary and Lohse, Keith R. and Nunan, David and {Consortium for Transparency in Exercise Science (COTES) Collaborators}},
  year = {2020},
  month = mar,
  volume = {50},
  pages = {449--459},
  issn = {1179-2035},
  doi = {10.1007/s40279-019-01227-1},
  abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportR\$\$\textbackslash chi \$\$iv: https://osf.io/preprints/sportrxiv/fxe7a/.},
  journal = {Sports Medicine},
  keywords = {transparency},
  language = {en},
  number = {3}
}

@article{camerer_Evaluating_2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  volume = {2},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash 36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  journal = {Nature Human Behaviour},
  keywords = {crisis},
  language = {en},
  number = {9}
}

@article{campbell_Enhancing_2014,
  title = {Enhancing Transparency of the Research Process to Increase Accuracy of Findings: {{A}} Guide for Relationship Researchers},
  shorttitle = {Enhancing Transparency of the Research Process to Increase Accuracy of Findings},
  author = {Campbell, Lorne and Loving, Timothy J. and Lebel, Etienne P.},
  year = {2014},
  volume = {21},
  pages = {531--545},
  issn = {1475-6811},
  doi = {10.1111/pere.12053},
  abstract = {The purpose of this paper is to extend to the field of relationship science, recent discussions and suggested changes in open research practises. We demonstrate different ways that greater transparency of the research process in our field will accelerate scientific progress by increasing accuracy of reported research findings. Importantly, we make concrete recommendations for how relationship researchers can transition to greater disclosure of research practices in a manner that is sensitive to the unique design features of methodologies employed by relationship scientists. We discuss how to implement these recommendations for four different research designs regularly used in relationship research and practical limitations regarding implementing our recommendations and provide potential solutions to these problems.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/pere.12053},
  journal = {Personal Relationships},
  language = {en},
  number = {4}
}

@article{card_Role_2011,
  title = {The {{Role}} of {{Theory}} in {{Field Experiments}}},
  author = {Card, David and DellaVigna, Stefano and Malmendier, Ulrike},
  year = {2011},
  month = sep,
  volume = {25},
  pages = {39--62},
  issn = {0895-3309},
  doi = {10.1257/jep.25.3.39},
  abstract = {studies that estimate structural parameters in a completely specified model. We also classify laboratory experiments published in these journals over the same period and find that economic theory has played a more central role in the laboratory than in the field. Finally, we discuss in detail three sets of field experiments\textemdash on gift exchange, on charitable giving, and on negative income tax\textemdash that illustrate both the benefits and the potential costs of a tighter link between experimental design and theoretical underpinnings.},
  journal = {Journal of Economic Perspectives},
  keywords = {Field Experiments},
  language = {en},
  number = {3}
}

@article{carey_Fraud_2011,
  title = {Fraud {{Case Seen}} as a {{Red Flag}} for {{Psychology Research}}},
  author = {Carey, Benedict},
  year = {2011},
  month = nov,
  issn = {0362-4331},
  abstract = {A Dutch scholar was found to have falsified findings in dozens of papers, in a field that critics say is vulnerable to such abuses.},
  chapter = {Health},
  journal = {The New York Times},
  keywords = {Falsification of Data,Frauds and Swindling,Psychology and Psychologists,Research,Stapel; Diederik},
  language = {en-US}
}

@article{carrier_Facing_2017,
  title = {Facing the {{Credibility Crisis}} of {{Science}}: {{On}} the {{Ambivalent Role}} of {{Pluralism}} in {{Establishing Relevance}} and {{Reliability}}},
  shorttitle = {Facing the {{Credibility Crisis}} of {{Science}}},
  author = {Carrier, Martin},
  year = {2017},
  month = may,
  volume = {25},
  pages = {439--464},
  issn = {1063-6145},
  doi = {10.1162/POSC_a_00249},
  abstract = {Science at the interface with society is regarded with mistrust among parts of the public. Scientific judgments on matters of practical concern are not infrequently suspected of being incompetent and biased. I discuss two proposals for remedying this deficiency. The first aims at strengthening the independence of science and suggests increasing the distance to political and economic powers. The drawback is that this runs the risk of locking science in an academic ivory tower. The second proposal favors ``counter-politicization'' in that research is strongly focused on projects ``in the public interest,'' that is, on projects whose expected results will benefit all those concerned by these results. The disadvantage is that the future use of research findings cannot be delineated reliably in advance. I argue that the underlying problem is the perceived lack of relevance and reliability and that pluralism is an important step toward its solution. Pluralism serves to stimulate a more inclusive research agenda and strengthens the well-testedness of scientific approaches. However, pluralism also prevents the emergence of clear-cut practical suggestions. Accordingly, pluralism is part of the solution to the credibility crisis of science, but also part of the problem. In order for science to be suitable as a guide for practice, the leeway of scientific options needs to be narrowed \textendash{} in spite of uncertainty in epistemic respect. This reduction can be achieved by appeal to criteria that do not focus on the epistemic credentials of the suggestions but on their appropriateness in practical respect.},
  journal = {Perspectives on Science},
  keywords = {crisis},
  number = {4}
}

@article{chambers_Registered_2013,
  title = {Registered {{Reports}}: {{A}} New Publishing Initiative at~{{Cortex}}},
  shorttitle = {Registered {{Reports}}},
  author = {Chambers, Christopher D.},
  year = {2013},
  month = mar,
  volume = {49},
  pages = {609--610},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2012.12.016},
  journal = {Cortex},
  keywords = {forrt,reports},
  language = {en},
  number = {3}
}

@misc{chambers_Registered_2014,
  title = {Registered {{Reports}}: {{A}} Step Change in Scientific Publishing},
  author = {Chambers, Christopher D.},
  year = {2014},
  abstract = {Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept's founders, on how the initiative combats publication bias},
  howpublished = {https://www.elsevier.com/connect/reviewers-update/registered-reports-a-step-change-in-scientific-publishing},
  journal = {Reviewers' Update},
  keywords = {forrt,reports},
  language = {en}
}

@article{chambers_Registered_2015,
  title = {Registered Reports: Realigning Incentives in Scientific Publishing},
  shorttitle = {Registered Reports},
  author = {Chambers, Christopher D. and Dienes, Zoltan and McIntosh, Robert D. and Rotshtein, Pia and Willmes, Klaus},
  year = {2015},
  month = may,
  volume = {66},
  pages = {A1-2},
  issn = {1973-8102},
  doi = {10.1016/j.cortex.2015.03.022},
  journal = {Cortex; a Journal Devoted to the Study of the Nervous System and Behavior},
  keywords = {Biomedical Research,Editorial Policies,forrt,Humans,Motivation,Peer Review; Research,Publication Bias,Publishing,reports,Reproducibility of Results},
  language = {eng},
  pmid = {25892410}
}

@article{christensen_Transparency_2018,
  title = {Transparency, {{Reproducibility}}, and the {{Credibility}} of {{Economics Research}}},
  author = {Christensen, Garret and Miguel, Edward},
  year = {2018},
  month = sep,
  volume = {56},
  pages = {920--980},
  issn = {0022-0515},
  doi = {10.1257/jel.20171350},
  abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
  journal = {Journal of Economic Literature},
  keywords = {Market for Economists; Methodological Issues: General; Higher Education,Research Institutions,Role of Economics,Role of Economists},
  language = {en},
  number = {3}
}

@book{christensen_Transparent_2019,
  title = {Transparent and Reproducible Social Science Research: How to Do Open Science},
  shorttitle = {Transparent and Reproducible Social Science Research},
  author = {Christensen, Garret S. and Freese, Jeremy and Miguel, Edward},
  year = {2019},
  publisher = {{University of California Press}},
  address = {{Oakland, California}},
  abstract = {"Social science practitioners have recently witnessed numerous episodes of influential research that fell apart upon close scrutiny. These instances have spurred suspicions that other published results may contain errors or may at least be less robust than they appear. In response, an influential movement has emerged across the social sciences for greater research transparency, openness, and reproducibility. Transparent and Reproducible Social Science Research crystallizes the new insights, practices, and methods of this rising interdisciplinary field"--Provided by publisher},
  isbn = {978-0-520-96923-0},
  keywords = {Reproducible research,Research,Social sciences,transparency},
  lccn = {Q180.55.S7}
}

@article{chubin_Open_1985,
  title = {Open {{Science}} and {{Closed Science}}: {{Tradeoffs}} in a {{Democracy}}},
  shorttitle = {Open {{Science}} and {{Closed Science}}},
  author = {Chubin, Daryl E.},
  year = {1985},
  month = apr,
  volume = {10},
  pages = {73--80},
  publisher = {{SAGE Publications Inc}},
  issn = {0162-2439},
  doi = {10.1177/016224398501000211},
  journal = {Science, Technology, \& Human Values},
  language = {en},
  number = {2}
}

@article{cruwell_Easy_2018,
  title = {7 {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {7 {{Easy Steps}} to {{Open Science}}},
  author = {Cr{\"u}well, Sophia and van Doorn, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse and Orben, Amy and Parsons, Sam and {Schulte-Mecklenbeck}, Michael},
  year = {2018},
  month = nov,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/cfzyx},
  abstract = {The Open Science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz et al. (2018). Written for researchers and students - particularly in psychological science - it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  keywords = {Meta-science,Meta-Science,Open Access,Open Science,other,Psychology,Reproducibility,revisado,Social and Behavioral Sciences,Transparency}
}

@article{dal-re_Making_2014,
  title = {Making {{Prospective Registration}} of {{Observational Research}} a {{Reality}}},
  author = {{Dal-R{\'e}}, Rafael and Ioannidis, John P. and Bracken, Michael B. and Buffler, Patricia A. and Chan, An-Wen and Franco, Eduardo L. and Vecchia, Carlo La and Weiderpass, Elisabete},
  year = {2014},
  month = feb,
  volume = {6},
  pages = {224cm1-224cm1},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.3007513},
  abstract = {The vast majority of health-related observational studies are not prospectively registered and the advantages of registration have not been fully appreciated. Nonetheless, international standards require approval of study protocols by an independent ethics committee before the study can begin. We suggest that there is an ethical and scientific imperative to publicly preregister key information from newly approved protocols, which should be required by funders. Ultimately, more complete information may be publicly available by disclosing protocols, analysis plans, data sets, and raw data. Key information about human observational studies should be publicly available before the study is initiated. Key information about human observational studies should be publicly available before the study is initiated.},
  chapter = {Commentary},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  journal = {Science Translational Medicine},
  keywords = {reports},
  language = {en},
  number = {224},
  pmid = {24553383}
}

@article{derond_Publish_2005,
  title = {Publish or {{Perish}}: {{Bane}} or {{Boon}} of {{Academic Life}}?},
  shorttitle = {Publish or {{Perish}}},
  author = {De Rond, Mark and Miller, Alan N.},
  year = {2005},
  month = dec,
  volume = {14},
  pages = {321--329},
  publisher = {{SAGE Publications Inc}},
  issn = {1056-4926},
  doi = {10.1177/1056492605276850},
  abstract = {There are few more familiar aphorisms in the academic community than ``publish or perish.'' Venerated by many and dreaded by more, this phenomenon is the subject of the authors' essay. Here they consider the publish or perish principle that has come to characterize life at many business schools. They explain when and why it began and suggest reasons for its persistence. This exercise elicits questions that appear as relatively neglected but are integral to our profession, namely, the effect of publish or perish on the creativity, intellectual lives, morale, and psychological and emotional states of faculty.},
  journal = {Journal of Management Inquiry},
  keywords = {business schools,institutional,publish,research,tenure},
  language = {en},
  number = {4}
}

@article{diaz_Mala_2018,
  title = {{Mala conducta cient\'ifica en la publicaci\'on}},
  author = {D{\'i}az, Rosa Mar{\'i}a Lam},
  year = {2018},
  month = jan,
  volume = {34},
  issn = {1561-2996},
  abstract = {La publicaci\'on en revistas cient\'ificas constituye la forma m\'as aceptada para validar una investigaci\'on debido a que pasa por un riguroso proceso de revisi\'on por expertos, que deciden entre lo publicable y lo no publicable con vista a garantizar la calidad de los trabajos. A pesar de esto con frecuencia aparecen pr\'acticas incorrectas relacionadas con la \'etica durante la publicaci\'on, que se conocen como mala conducta cient\'ifica. Las manifestaciones de mala conducta cient\'ifica van desde el fraude cient\'ifico hasta una variedad de faltas que se cometen en el proceso de publicaci\'on. El fraude cient\'ifico incluye la invenci\'on, la falsificaci\'on y el plagio. Las faltas en el proceso de publicaci\'on incluyen la autor\'ia ficticia, la autor\'ia fantasma, la publicaci\'on duplicada, la publicaci\'on fragmentada o publicaci\'on salami, la publicaci\'on inflada, el autoplagio, la incorrecci\'on de citas bibliogr\'aficas, los sesgos de publicaci\'on y la publicaci\'on anticipada.},
  copyright = {Copyright (c) 2018 Revista Cubana de Hematolog\'ia, Inmunolog\'ia y Hemoterapia},
  journal = {Revista Cubana de Hematolog\'ia, Inmunolog\'ia y Hemoterapia},
  keywords = {autoría,conflicto de intereses,ética,fraude científico,investigación,plagio,practices,publicación},
  language = {es},
  number = {1}
}

@article{dutilh_Seven_2016,
  title = {Seven {{Selfish Reasons}} for {{Preregistration}}},
  author = {Dutilh, Eric-Jan Wagenmakers {and} Gilles},
  year = {2016},
  month = oct,
  volume = {29},
  abstract = {Psychological scientists Eric-Jan Wagenmakers and Gilles Dutilh present an illustrated guide to the career benefits of submitting your research plans before beginning your data collection.},
  journal = {APS Observer},
  language = {en-US},
  number = {9}
}

@article{earth_Reproducibility_2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {on Earth, Division and on Behavioral, Cognitive Board},
  year = {2019},
  abstract = {Semantic Scholar extracted view of \&quot;Reproducibility and Replicability in Science\&quot; by Division on Earth et al.},
  journal = {undefined},
  language = {en}
}

@article{editors_Observational_2014,
  title = {Observational {{Studies}}: {{Getting Clear}} about {{Transparency}}},
  shorttitle = {Observational {{Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2014},
  month = aug,
  volume = {11},
  pages = {e1001711},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001711},
  abstract = {The PLOS Medicine Editors endorse four measures to ensure transparency in the analysis and reporting of observational studies. Please see later in the article for the Editors' Summary},
  journal = {PLOS Medicine},
  keywords = {Clinical trial reporting,Clinical trials,Cross-sectional studies,Health care policy,Medical journals,Observational studies,Research reporting guidelines,Science policy},
  language = {en},
  number = {8}
}

@article{editors_Observational_2014a,
  title = {Observational {{Studies}}: {{Getting Clear}} about {{Transparency}}},
  shorttitle = {Observational {{Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2014},
  month = aug,
  volume = {11},
  pages = {e1001711},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001711},
  abstract = {The PLOS Medicine Editors endorse four measures to ensure transparency in the analysis and reporting of observational studies. Please see later in the article for the Editors' Summary},
  journal = {PLOS Medicine},
  keywords = {Clinical trial reporting,Clinical trials,Cross-sectional studies,Health care policy,Medical journals,Observational studies,Research reporting guidelines,Science policy},
  language = {en},
  number = {8}
}

@article{editors_Transparency_2015,
  title = {Transparency in {{Reporting Observational Studies}}: {{Reflections}} after a {{Year}}},
  shorttitle = {Transparency in {{Reporting Observational Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2015},
  month = oct,
  volume = {12},
  pages = {e1001896},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001896},
  abstract = {The PLOS Medicine Editors take stock of changes in the reporting of observational studies following our new transparency guidelines from August 2014.},
  journal = {PLOS Medicine},
  keywords = {Cohort studies,Diagnostic medicine,Health care policy,Observational studies,Open access medical journals,Peer review,Reflection,Water resources},
  language = {en},
  number = {10}
}

@article{elliott_Taxonomy_2020,
  title = {A {{Taxonomy}} of {{Transparency}} in {{Science}}},
  author = {Elliott, Kevin C.},
  year = {2020},
  pages = {1--14},
  publisher = {{Cambridge University Press}},
  issn = {0045-5091, 1911-0820},
  doi = {10.1017/can.2020.21},
  abstract = {Both scientists and philosophers of science have recently emphasized the importance of promoting transparency in science. For scientists, transparency is a way to promote reproducibility, progress, and trust in research. For philosophers of science, transparency can help address the value-ladenness of scientific research in a responsible way. Nevertheless, the concept of transparency is a complex one. Scientists can be transparent about many different things, for many different reasons, on behalf of many different stakeholders. This paper proposes a taxonomy that clarifies the major dimensions along which approaches to transparency can vary. By doing so, it provides several insights that philosophers and other science studies scholars can pursue. In particular, it helps address common objections to pursuing transparency in science, it clarifies major forms of transparency, and it suggests avenues for further research on this topic.},
  journal = {Canadian Journal of Philosophy},
  keywords = {herramienta,open science,research ethics,science communication,transparency,value judgments,values and science},
  language = {en}
}

@article{engzell_Improving_2020,
  title = {Improving {{Social Science}}: {{Lessons}} from the {{Open Science Movement}}},
  shorttitle = {Improving {{Social Science}}},
  author = {Engzell, Per and Rohrer, Julia M.},
  year = {2020},
  month = apr,
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/6whjt},
  abstract = {The transdisciplinary movement towards greater research transparency opens the door for a meta-scientific exchange between different social sciences. In the spirit of such an exchange, we offer some lessons inspired by ongoing debates in psychology, highlighting the broad benefits of open science but also potential pitfalls, as well as practical challenges in the implementation that have not yet been fully resolved. Our discussion is aimed towards political scientists but relevant for population sciences more broadly.},
  keywords = {credibility,meta-science,open science,replication,reproducibility,Social and Behavioral Sciences,transparency}
}

@article{fanelli_How_2009,
  title = {How {{Many Scientists Fabricate}} and {{Falsify Research}}? {{A Systematic Review}} and {{Meta}}-{{Analysis}} of {{Survey Data}}},
  shorttitle = {How {{Many Scientists Fabricate}} and {{Falsify Research}}?},
  author = {Fanelli, Daniele},
  year = {2009},
  month = may,
  volume = {4},
  pages = {e5738},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005738},
  abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, ``cooking'' of data, etc\ldots{} Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86\textendash 4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once \textendash a serious form of misconduct by any standard\textendash{} and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91\textendash 19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words ``falsification'' or ``fabrication'', and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
  journal = {PLOS ONE},
  keywords = {Deception,important,Medical journals,Medicine and health sciences,Metaanalysis,practices,Scientific misconduct,Scientists,Social research,Surveys},
  language = {en},
  number = {5}
}

@article{fanelli_Opinion_2018,
  title = {Opinion: {{Is}} Science Really Facing a Reproducibility Crisis, and Do We Need It To?},
  shorttitle = {Opinion},
  author = {Fanelli, Daniele},
  year = {2018},
  month = mar,
  volume = {115},
  pages = {2628--2631},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708272114},
  abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {crisis,forrt},
  language = {en},
  number = {11}
}

@inproceedings{fernandezmolina_Derechos_2018,
  title = {Derechos de Autor y Ciencia Abierta: El Papel de La Biblioteca Universitaria},
  shorttitle = {Derechos de Autor y Ciencia Abierta},
  booktitle = {{{VIII Conferencia Internacional}} Sobre {{Bibliotecas}} y {{Repositorios Digitales BIREDIAL}}-{{ISTEC}} ({{Lima}}, 2018)},
  author = {Fern{\'a}ndez Molina, Juan Carlos and Graziosi Silva, Eduardo and Mart{\'i}nez {\'A}vila, Daniel},
  year = {2018}
}

@incollection{fidler_Reproducibility_2021,
  title = {Reproducibility of {{Scientific Results}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Fidler, Fiona and Wilcox, John},
  editor = {Zalta, Edward N.},
  year = {2021},
  edition = {Summer 2021},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {The terms ``reproducibility crisis'' and ``replicationcrisis'' gained currency in conversation and in print over thelast decade (e.g., Pashler \& Wagenmakers 2012), as disappointingresults emerged from large scale reproducibility projects in variousmedical, life and behavioural sciences (e.g., Open ScienceCollaboration, OSC 2015). In 2016, a poll conducted by the journalNature reported that more than half (52\%) of scientistssurveyed believed science was facing a ``replicationcrisis'' (Baker 2016). More recently, some authors have moved tomore positive terms for describing this episode in science; forexample, Vazire (2018) refers instead to a ``credibilityrevolution'' highlighting the improved methods and open sciencepractices it has motivated., The crisis often refers collectively to at least the following things:, The associated open science reform movement aims to rectify conditionsthat led to the crisis. This is done by promoting activities such asdata sharing and public pre-registration of studies, and by advocatingstricter editorial policies around statistical reporting includingpublishing replication studies and statistically non-significantresults., This review consists of four distinct parts. First, we look at theterm ``reproducibility'' and related terms like``repeatability'' and ``replication'', presentingsome definitions and conceptual discussion about the epistemicfunction of different types of replication studies. Second, wedescribe the meta-science research that has established andcharacterised the reproducibility crisis, including large scalereplication projects and surveys of questionable research practices invarious scientific communities. Third, we look at attempts to addressepistemological questions about the limitations of replication, andwhat value it holds for scientific inquiry and the accumulation ofknowledge. The fourth and final part describes some of the manyinitiatives the open science reform movement has proposed (and in manycases implemented) to improve reproducibility in science. In addition,we reflect there on the values and norms which those reforms embody,noting their relevance to the debate about the role of values in thephilosophy of science.}
}

@incollection{figueiredo_Data_2020,
  title = {Data {{Collection With Indigenous People}}: {{Fieldwork Experiences From Chile}}},
  shorttitle = {Data {{Collection With Indigenous People}}},
  booktitle = {Researching {{Peace}}, {{Conflict}}, and {{Power}} in the {{Field}}: {{Methodological Challenges}} and {{Opportunities}}},
  author = {Figueiredo, Ana and Rocha, Carolina and Montagna, Pietro},
  editor = {Acar, Yasemin G{\"u}ls{\"u}m and Moss, Sigrun Marie and Ulu{\u g}, {\"O}zden Melis},
  year = {2020},
  pages = {105--127},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-44113-5_7},
  abstract = {At present, the Mapuche are the largest indigenous group living in Chile and, up until the present day, they are considered a disadvantaged group in Chilean society in terms of poverty, education and discrimination indicators. In recent decades, this group has been involved in a violent conflict with the Chilean state, forestry and hydroelectric industries and big landowners due mainly to territorial claims of the ancestral land that is currently inhabited and exploited by these different actors. In the present chapter, we narrate the process of data collection with indigenous participants within the framework of a three-year long project about representations of history and present-day intergroup relations between the Mapuche and the non-indigenous majority in Chile. We focus on the challenges that data collection involved by highlighting the process of participant recruitment and trust issues revolving around data collection, as well as retribution practices. Moreover, we also highlight the pros and cons of having non-indigenous Chilean and international researchers conducting fieldwork in this context. Another aspect we address is how methodological approaches may influence the data quality and participants' degree of involvement with the project, by highlighting how these issues interconnect with cultural differences and this indigenous group's worldview and cultural practices. We hope this chapter may provide significant insights on how to deal with some of the difficulties that data collection with indigenous people may involve.},
  isbn = {978-3-030-44113-5},
  keywords = {Chile,Fieldwork,Mapuche,Qualitative research,Quantitative research},
  language = {en},
  series = {Peace {{Psychology Book Series}}}
}

@article{figueiredo_Groupbased_2015,
  title = {Group-Based {{Compunction}} and {{Anger}}: {{Their Antecedents}} and {{Consequences}} in {{Relation}} to {{Colonial Conflicts}}},
  shorttitle = {Group-Based {{Compunction}} and {{Anger}}},
  author = {Figueiredo, Ana and Doosje, Bertjan and Valentim, Joaquim Pires},
  year = {2015},
  volume = {9},
  pages = {90--105},
  issn = {1864-1385},
  doi = {10.4119/ijcv-3070},
  copyright = {Copyright (c) 2016 International Journal of Conflict and Violence},
  journal = {International Journal of Conflict and Violence (IJCV)},
  language = {en}
}

@incollection{figueiredo_Representations_2019,
  title = {Representations of History and Present-Day Intergroup Relations between Indigenous and Non-Indigenous People: {{The Mapuche}} in {{Chile}}},
  shorttitle = {Representations of History and Present-Day Intergroup Relations between Indigenous and Non-Indigenous People},
  author = {Figueiredo, Ana and Rocha, Carolina and Montagna, Pietro and Ferreiro, Trinidad and Guerrero, Catarina and Varela O'Reilly, Micaela and Garc{\'i}a, Bernardita and Mu{\~n}oz, Loreto and Schmidt, Magdalena and Cornejo, Marcela and Licata, Laurent},
  year = {2019},
  month = jan,
  pages = {79--104},
  isbn = {978-1-5361-6164-9}
}

@article{figueiredo_Too_2015,
  title = {Too Long to Compensate? {{Time}} Perceptions, Emotions, and Compensation for Colonial Conflicts},
  shorttitle = {Too Long to Compensate?},
  author = {Figueiredo, Ana Mateus and Valentim, Joaquim Pires and Doosje, Bertjan},
  year = {2015},
  volume = {21},
  pages = {500--504},
  publisher = {{Educational Publishing Foundation}},
  address = {{US}},
  issn = {1532-7949(ELECTRONIC),1078-1919(PRINT)},
  doi = {10.1037/pac0000114},
  abstract = {In the present article we analyze the role of perceptions of time and ingroup-focused compunction and anger on the desire to compensate the outgroup in relation to historical colonial conflicts. Furthermore, we analyze the relationships between the aforementioned variables and perceptions of the past as being violent and perceptions that compensation has been enough. By means of multiple group structural equation modeling using 1 Portuguese sample (N = 170) and 1 Dutch sample (N = 238), we were able to show that perceptions of the time passed between the negative events and the present day are negatively related to compensatory behavioral intentions. Furthermore, the belief that past compensation has been enough is negatively related to ingroup-focused anger and compunction. Anger (Portuguese sample only) and compunction are positively associated with intentions of compensation. The implications of our results for the field of intergroup relations are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Peace and Conflict: Journal of Peace Psychology},
  keywords = {Conflict,Emotions,History,Ingroup Outgroup,Time Perception},
  number = {3}
}

@article{flier_Faculty_2017,
  title = {Faculty Promotion Must Assess Reproducibility},
  author = {Flier, Jeffrey},
  year = {2017},
  month = sep,
  volume = {549},
  pages = {133--133},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/549133a},
  abstract = {Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.},
  copyright = {2017 Nature Publishing Group},
  journal = {Nature},
  keywords = {forrt},
  language = {en},
  number = {7671}
}

@article{franca_Reproducibility_2019,
  title = {Reproducibility Crisis, the Scientific Method, and the Quality of Published Studies: {{Untangling}} the Knot},
  shorttitle = {Reproducibility Crisis, the Scientific Method, and the Quality of Published Studies},
  author = {Fran{\c c}a, Thiago F. A. and Monserrat, Jos{\'e} Maria},
  year = {2019},
  month = oct,
  volume = {32},
  pages = {406--408},
  issn = {0953-1513, 1741-4857},
  doi = {10.1002/leap.1250},
  journal = {Learned Publishing},
  keywords = {crisis},
  language = {en},
  number = {4}
}

@article{franco_Publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, A. and Malhotra, N. and Simonovits, G.},
  year = {2014},
  month = sep,
  volume = {345},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  journal = {Science},
  keywords = {practices},
  language = {en},
  number = {6203}
}

@article{freese_Replication_2017,
  title = {Replication in {{Social Science}}},
  author = {Freese, Jeremy and Peterson, David},
  year = {2017},
  month = jul,
  volume = {43},
  pages = {147--165},
  publisher = {{Annual Reviews}},
  issn = {0360-0572},
  doi = {10.1146/annurev-soc-060116-053450},
  abstract = {Across the medical and social sciences, new discussions about replication have led to transformations in research practice. Sociologists, however, have been largely absent from these discussions. The goals of this review are to introduce sociologists to these developments, synthesize insights from science studies about replication in general, and detail the specific issues regarding replication that occur in sociology. The first half of the article argues that a sociologically sophisticated understanding of replication must address both the ways that replication rules and conventions evolved within an epistemic culture and how those cultures are shaped by specific research challenges. The second half outlines the four main dimensions of replicability in quantitative sociology\textemdash verifiability, robustness, repeatability, and generalizability\textemdash and discusses the specific ambiguities of interpretation that can arise in each. We conclude by advocating some commonsense changes to promote replication while acknowledging the epistemic diversity of our field.},
  journal = {Annual Review of Sociology},
  number = {1}
}

@article{frey_Publishing_2003,
  title = {Publishing as {{Prostitution}}? \textendash{} {{Choosing Between One}}'s {{Own Ideas}} and {{Academic Success}}},
  shorttitle = {Publishing as {{Prostitution}}?},
  author = {Frey, Bruno S.},
  year = {2003},
  month = jul,
  volume = {116},
  pages = {205--223},
  issn = {1573-7101},
  doi = {10.1023/A:1024208701874},
  abstract = {Survival in academia depends on publications in refereedjournals. Authors only get their papers accepted if theyintellectually prostitute themselves by slavishly followingthe demands made by anonymous referees who have no propertyrights to the journals they advise. Intellectual prostitutionis neither beneficial to suppliers nor consumers. But it isavoidable. The editor (with property rights to the journal)should make the basic decision of whether a paper is worthpublishing or not. The referees should only offer suggestionsfor improvement. The author may disregard this advice. Thisreduces intellectual prostitution and produces more originalpublications.},
  journal = {Public Choice},
  keywords = {institutional},
  language = {en},
  number = {1}
}

@article{gall_credibility_2017,
  title = {The Credibility Crisis in Research: {{Can}} Economics Tools Help?},
  shorttitle = {The Credibility Crisis in Research},
  author = {Gall, Thomas and Ioannidis, John P. A. and Maniadis, Zacharias},
  year = {2017},
  month = apr,
  volume = {15},
  pages = {e2001846},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2001846},
  abstract = {The issue of nonreplicable evidence has attracted considerable attention across biomedical and other sciences. This concern is accompanied by an increasing interest in reforming research incentives and practices. How to optimally perform these reforms is a scientific problem in itself, and economics has several scientific methods that can help evaluate research reforms. Here, we review these methods and show their potential. Prominent among them are mathematical modeling and laboratory experiments that constitute affordable ways to approximate the effects of policies with wide-ranging implications.},
  journal = {PLOS Biology},
  keywords = {crisis,Economic models,Economics,Experimental economics,Game theory,Health economics,Labor economics,Mathematical modeling,Randomized controlled trials},
  language = {en},
  number = {4}
}

@article{gerber_Publication_2008,
  title = {Publication {{Bias}} in {{Empirical Sociological Research}}: {{Do Arbitrary Significance Levels Distort Published Results}}?},
  shorttitle = {Publication {{Bias}} in {{Empirical Sociological Research}}},
  author = {Gerber, Alan S. and Malhotra, Neil},
  year = {2008},
  month = aug,
  volume = {37},
  pages = {3--30},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124108318973},
  abstract = {Despite great attention to the quality of research methods in individual studies, if publication decisions of journals are a function of the statistical significance of research findings, the published literature as a whole may not produce accurate measures of true effects. This article examines the two most prominent sociology journals (the American Sociological Review and the American Journal of Sociology) and another important though less influential journal (The Sociological Quarterly) for evidence of publication bias. The effect of the .05 significance level on the pattern of published findings is examined using a ``caliper'' test, and the hypothesis of no publication bias can be rejected at approximately the 1 in 10 million level. Findings suggest that some of the results reported in leading sociology journals may be misleading and inaccurate due to publication bias. Some reasons for publication bias and proposed reforms to reduce its impact on research are also discussed.},
  journal = {Sociological Methods \& Research},
  keywords = {caliper test,hypothesis testing,meta-analysis,practices,publication bias},
  language = {en},
  number = {1}
}

@article{gerber_Statistical_2008,
  title = {Do {{Statistical Reporting Standards Affect What Is Published}}? {{Publication Bias}} in {{Two Leading Political Science Journals}}},
  shorttitle = {Do {{Statistical Reporting Standards Affect What Is Published}}?},
  author = {Gerber, Alan and Malhotra, Neil},
  year = {2008},
  month = oct,
  volume = {3},
  pages = {313--326},
  publisher = {{Now Publishers, Inc.}},
  issn = {1554-0626, 1554-0634},
  doi = {10.1561/100.00008024},
  abstract = {Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals},
  journal = {Quarterly Journal of Political Science},
  keywords = {practices},
  language = {English},
  number = {3}
}

@article{gilbert_Comment_2016,
  title = {Comment on "{{Estimating}} the Reproducibility of Psychological Science"},
  author = {Gilbert, D. T. and King, G. and Pettigrew, S. and Wilson, T. D.},
  year = {2016},
  month = mar,
  volume = {351},
  pages = {1037--1037},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aad7243},
  journal = {Science},
  language = {en},
  number = {6277}
}

@article{goodman_What_2016,
  title = {What Does Research Reproducibility Mean?},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2016},
  month = jun,
  volume = {8},
  pages = {341ps12-341ps12},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  abstract = {{$<$}p{$>$}The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ``truth.''{$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2016, American Association for the Advancement of Science},
  journal = {Science Translational Medicine},
  language = {en},
  number = {341},
  pmid = {27252173}
}

@article{guttinger_limits_2020,
  title = {The Limits of Replicability},
  author = {Guttinger, Stephan},
  year = {2020},
  month = jan,
  volume = {10},
  pages = {10},
  issn = {1879-4920},
  doi = {10.1007/s13194-019-0269-1},
  abstract = {Discussions about a replicability crisis in science have been driven by the normative claim that all of science should be replicable and the empirical claim that most of it isn't. Recently, such crisis talk has been challenged by a new localism, which argues a) that serious problems with replicability are not a general occurrence in science and b) that replicability itself should not be treated as a universal standard. The goal of this article is to introduce this emerging strand of the debate and to discuss some of its implications and limitations. I will in particular highlight the issue of demarcation that localist accounts have to address, i.e. the question of how we can distinguish replicable science from disciplines where replicability does not apply.},
  journal = {European Journal for Philosophy of Science},
  language = {en},
  number = {2}
}

@article{haven_Preregistering_2019,
  title = {Preregistering Qualitative Research},
  author = {Haven, Tamarinde L. and Grootel, Dr Leonie Van},
  year = {2019},
  month = apr,
  volume = {26},
  pages = {229--244},
  publisher = {{Taylor \& Francis}},
  issn = {0898-9621},
  doi = {10.1080/08989621.2019.1580147},
  abstract = {The threat to reproducibility and awareness of current rates of research misbehavior sparked initiatives to better academic science. One initiative is preregistration of quantitative research. We investigate whether the preregistration format could also be used to boost the credibility of qualitative research. A crucial distinction underlying preregistration is that between prediction and postdiction. In qualitative research, data are used to decide which way interpretation should move forward, using data to generate hypotheses and new research questions. Qualitative research is thus a real-life example of postdiction research. Some may object to the idea of preregistering qualitative studies because qualitative research generally does not test hypotheses, and because qualitative research design is typically flexible and subjective. We rebut these objections, arguing that making hypotheses explicit is just one feature of preregistration, that flexibility can be tracked using preregistration, and that preregistration would provide a check on subjectivity. We then contextualize preregistrations alongside another initiative to enhance credibility in qualitative research: the confirmability audit. Besides, preregistering qualitative studies is practically useful to combating dissemination bias and could incentivize qualitative researchers to report constantly on their study's development. We conclude with suggested modifications to the Open Science Framework preregistration form to tailor it for qualitative studies.},
  annotation = {\_eprint: https://doi.org/10.1080/08989621.2019.1580147},
  journal = {Accountability in Research},
  keywords = {Preregistration,qualitative research,reports,transparency},
  number = {3},
  pmid = {30741570}
}

@article{head_Extent_2015,
  title = {The {{Extent}} and {{Consequences}} of {{P}}-{{Hacking}} in {{Science}}},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  month = mar,
  volume = {13},
  pages = {e1002106},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002106},
  abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as ``p-hacking,'' occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  journal = {PLOS Biology},
  keywords = {Bibliometrics,Binomials,Medicine and health sciences,Metaanalysis,practices,Publication ethics,Reproducibility,Statistical data,Test statistics},
  language = {en},
  number = {3}
}

@article{hollenbeck_Harking_2017,
  title = {Harking, {{Sharking}}, and {{Tharking}}: {{Making}} the {{Case}} for {{Post Hoc Analysis}} of {{Scientific Data}}},
  shorttitle = {Harking, {{Sharking}}, and {{Tharking}}},
  author = {Hollenbeck, John R. and Wright, Patrick M.},
  year = {2017},
  month = jan,
  volume = {43},
  pages = {5--18},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206316679487},
  abstract = {In this editorial we discuss the problems associated with HARKing (Hypothesizing After Results Are Known) and draw a distinction between Sharking (Secretly HARKing in the Introduction section) and Tharking (Transparently HARKing in the Discussion section). Although there is never any justification for the process of Sharking, we argue that Tharking can promote the effectiveness and efficiency of both scientific inquiry and cumulative knowledge creation. We argue that the discussion sections of all empirical papers should include a subsection that reports post hoc exploratory data analysis. We explain how authors, reviewers, and editors can best leverage post hoc analyses in the spirit of scientific discovery in a way that does not bias parameter estimates and recognizes the lack of definitiveness associated with any single study or any single replication. We also discuss why the failure to Thark in high-stakes contexts where data is scarce and costly may also be unethical.},
  journal = {Journal of Management},
  keywords = {macro topics,micro topics,philosophy of science,practices,research design,research methods,statistical methods},
  language = {en},
  number = {1}
}

@misc{horgan_Psychology_,
  title = {Psychology's {{Credibility Crisis}}: The {{Bad}}, the {{Good}} and the {{Ugly}}},
  shorttitle = {Psychology's {{Credibility Crisis}}},
  author = {Horgan, John},
  doi = {10.1038/scientificamericanmind0716-18},
  abstract = {As more studies are called into question and researchers bicker over methodology, the field is showing a healthy willingness to face its problems\&nbsp;},
  howpublished = {https://www.scientificamerican.com/article/psychology-s-credibility-crisis-the-bad-the-good-and-the-ugly/},
  journal = {Scientific American},
  keywords = {crisis},
  language = {en}
}

@article{ioannidis_Power_2017,
  title = {The {{Power}} of {{Bias}} in {{Economics Research}}},
  author = {Ioannidis, John P. A. and Stanley, T. D. and Doucouliagos, Hristos},
  year = {2017},
  volume = {127},
  pages = {F236-F265},
  issn = {1468-0297},
  doi = {10.1111/ecoj.12461},
  abstract = {We investigate two critical dimensions of the credibility of empirical economics research: statistical power and bias. We survey 159 empirical economics literatures that draw upon 64,076 estimates of economic parameters reported in more than 6,700 empirical studies. Half of the research areas have nearly 90\% of their results under-powered. The median statistical power is 18\%, or less. A simple weighted average of those reported results that are adequately powered (power {$\geq$} 80\%) reveals that nearly 80\% of the reported effects in these empirical economics literatures are exaggerated; typically, by a factor of two and with one-third inflated by a factor of four or more.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ecoj.12461},
  copyright = {\textcopyright{} 2017 Royal Economic Society},
  journal = {The Economic Journal},
  keywords = {bias,credibility,empirical economics,practices,publication bias,statistical power},
  language = {en},
  number = {605}
}

@article{ioannidis_Why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  volume = {2},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  journal = {PLOS Medicine},
  keywords = {Cancer risk factors,crisis,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,practices,Randomized controlled trials,Research design,Schizophrenia},
  language = {en},
  number = {8}
}

@article{jara_Tracing_2018,
  title = {Tracing {{Mapuche Exclusion}} from {{Post}}-{{Dictatorial Truth Commissions}} in {{Chile}}: {{Official}} and {{Grassroots Initiatives}}},
  shorttitle = {Tracing {{Mapuche Exclusion}} from {{Post}}-{{Dictatorial Truth Commissions}} in {{Chile}}},
  author = {Jara, Daniela and Badilla, Manuela and Figueiredo, Ana and Cornejo, Marcela and Riveros, Victoria},
  year = {2018},
  month = nov,
  volume = {12},
  pages = {479--498},
  issn = {1752-7716},
  doi = {10.1093/ijtj/ijy025},
  abstract = {This article critically examines the official misrecognition of Mapuche experiences of violence during Augusto Pinochet's dictatorship (1973\textendash 1990) in state-sponsored truth commissions in Chile. We examine official post-dictatorial truth commission politics, narratives and procedures, analyzing how they envisioned the Mapuche as a political (absent) subject and how specific and homogenizing notions of victimhood were produced. We draw attention to three forms of cultural response by the Mapuche toward the official practices of the truth commissions from a bottom-up perspective: indifference, ambivalence and cultural resistance. We then draw attention to unofficial initiatives by nongovernmental organizations (NGOs) and grassroots groups that have aimed to tackle this gap in the transitional justice mechanisms by creating oppositional knowledge. We see in these counter initiatives a valuable knowledge that could allow the creation of bridges between Mapuche communities, mechanisms of transitional justice, grassroots and NGO activism and the Chilean state.},
  journal = {International Journal of Transitional Justice},
  number = {3}
}

@article{jerolmack_Ethical_2019,
  title = {The {{Ethical Dilemmas}} and {{Social Scientific Trade}}-Offs of {{Masking}} in {{Ethnography}}},
  author = {Jerolmack, Colin and Murphy, Alexandra K.},
  year = {2019},
  month = nov,
  volume = {48},
  pages = {801--827},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124117701483},
  abstract = {Masking, the practice of hiding or distorting identifying information about people, places, and organizations, is usually considered a requisite feature of ethnographic research and writing. This is justified both as an ethical obligation to one's subjects and as a scientifically neutral position (as readers are enjoined to treat a case's idiosyncrasies as sociologically insignificant). We question both justifications, highlighting potential ethical dilemmas and obstacles to constructing cumulative social science that can arise through masking. Regarding ethics, we show, on the one hand, how masking may give subjects a false sense of security because it implies a promise of confidentiality that it often cannot guarantee and, on the other hand, how naming may sometimes be what subjects want and expect. Regarding scientific tradeoffs, we argue that masking can reify ethnographic authority, exaggerate the universality of the case (e.g., ``Middletown''), and inhibit replicability (or ``revisits'') and sociological comparison. While some degree of masking is ethically and practically warranted in many cases and the value of disclosure varies across ethnographies, we conclude that masking should no longer be the default option that ethnographers unquestioningly choose.},
  journal = {Sociological Methods \& Research},
  keywords = {disclosure,ethics,ethnography,generalizability,masking,pseudonyms},
  language = {en},
  number = {4}
}

@article{jl_It_2017,
  title = {It's {{Time}} to {{Broaden}} the {{Replicability Conversation}}: {{Thoughts}} for and {{From Clinical Psychological Science}}},
  shorttitle = {It's {{Time}} to {{Broaden}} the {{Replicability Conversation}}},
  author = {Jl, Tackett and So, Lilienfeld and Cj, Patrick and Sl, Johnson and Rf, Krueger and Jd, Miller and Tf, Oltmanns and Pe, Shrout},
  year = {2017},
  month = sep,
  volume = {12},
  publisher = {{Perspect Psychol Sci}},
  issn = {1745-6924},
  doi = {10.1177/1745691617690042},
  abstract = {Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psychology. This important conversation has largely been focused on social psychology, with some active participation from cognitive psychology. N \ldots},
  journal = {Perspectives on psychological science : a journal of the Association for Psychological Science},
  language = {en},
  number = {5},
  pmid = {28972844}
}

@article{john_Measuring_2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  volume = {23},
  pages = {524--532},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  journal = {Psychological Science},
  keywords = {disclosure,judgment,methodology,practices,professional standards},
  language = {en},
  number = {5}
}

@article{kapiszewski_Openness_2019,
  title = {Openness in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2019},
  month = jun,
  doi = {10.33774/apsa-2019-if2he},
  abstract = {The discipline of political science has been engaged in discussion about when, why, and how to make scholarship more open for at least three decades.This piece argues that the best way to resolve our differences and develop appropriate norms and guidelines for making different types of qualitative research more open is to move from ``if'' to ``how'' \textendash{} for individual political scientists to make their work more open \textendash{} generating examples from which we can learn and on which we can build. We begin by articulating a series of principles that underlie our views on openness. We then consider the ``state of the debate,'' briefly outlining the contours of the scholarship on openness in political and other social sciences, highlighting the fractured nature of the discussion. The heart of the piece considers various strategies, illustrated by exemplary applications, for making qualitative research more open.},
  language = {en}
}

@article{kapiszewski_Transparency_2021,
  title = {Transparency in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2021},
  month = apr,
  volume = {54},
  pages = {285--291},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096520000955},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096520000955/resource/name/firstPage-S1049096520000955a.jpg},
  journal = {PS: Political Science \& Politics},
  keywords = {transparency},
  language = {en},
  number = {2}
}

@article{klein_Practical_2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and IJzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
  editor = {Nuijten, Mich{\'e}le and Vazire, Simine},
  year = {2018},
  month = jun,
  volume = {4},
  issn = {2474-7394},
  doi = {10.1525/collabra.158},
  abstract = {The credibility of scientific claims depends upon the transparency of the research products upon which they are based (e.g., study protocols, data, materials, and analysis scripts). As psychology navigates a period of unprecedented introspection, user-friendly tools and services that support open science have flourished. However, the plethora of decisions and choices involved can be bewildering. Here we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research (e.g., choosing a repository, preparing their research products for sharing, structuring folders, etc.). Being an open scientist means adopting a few straightforward research management practices, which lead to less error prone, reproducible research workflows. Further, this adoption can be piecemeal \textendash{} each incremental step towards complete transparency adds positive value. Transparent research practices not only improve the efficiency of individual researchers, they enhance the credibility of the knowledge generated by the scientific community.},
  journal = {Collabra: Psychology},
  keywords = {herramienta},
  number = {1}
}

@techreport{lareferencia.consejodirectivo_Comunicacion_2019,
  title = {{Comunicaci\'on Acad\'emica y Acceso Abierto: Acciones para un Pol\'itica P\'ublica en Am\'erica Latina}},
  shorttitle = {{Comunicaci\'on Acad\'emica y Acceso Abierto}},
  author = {LA Referencia. Consejo Directivo},
  year = {2019},
  month = may,
  institution = {{Zenodo}},
  doi = {10.5281/ZENODO.3229410},
  abstract = {Documento redactado como insumo  para las autoridades regionales que asistieron a la reuni\'on anual del Global Research Council con acuerdo del Consejo Directivo.   La publicaci\'on y difusi\'on del mismo se realiza con el fin de favorecer el di\'alogo y la construcci\'on de una visi\'on conjunta sobre la cual se debe profundizar y actualizar a la luz de los desaf\'ios del Acceso Abierto en la regi\'on en el corto y mediano plazo. La comunicaci\'on cient\'ifica y el cambio del modelo; la situaci\'on de Am\'erica Latina; el sistema de comunicaci\'on acad\'emica de la regi\'on, principios y acciones y recomendaciones para repositorios, consorcios y revistas son los ejes tem\'aticos que se abordan a lo largo de sus p\'aginas. El art\'iculo  refuerza la premisa de que se deben tomar acciones decididas para que los resultados financiados total o parcialmente con fondos p\'ublicos est\'en en Acceso Abierto y reafirma el rol central de los organismos de CyT para lograrlo. Basado en la realidad regional, propone principios generales y acciones para los repositorios de Acceso Abierto, consorcios y revistas con una mirada m\'as sist\'emica desde las pol\'iticas p\'ublicas. Concluye con la necesidad de un di\'alogo con iniciativas como el ``Plan S'' se\~nalando los puntos de acuerdo, as\'i como diferencias, dado el contexto regional, en temas como el APC o una valorizaci\'on del rol de los repositorios. Presentado en la reuni\'on  de   COAR. 2019.  Technical and Strategic Meeting of Repository Networks. Mayo 21, 2019 - Lyon, France. Alberto Cabezas Bullemore, Secretario Ejecutivo,   LA Referencia.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  keywords = {Acceso Abierto,Ciencia Abierta,Financiadores,ONCYTs,Plan S,Repositorios,revisado},
  language = {es}
}

@article{lewandowsky_Research_2016,
  title = {Research Integrity: {{Don}}'t Let Transparency Damage Science},
  shorttitle = {Research Integrity},
  author = {Lewandowsky, Stephan and Bishop, Dorothy},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {459--461},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/529459a},
  abstract = {Stephan Lewandowsky and Dorothy Bishop explain how the research community should protect its members from harassment, while encouraging the openness that has become essential to science.},
  copyright = {2016 Nature Publishing Group},
  journal = {Nature},
  keywords = {transparency},
  language = {en},
  number = {7587}
}

@article{lindsay_Seven_2020,
  title = {Seven Steps toward Transparency and Replicability in Psychological Science.},
  author = {Lindsay, D. Stephen},
  year = {2020},
  month = nov,
  volume = {61},
  pages = {310--317},
  issn = {1878-7304, 0708-5591},
  doi = {10.1037/cap0000222},
  journal = {Canadian Psychology/Psychologie canadienne},
  keywords = {forrt,transparency},
  language = {en},
  number = {4}
}

@article{linton_Publish_2011,
  title = {Publish or {{Perish}}: {{How Are Research}} and {{Reputation Related}}?},
  shorttitle = {Publish or {{Perish}}},
  author = {Linton, Jonathan D. and Tierney, Robert and Walsh, Steven T.},
  year = {2011},
  month = dec,
  volume = {37},
  pages = {244--257},
  publisher = {{Routledge}},
  issn = {0098-7913},
  doi = {10.1080/00987913.2011.10765398},
  abstract = {A study of twenty-seven fields in 350 highly ranked universities examines the relationship between reputation and rank. We find that many metrics associated with research prowess significantly correlate to university reputation. However, the next logical step\textendash{} looking at the relationship that links different academic fields with the reputation of the university\textendash did not always offer the expected results. The phrase ``publish or perish'' clearly has very different meanings in different fields.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00987913.2011.10765398},
  journal = {Serials Review},
  keywords = {Academic reputation,institutional,Interdisciplinary studies,Publish or perish,University research reputation},
  number = {4}
}

@article{loredo_Mexico_2006,
  title = {M\'exico: {{Derecho Comparado}}: {{Derecho}} de {{Autor}} y {{Copyright}}. {{Dos}} Caminos Que Se Encuentran},
  shorttitle = {M\'exico},
  author = {Loredo, Alejandro},
  year = {2006},
  pages = {2},
  publisher = {{Alfa-Redi}},
  journal = {AR: Revista de Derecho Inform\'atico},
  number = {91}
}

@techreport{luke_Epistemological_2019,
  title = {Epistemological and {{Ontological Priors}}: {{Explicating}} the {{Perils}} of {{Transparency}}},
  shorttitle = {Epistemological and {{Ontological Priors}}},
  author = {Luke, Timothy W. and {V{\'a}zquez-Arroyo}, Antonio and Hawkesworth, Mary},
  year = {2019},
  month = feb,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3332878},
  abstract = {The discipline of political science encompasses multiple research communities, which have grown out of and rely upon different epistemological and ontological presuppositions.  Recent debates about transparency raise important questions about which of these research communities will be accredited within the discipline, whose values, norms, and methods of knowledge production will gain ascendency and whose will be marginalized.  Although the language of "transparency" makes it appear that these debates are apolitical, simply elaborating standards that all political scientists share, the intensity and content of recent contestations about DA-RT, JETS, and QTD attest to the profoundly political nature of these methodological discussions. This report traces the epistemological and ontological assumptions that have shaped diverse research communities within the discipline, situating "transparency" in relation to classical (Aristotelian), modern (Baconian) and twentieth-century (positivist, critical rationalist, and postpositivist) versions of empiricism.  It shows how recent discussions of transparency accredit certain empirical approaches by collapsing the scope of empirical investigation and the parameters of the knowable.  And it argues that "transparency" is inappropriate as a regulative ideal for political science because it misconstrues the roles of theory, social values, and critique in scholarly investigation.},
  keywords = {epistemology,ontology,philosophy of science,qualitative methods,Qualitative Transparency Deliberations,research transparency},
  language = {en},
  number = {ID 3332878},
  type = {{{SSRN Scholarly Paper}}}
}

@article{martinson_Scientists_2005,
  title = {Scientists Behaving Badly},
  author = {Martinson, Brian C. and Anderson, Melissa S. and {de Vries}, Raymond},
  year = {2005},
  month = jun,
  volume = {435},
  pages = {737--738},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/435737a},
  abstract = {To protect the integrity of science, we must look beyond falsification, fabrication and plagiarism, to a wider range of questionable research practices, argue Brian C. Martinson, Melissa S. Anderson and Raymond de Vries.},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion},
  copyright = {2005 Nature Publishing Group},
  journal = {Nature},
  keywords = {practices},
  language = {en},
  number = {7043}
}

@article{mcgrail_Publish_2006,
  title = {Publish or Perish: A Systematic Review of Interventions to Increase Academic Publication Rates},
  shorttitle = {Publish or Perish},
  author = {McGrail, Matthew R. and Rickard, Claire M. and Jones, Rebecca},
  year = {2006},
  month = feb,
  volume = {25},
  pages = {19--35},
  publisher = {{Routledge}},
  issn = {0729-4360},
  doi = {10.1080/07294360500453053},
  abstract = {Academics are expected to publish. In Australia universities receive extra funding based on their academic publication rates and academic promotion is difficult without a good publication record. However, the reality is that only a small percentage of academics are actively publishing. To fix this problem, a number of international universities and other higher education institutions have implemented interventions with the main aim being to increase the number of publications. A comprehensive literature search identified 17 studies published between 1984 and 2004, which examined the effects of these interventions. Three key types of interventions were identified: writing courses, writing support groups and writing coaches. The resulting publication output varied, but all interventions led to an increase in average publication rates for the participants.},
  annotation = {\_eprint: https://doi.org/10.1080/07294360500453053},
  journal = {Higher Education Research \& Development},
  keywords = {institutional},
  number = {1}
}

@article{mckiernan_How_2016,
  title = {How Open Science Helps Researchers Succeed},
  author = {McKiernan, Erin C and Bourne, Philip E and Brown, C Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A and Ram, Karthik and Soderberg, Courtney K and Spies, Jeffrey R and Thaney, Kaitlin and Updegrove, Andrew and Woo, Kara H and Yarkoni, Tal},
  editor = {Rodgers, Peter},
  year = {2016},
  month = jul,
  volume = {5},
  pages = {e16800},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.16800},
  abstract = {Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these practices has not yet been achieved. One reason is that researchers are uncertain about how sharing their work will affect their careers. We review literature demonstrating that open research is associated with increases in citations, media attention, potential collaborators, job opportunities and funding opportunities. These findings are evidence that open research practices bring significant benefits to researchers relative to more traditional closed practices.},
  journal = {eLife},
  keywords = {open access,open data,open science,open source,research}
}

@article{mcvay_Transparency_2021,
  title = {Transparency and Openness in Behavioral Medicine Research},
  author = {McVay, Megan A and Conroy, David E},
  year = {2021},
  month = jan,
  volume = {11},
  pages = {287--290},
  issn = {1869-6716},
  doi = {10.1093/tbm/ibz154},
  abstract = {Behavioral medicine aims to improve the health of individuals and communities by addressing behavioral, psychosocial, and environmental contributors to health. Succeeding in this endeavor requires rigorous research and effective communication of this research to relevant stakeholders and the public at large [1]. Both research rigor and effective communication of research may benefit from adopting transparent and open research practices [2\textendash 4], sometimes called ``open science.'' Such practices include preregistering designs, hypotheses, and data analysis plans; making publically available study materials, data, and analytic code; sharing preprints (works-in-progress) of articles; and publishing open access [2]. In this commentary, we describe the evolving pressures to increase the transparency and openness of research, examine the status of open science practices in behavioral medicine, and recommend a path forward to find the right fit for these practices in behavioral medicine research.},
  journal = {Translational Behavioral Medicine},
  keywords = {transparency},
  number = {1}
}

@article{mertens_Preregistration_2019,
  title = {Preregistration of {{Analyses}} of {{Preexisting Data}}},
  author = {Mertens, Ga{\"e}tan and Krypotos, Angelos-Miltiadis},
  year = {2019},
  volume = {59},
  pages = {338--352},
  issn = {0033-2879},
  doi = {10.5334/pb.493},
  abstract = {The preregistration of a study's hypotheses, methods, and data-analyses steps is becoming a popular psychological research practice. To date, most of the discussion on study preregistration has focused on the preregistration of studies that include the collection of original data. However, much of the research in psychology relies on the (re-)analysis of preexisting data. Importantly, this type of studies is different from original studies as researchers cannot change major aspects of the study (e.g., experimental manipulations, sample size). Here, we provide arguments as to why it is useful to preregister analyses of preexisting data, discuss practical considerations, consider potential concerns, and introduce a preregistration template tailored for studies focused on the analyses of preexisting data. We argue that the preregistration of hypotheses and data-analyses for analyses of preexisting data is an important step towards more transparent psychological research.},
  journal = {Psychologica Belgica},
  number = {1},
  pmcid = {PMC6706998},
  pmid = {31497308}
}

@article{miguel_Promoting_2014,
  title = {Promoting {{Transparency}} in {{Social Science Research}}},
  author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and der Laan, M. Van},
  year = {2014},
  month = jan,
  volume = {343},
  pages = {30--31},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1245317},
  abstract = {There is growing appreciation for the advantages of experimentation in the social sciences. Policy-relevant claims that in the past were backed by theoretical arguments and inconclusive correlations are now being investigated using more credible methods. Changes have been particularly pronounced in development economics, where hundreds of randomized trials have been carried out over the last decade. When experimentation is difficult or impossible, researchers are using quasi-experimental designs. Governments and advocacy groups display a growing appetite for evidence-based policy-making. In 2005, Mexico established an independent government agency to rigorously evaluate social programs, and in 2012, the U.S. Office of Management and Budget advised federal agencies to present evidence from randomized program evaluations in budget requests (1, 2). Social scientists should adopt higher transparency standards to improve the quality and credibility of research. Social scientists should adopt higher transparency standards to improve the quality and credibility of research.},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  journal = {Science},
  keywords = {transparency},
  language = {en},
  number = {6166},
  pmid = {24385620}
}

@article{moore_Preregister_2016,
  title = {Preregister If You Want To},
  author = {Moore, Don A.},
  year = {2016},
  month = apr,
  volume = {71},
  pages = {238--239},
  issn = {1935-990X},
  doi = {10.1037/a0040195},
  abstract = {Prespecification of confirmatory hypothesis tests is a useful tool that makes our statistical tests informative. On the other hand, selectively reporting studies, measures, or statistical tests renders the probability of false positives higher than the p values would imply. The bad news is that it is usually difficult to tell how much higher the probability is. Fortunately, there are enormous opportunities to improve the quality of our science by preregistering our research plans. Preregistration is a highly distinctive strength that should increase our faith in the veracity and replicability of a research result.},
  journal = {The American Psychologist},
  keywords = {Clinical Trials as Topic,Humans,Information Dissemination,Reproducibility of Results,Research Design,Science},
  language = {eng},
  number = {3},
  pmid = {27042885}
}

@article{motta_Dynamics_2018,
  title = {The {{Dynamics}} and {{Political Implications}} of {{Anti}}-{{Intellectualism}} in the {{United States}}},
  author = {Motta, Matthew},
  year = {2018},
  month = may,
  volume = {46},
  pages = {465--498},
  publisher = {{SAGE Publications Inc}},
  issn = {1532-673X},
  doi = {10.1177/1532673X17719507},
  abstract = {Recently, Americans have become increasingly likely to hold anti-intellectual attitudes (i.e., negative affect toward scientists and other experts). However, few have investigated the political implications of anti-intellectualism, and much empirical uncertainty surrounds whether or not these attitudes can be mitigated. Drawing on cross-sectional General Social Survey (GSS) data and a national election panel in 2016, I find that anti-intellectualism is associated with not only the rejection of policy-relevant matters of scientific consensus but support for political movements (e.g., ``Brexit'') and politicians (e.g., George Wallace, Donald Trump) who are skeptical of experts. Critically, though, I show that these effects can be mitigated. Verbal intelligence plays a strong role in mitigating anti-intellectual sympathies, compared with previously studied potential mitigators. I conclude by discussing how scholars might build on this research to study the political consequences of anti-intellectualism in the future.},
  journal = {American Politics Research},
  keywords = {anti-intellectualism,antiscience attitudes,political psychology,public opinion,verbal intelligence},
  language = {en},
  number = {3}
}

@article{motyl_state_2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  month = jul,
  volume = {113},
  pages = {34--58},
  issn = {1939-1315},
  doi = {10.1037/pspa0000084},
  abstract = {The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of Questionable Research Practices (QRPs) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In Study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field's research practices. In Study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003-2004 and 2013-2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic; (b) the discussion has increased researchers' intentions to avoid QRPs and adopt proposed best practices, (c) the estimated replicability of research published in 2003-2004 may not be as bad as many feared, and (d) research published in 2013-2014 shows some improvement over research published in 2003-2004, a result that suggests the field is evolving in a positive direction. (PsycINFO Database Record},
  journal = {Journal of Personality and Social Psychology},
  keywords = {Attitude of Health Personnel,Ethics; Research,Female,Humans,Male,Personality,Psychology,Psychology; Social,Research,Research Design,Surveys and Questionnaires},
  language = {eng},
  number = {1},
  pmid = {28447837}
}

@article{munafo_manifesto_2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  volume = {1},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Social sciences Subject\_term\_id: social-sciences},
  copyright = {2017 Macmillan Publishers Limited},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {1}
}

@misc{nassi-calo_Open_2013,
  title = {Open {{Access}} and a Call to Prevent the Looming Crisis in Science | {{SciELO}} in {{Perspective}}},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2013},
  month = jul,
  abstract = {The number of retracted articles has recently been on the rise. Bj\"orn Brembs identifies this tendency as a reflection of an imminent crisis in science whose},
  keywords = {crisis},
  language = {en-US}
}

@misc{nassi-calo_reproducibilidad_2014,
  title = {La Reproducibilidad En Los Resultados de Investigaci\'on: La Mirada Subjetiva | {{SciELO}} En {{Perspectiva}}},
  shorttitle = {La Reproducibilidad En Los Resultados de Investigaci\'on},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2014},
  month = feb,
  abstract = {En una \'epoca en que las discusiones sobre \'etica en la experimentaci\'on y la publicaci\'on cient\'ifica traspasan los laboratorios y ambientes acad\'emicos para},
  language = {en-US}
}

@article{naturehumanbehaviour_Tell_2020,
  title = {Tell It like It Is},
  author = {{Nature human behaviour}},
  year = {2020},
  month = jan,
  volume = {4},
  pages = {1--1},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-0818-9},
  abstract = {Every research paper tells a story, but the pressure to provide `clean' narratives is harmful for the scientific endeavour.},
  copyright = {2020 Springer Nature Limited},
  journal = {Nature Human Behaviour},
  keywords = {forrt,transparency},
  language = {en},
  number = {1}
}

@article{nosek_preregistration_2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  volume = {115},
  pages = {2600--2606},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes\textemdash a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  chapter = {Colloquium Paper},
  copyright = {\textcopyright{} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration,reports,revisado},
  language = {en},
  number = {11},
  pmid = {29531091}
}

@article{nosek_Preregistration_2019,
  title = {Preregistration {{Is Hard}}, {{And Worthwhile}}},
  author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and {van 't Veer}, Anna E. and Vazire, Simine},
  year = {2019},
  month = oct,
  volume = {23},
  pages = {815--818},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.07.009},
  abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
  journal = {Trends in Cognitive Sciences},
  keywords = {confirmatory research,exploratory research,preregistration,reproducibility,transparency},
  language = {en},
  number = {10}
}

@article{nosek_Promoting_2015,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1422--1425},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  journal = {Science},
  keywords = {revisado},
  language = {en},
  number = {6242},
  pmid = {26113702}
}

@article{nosek_Registered_2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  month = may,
  volume = {45},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  journal = {Social Psychology},
  keywords = {forrt,reports},
  language = {en},
  number = {3}
}

@article{nosek_Transparency_2014,
  title = {Transparency and {{Openness Promotion}} ({{TOP}}) {{Guidelines}}},
  author = {Nosek, Brian A. and Alter, George and Banks, George Christopher and Borsboom, Denny and Bowman, Sara and Breckler, Steven and Buck, Stuart and Chambers, Chris and Chin, Gilbert and Christensen, Garret},
  year = {2014},
  month = aug,
  publisher = {{OSF}},
  abstract = {The Transparency and Openness Promotion (TOP) Committee met in November 2014 to address one important element of the incentive systems - journals' procedures and  policies for publication. The outcome of the effort is the TOP Guidelines. There are eight standards in the TOP guidelines; each move scientific communication toward greater openness.  These standards are modular, facilitating adoption in whole or in part. However, they also complement each other, in that commitment to one standard may facilitate adoption of others. Moreover, the guidelines are sensitive to barriers to openness by articulating, for example, a process for exceptions to sharing because of ethical issues, intellectual property concerns, or availability of necessary resources.      Hosted on the Open Science Framework},
  keywords = {(,transparency},
  language = {en}
}

@misc{nw_Trust_2019,
  title = {Trust and {{Mistrust}} in {{Americans}}' {{Views}} of {{Scientific Experts}}},
  author = {NW, 1615 L. St and Suite 800Washington and Inquiries, DC 20036USA202-419-4300 | Main202-857-8562 | Fax202-419-4372 | Media},
  year = {2019},
  month = aug,
  abstract = {Public confidence in scientists is on the upswing, and six-in-ten Americans say scientists should play an active role in policy debates about scientific issues, according to a new Pew Research Center survey.},
  journal = {Pew Research Center Science \& Society},
  language = {en-US}
}

@article{oboyle_Chrysalis_2017,
  title = {The {{Chrysalis Effect}}: {{How Ugly Initial Results Metamorphosize Into Beautiful Articles}}},
  shorttitle = {The {{Chrysalis Effect}}},
  author = {O'Boyle, Ernest Hugh and Banks, George Christopher and {Gonzalez-Mul{\'e}}, Erik},
  year = {2017},
  month = feb,
  volume = {43},
  pages = {376--399},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206314527133},
  abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the ``Chrysalis Effect.''},
  journal = {Journal of Management},
  keywords = {ethics,morality and moral behavior,philosophy of science,revisado,statistical methods,transparency},
  language = {en},
  number = {2}
}

@article{opensciencecollaboration_Estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  volume = {349},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  journal = {Science},
  language = {en},
  number = {6251}
}

@article{ospina_Problemas_2014,
  title = {Problemas de Propiedad Intelectual En El Entorno Universitario. {{Un}} Acercamiento General},
  author = {Ospina, Adriana Mar{\'i}a Restrepo},
  year = {2014},
  volume = {71},
  pages = {69--96},
  journal = {Estudios de Derecho},
  number = {158}
}

@article{patil_visual_2019,
  title = {A Visual Tool for Defining Reproducibility and Replicability},
  author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
  year = {2019},
  month = jul,
  volume = {3},
  pages = {650--652},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0629-z},
  abstract = {Reproducibility and replicability are fundamental requirements of scientific studies. Disagreements over universal definitions for these terms have affected the interpretation of large-scale replication attempts. We provide a visual tool for representing definitions and use it to re-examine these attempts.},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Software;Statistics Subject\_term\_id: software;statistics},
  copyright = {2019 Springer Nature Limited},
  journal = {Nature Human Behaviour},
  keywords = {herramienta},
  language = {en},
  number = {7}
}

@article{penders_Rinse_2019,
  title = {Rinse and {{Repeat}}: {{Understanding}} the {{Value}} of {{Replication}} across {{Different Ways}} of {{Knowing}}},
  shorttitle = {Rinse and {{Repeat}}},
  author = {Penders, Bart and Holbrook, J. Britt and {de Rijcke}, Sarah},
  year = {2019},
  month = sep,
  volume = {7},
  pages = {52},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/publications7030052},
  abstract = {The increasing pursuit of replicable research and actual replication of research is a political project that articulates a very specific technology of accountability for science. This project was initiated in response to concerns about the openness and trustworthiness of science. Though applicable and valuable in many fields, here we argue that this value cannot be extended everywhere, since the epistemic content of fields, as well as their accountability infrastructures, differ. Furthermore, we argue that there are limits to replicability across all fields; but in some fields, including parts of the humanities, these limits severely undermine the value of replication to account for the value of research.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  journal = {Publications},
  keywords = {accountability,epistemic pluralism,humanities,Replicability,replication,reproducibility,reproduction},
  language = {en},
  number = {3}
}

@article{peng_reproducibility_2015,
  title = {The Reproducibility Crisis in Science: {{A}} Statistical Counterattack},
  shorttitle = {The Reproducibility Crisis in Science},
  author = {Peng, Roger},
  year = {2015},
  volume = {12},
  pages = {30--32},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2015.00827.x},
  abstract = {More people have more access to data than ever before. But a comparative lack of analytical skills has resulted in scientific findings that are neither replicable nor reproducible. It is time to invest in statistics education, says Roger Peng},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00827.x},
  copyright = {\textcopyright{} 2015 The Royal Statistical Society},
  journal = {Significance},
  keywords = {crisis},
  language = {en},
  number = {3}
}

@article{peng_Reproducible_2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, R. D.},
  year = {2011},
  month = dec,
  volume = {334},
  pages = {1226--1227},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  journal = {Science},
  language = {en},
  number = {6060}
}

@article{petousi_Contextualising_2020,
  title = {Contextualising Harm in the Framework of Research Misconduct. {{Findings}} from Discourse Analysis of Scientific Publications},
  author = {Petousi, Vasiliki and Sifaki, Eirini},
  year = {2020},
  month = jan,
  volume = {23},
  pages = {149--174},
  publisher = {{Inderscience Publishers}},
  issn = {0960-1406},
  doi = {10.1504/IJSD.2020.115206},
  abstract = {This article reports on research, which deals with dimensions of harm resulting from research misconduct, in articles published in scientific journals. An appropriate sample of publications retrieved from Pubmed, Scopus and WOS was selected across various disciplines and topics. Implementing discourse analysis, articles were classified according to the narratives of 'individual impurity', 'institutional failure' and 'structural crisis'. Most of the articles analysed fall within the narrative of structural crisis. The main argument advanced is that research misconduct harms the scientific enterprise as a whole. Harm is narrated in the context of institutional characteristics, policies, procedures, guidelines, and work environment. Mainly, however, harm is narrated in the context of structural characteristics of contemporary scientific practices, which result in normative dissonance for scientists and loss of trust in science in the relation between science and society and within the scientific enterprise itself. We conclude that new grounds for building trust and confidence in science are needed.},
  journal = {International Journal of Sustainable Development},
  keywords = {practices},
  number = {3-4}
}

@article{piwowar_Future_2019,
  title = {The {{Future}} of {{OA}}: {{A}} Large-Scale Analysis Projecting {{Open Access}} Publication and Readership},
  shorttitle = {The {{Future}} of {{OA}}},
  author = {Piwowar, Heather and Priem, Jason and Orr, Richard},
  year = {2019},
  pages = {795310},
  publisher = {{Cold Spring Harbor Laboratory}},
  journal = {BioRxiv}
}

@article{piwowar_state_2018,
  title = {The State of {{OA}}: A Large-Scale Analysis of the Prevalence and Impact of {{Open Access}} Articles},
  shorttitle = {The State of {{OA}}},
  author = {Piwowar, Heather and Priem, Jason and Larivi{\`e}re, Vincent and Alperin, Juan Pablo and Matthias, Lisa and Norlander, Bree and Farley, Ashley and West, Jevin and Haustein, Stefanie},
  year = {2018},
  volume = {6},
  pages = {e4375},
  publisher = {{PeerJ Inc.}},
  journal = {PeerJ}
}

@incollection{poumadere_Credibility_1991,
  title = {The {{Credibility Crisis}}},
  booktitle = {Chernobyl: {{A Policy Response Study}}},
  author = {Poumad{\`e}re, Marc},
  editor = {Segerst{\aa}hl, Boris},
  year = {1991},
  pages = {149--171},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-84367-9_8},
  abstract = {Those who said that the twentieth century is the century of the atom didn't know how right they were. Certainly, the scientific discovery of a particularly powerful new energy source had produced hopes and applications in both the civil and military sectors. In the final equation, though, it is the major accident at Chernobyl that dramatically and suddenly brought the reality of the atom's presence home to many persons and groups around the world.},
  isbn = {978-3-642-84367-9},
  keywords = {Chernobyl Accident,crisis,Nuclear Energy,Nuclear Power Plant,Social Defense,Social Distance},
  language = {en},
  series = {Springer {{Series}} on {{Environmental Management}}}
}

@article{price_Problem_2020,
  title = {Problem with p Values: Why p Values Do Not Tell You If Your Treatment Is Likely to Work},
  shorttitle = {Problem with p Values},
  author = {Price, Robert and Bethune, Rob and Massey, Lisa},
  year = {2020},
  month = jan,
  volume = {96},
  pages = {1--3},
  issn = {0032-5473, 1469-0756},
  doi = {10.1136/postgradmedj-2019-137079},
  journal = {Postgraduate Medical Journal},
  keywords = {forrt,practices},
  language = {en},
  number = {1131}
}

@article{redish_Opinion_2018,
  title = {Opinion: {{Reproducibility}} Failures Are Essential to Scientific Inquiry},
  shorttitle = {Opinion},
  author = {Redish, A. David and Kummerfeld, Erich and Morris, Rebecca Lea and Love, Alan C.},
  year = {2018},
  month = may,
  volume = {115},
  pages = {5042--5046},
  journal = {Proceedings of the National Academy of Sciences},
  number = {20}
}

@article{rinke_Probabilistic_2018,
  title = {Probabilistic {{Misconceptions Are Pervasive Among Communication Researchers}}},
  author = {Rinke, Eike Mark and Schneider, Frank M.},
  year = {2018},
  month = sep,
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/h8zbe},
  abstract = {Across all areas of communication research, the most popular approach to generating insights about communication is the classical significance test (also called null hypothesis significance testing, NHST). The predominance of NHST in communication research is in spite of serious concerns about the ability of researchers to properly interpret its results. We draw on data from a survey of the ICA membership to assess the evidential basis of these concerns. The vast majority of communication researchers misinterpreted NHST (91\%) and the most prominent alternative, confidence intervals (96\%), while overestimating their competence. Academic seniority and statistical experience did not predict better interpretation outcomes. These findings indicate major problems regarding the generation of knowledge in the field of communication research.},
  keywords = {Communication,confidence intervals,data analysis,misconceptions,practices,significance testing,Social and Behavioral Sciences,statistical inference,statistics}
}

@article{rodriguez-sanchez_Ciencia_2016,
  title = {{Ciencia reproducible: qu\'e, por qu\'e, c\'omo}},
  shorttitle = {{Ciencia reproducible}},
  author = {{Rodriguez-Sanchez}, Francisco and {P{\'e}rez-Luque}, Antonio Jes{\'u}s and Bartomeus, Ignasi and Varela, Sara},
  year = {2016},
  month = jul,
  volume = {25},
  pages = {83--92},
  issn = {1697-2473},
  doi = {10.7818/ECOS.2016.25-2.11},
  copyright = {Derechos de autor},
  journal = {Ecosistemas},
  keywords = {reproducibilidad,revisado},
  language = {es},
  number = {2}
}

@misc{rowe_Preview_2018,
  title = {Preview My New Book: {{Introduction}} to {{Reproducible Science}} in {{R}} | {{R}}-Bloggers},
  shorttitle = {Preview My New Book},
  author = {Rowe, Brian Lee Yung},
  year = {2018},
  month = nov,
  abstract = {I'm pleased to share Part I of my new book ``Introduction to Reproducible Science in R``. The purpose of this \ldots Continue reading \textrightarrow},
  language = {en-US}
}

@article{schnell_Reproducible_2018,
  title = {``{{Reproducible}}'' {{Research}} in {{Mathematical Sciences Requires Changes}} in Our {{Peer Review Culture}} and {{Modernization}} of Our {{Current Publication Approach}}},
  author = {Schnell, Santiago},
  year = {2018},
  month = dec,
  volume = {80},
  pages = {3095--3105},
  issn = {1522-9602},
  doi = {10.1007/s11538-018-0500-9},
  abstract = {The nature of scientific research in mathematical and computational biology allows editors and reviewers to evaluate the findings of a scientific paper. Replication of a research study should be the minimum standard for judging its scientific claims and considering it for publication. This requires changes in the current peer review practice and a strict adoption of a replication policy similar to those adopted in experimental fields such as organic synthesis. In the future, the culture of replication can be easily adopted by publishing papers through dynamic computational notebooks combining formatted text, equations, computer algebra and computer code.},
  journal = {Bulletin of Mathematical Biology},
  language = {en},
  number = {12}
}

@article{serra-garcia_Nonreplicable_2021,
  title = {Nonreplicable Publications Are Cited More than Replicable Ones},
  author = {{Serra-Garcia}, Marta and Gneezy, Uri},
  year = {2021},
  month = may,
  volume = {7},
  pages = {eabd1705},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abd1705},
  abstract = {Published papers that fail to replicate are cited more than those that replicate, even after the failure is published., We use publicly available data to show that published papers in top psychology, economics, and general interest journals that fail to replicate are cited more than those that replicate. This difference in citation does not change after the publication of the failure to replicate. Only 12\% of postreplication citations of nonreplicable findings acknowledge the replication failure. Existing evidence also shows that experts predict well which papers will be replicated. Given this prediction, why are nonreplicable papers accepted for publication in the first place? A possible answer is that the review team faces a trade-off. When the results are more ``interesting,'' they apply lower standards regarding their reproducibility.},
  journal = {Science Advances},
  number = {21},
  pmcid = {PMC8139580},
  pmid = {34020944}
}

@article{simmons_FalsePositive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  volume = {22},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  journal = {Psychological Science},
  keywords = {practices},
  language = {en},
  number = {11}
}

@article{simonsohn_Pcurve_2014,
  title = {P-Curve: {{A}} Key to the File-Drawer},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  volume = {143},
  pages = {534--547},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/a0033242},
  abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that ``work,'' readers must ask, ``Are these effects true, or do they merely reflect selective reporting?'' We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps p-curves\textemdash containing more low (.01s) than high (.04s) significant p values\textemdash only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Journal of Experimental Psychology: General},
  keywords = {herramienta,Hypothesis Testing,practices,Psychology,Scientific Communication,Statistics},
  number = {2}
}

@techreport{soderberg_Initial_2020,
  title = {Initial {{Evidence}} of {{Research Quality}} of {{Registered Reports Compared}} to the {{Traditional Publishing Model}}},
  author = {Soderberg, Courtney K. and Errington, Timothy M. and Schiavone, Sarah R. and Bottesini, Julia G. and Singleton Thorn, Felix and Vazire, Simine and Esterling, Kevin and Nosek, Brian A.},
  year = {2020},
  month = nov,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/7x9vy},
  abstract = {In Registered Reports (RRs), initial peer review and in-principle acceptance occurs before knowing the research outcomes. This combats publication bias and distinguishes planned and unplanned research. How RRs could improve the credibility of research findings is straightforward, but there is little empirical evidence. Also, there could be unintended costs such as reducing novelty. 353 researchers peer reviewed a pair of papers from 29 published RRs from psychology and neuroscience and 57 non-RR comparison papers. RRs outperformed comparison papers on all 19 criteria (mean difference=0.46; Scale range -4 to +4) with effects ranging from little improvement in novelty (0.13, 95\% credible interval [-0.24, 0.49]) and creativity (0.22, [-0.14, 0.58]) to larger improvements in rigor of methodology (0.99, [0.62, 1.35]) and analysis (0.97, [0.60, 1.34]) and overall paper quality (0.66, [0.30, 1.02]). RRs could improve research quality while reducing publication bias and ultimately improve the credibility of the published literature.},
  keywords = {reports,transparency},
  type = {Preprint}
}

@article{stamkou_Cultural_2019,
  title = {Cultural {{Collectivism}} and {{Tightness Moderate Responses}} to {{Norm Violators}}: {{Effects}} on {{Power Perception}}, {{Moral Emotions}}, and {{Leader Support}}},
  shorttitle = {Cultural {{Collectivism}} and {{Tightness Moderate Responses}} to {{Norm Violators}}},
  author = {Stamkou, Eftychia and {van Kleef}, Gerben A. and Homan, Astrid C. and Gelfand, Michele J. and {van de Vijver}, Fons J. R. and {van Egmond}, Marieke C. and Boer, Diana and Phiri, Natasha and Ayub, Nailah and Kinias, Zoe and Cantarero, Katarzyna and Efrat Treister, Dorit and Figueiredo, Ana and Hashimoto, Hirofumi and Hofmann, Eva B. and Lima, Renata P. and Lee, I-Ching},
  year = {2019},
  month = jun,
  volume = {45},
  pages = {947--964},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-1672},
  doi = {10.1177/0146167218802832},
  abstract = {Responses to norm violators are poorly understood. On one hand, norm violators are perceived as powerful, which may help them to get ahead. On the other hand, norm violators evoke moral outrage, which may frustrate their upward social mobility. We addressed this paradox by considering the role of culture. Collectivistic cultures value group harmony and tight cultures value social order. We therefore hypothesized that collectivism and tightness moderate reactions to norm violators. We presented 2,369 participants in 19 countries with a norm violation or a norm adherence scenario. In individualistic cultures, norm violators were considered more powerful than norm abiders and evoked less moral outrage, whereas in collectivistic cultures, norm violators were considered less powerful and evoked more moral outrage. Moreover, respondents in tighter cultures expressed a stronger preference for norm followers as leaders. Cultural values thus influence responses to norm violators, which may have downstream consequences for violators' hierarchical positions.},
  journal = {Personality and Social Psychology Bulletin},
  keywords = {collectivism,leadership,moral emotions,norm violation,tightness},
  language = {en},
  number = {6}
}

@article{steneck_Fostering_2006,
  title = {Fostering Integrity in Research: {{Definitions}}, Current Knowledge, and Future Directions},
  shorttitle = {Fostering Integrity in Research},
  author = {Steneck, Nicholas H.},
  year = {2006},
  month = mar,
  volume = {12},
  pages = {53--74},
  issn = {1471-5546},
  doi = {10.1007/PL00022268},
  abstract = {Over the last 25 years, a small but growing body of research on research behavior has slowly provided a more complete and critical understanding of research practices, particularly in the biomedical and behavioral sciences. The results of this research suggest that some earlier assumptions about irresponsible conduct are not reliable, leading to the conclusion that there is a need to change the way we think about and regulate research behavior. This paper begins with suggestions for more precise definitions of the terms ``responsible conduct of research,'' ``research ethics,'' and ``research integrity.'' It then summarizes the findings presented in some of the more important studies of research behavior, looking first at levels of occurrence and then impact. Based on this summary, the paper concludes with general observations about priorities and recommendations for steps to improve the effectiveness of efforts to respond to misconduct and foster higher standards for integrity in research.},
  journal = {Science and Engineering Ethics},
  keywords = {practices},
  language = {en},
  number = {1}
}

@article{stewart_Preregistration_2020,
  title = {Pre-Registration and {{Registered Reports}}: A {{Primer}} from {{UKRN}}},
  shorttitle = {Pre-Registration and {{Registered Reports}}},
  author = {Stewart, Suzanne and Rinke, Eike Mark and McGarrigle, Ronan and Lynott, Dermot and Lunny, Carole and Lautarescu, Alexandra and Galizzi, Matteo M. and Farran, Emily K. and Crook, Zander},
  year = {2020},
  month = oct,
  publisher = {{OSF Preprints}},
  doi = {10.31219/osf.io/8v2n7},
  abstract = {Help reduce questionable research practices, and prevent selective reporting.},
  keywords = {Architecture,Arts and Humanities,Business,Education,Engineering,Law,Life Sciences,Medicine and Health Sciences,Physical Sciences and Mathematics,pre-analysis plan,pre-registration,preregistration,primer,primers,prospective registration,registered reports,registration,reproducibility,Social and Behavioral Sciences,UK Reproducibility Network,UKRN}
}

@article{stodden_Trust_2011,
  title = {Trust {{Your Science}}? {{Open Your Data}} and {{Code}}},
  shorttitle = {Trust {{Your Science}}?},
  author = {Stodden, Victoria C.},
  year = {2011},
  volume = {409},
  pages = {21--22},
  doi = {10.7916/D8CJ8Q0P},
  abstract = {This is a view on the reproducibility of computational sciences by Victoria Stodden. It contains information on the Reproducibility, Replicability, and Repeatability of code created by the other sciences. Stodden also talks about the rising prominence of computational sciences as we are in the digital age and what that means for the future of science and collecting data.},
  keywords = {forrt},
  language = {en}
}

@article{szollosi_Preregistration_2020,
  title = {Is {{Preregistration Worthwhile}}?},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard and {van Rooij}, Iris and Van Zandt, Trisha and Donkin, Chris},
  year = {2020},
  month = feb,
  volume = {24},
  pages = {94--95},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.11.009},
  journal = {Trends in Cognitive Sciences},
  keywords = {forrt,reports},
  language = {en},
  number = {2}
}

@article{tennant_Ten_2019,
  title = {Ten {{Hot Topics}} around {{Scholarly Publishing}}},
  author = {Tennant, Jonathan P. and Crane, Harry and Crick, Tom and Davila, Jacinto and Enkhbayar, Asura and Havemann, Johanna and Kramer, Bianca and Martin, Ryan and Masuzzo, Paola and Nobes, Andy and Rice, Curt and {Rivera-L{\'o}pez}, B{\'a}rbara and {Ross-Hellauer}, Tony and Sattler, Susanne and Thacker, Paul D. and Vanholsbeeck, Marc},
  year = {2019},
  month = jun,
  volume = {7},
  pages = {34},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/publications7020034},
  abstract = {The changing world of scholarly communication and the emerging new wave of \&lsquo;Open Science\&rsquo; or \&lsquo;Open Research\&rsquo; has brought to light a number of controversial and hotly debated topics. Evidence-based rational debate is regularly drowned out by misinformed or exaggerated rhetoric, which does not benefit the evolving system of scholarly communication. This article aims to provide a baseline evidence framework for ten of the most contested topics, in order to help frame and move forward discussions, practices, and policies. We address issues around preprints and scooping, the practice of copyright transfer, the function of peer review, predatory publishers, and the legitimacy of \&lsquo;global\&rsquo; databases. These arguments and data will be a powerful tool against misinformation across wider academic research, policy and practice, and will inform changes within the rapidly evolving scholarly publishing system.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  journal = {Publications},
  keywords = {copyright,impact factor,open access,open science,peer review,research evaluation,scholarly communication,Scopus,web of science},
  language = {en},
  number = {2}
}

@article{thibodeaux_Production_2016,
  title = {Production as Social Change: {{Policy}} Sociology as a Public Good},
  shorttitle = {Production as Social Change},
  author = {Thibodeaux, Jarrett},
  year = {2016},
  month = may,
  volume = {36},
  pages = {183--190},
  publisher = {{Routledge}},
  issn = {0273-2173},
  doi = {10.1080/02732173.2015.1102666},
  abstract = {Burawoy described two ways sociology can aid the public, through: (1) instrumental (policy) sociology and (2) reflexive (public) sociology. This article elaborates the different assumptions of how social change occurs according to policy and public sociology (and how sociology effects social change). Policy sociology assumes social change occurs through the scientific elaboration of the best means to achieve goals. However, policy sociology largely takes the public as an object of power rather than subjects who can utilize scientific knowledge. Public sociology assumes that social change occurs through the exposure of contradictions in goals, which elaborates better goals. However, the elaboration of contradictions assumes that there is a fundamental thesis/antithesis in society. If there are multiple goals/theses, public sociology fails in at least three ways. Policy sociology, when reflexively selecting its public, provides the best way sociology can aid the public.},
  annotation = {\_eprint: https://doi.org/10.1080/02732173.2015.1102666},
  journal = {Sociological Spectrum},
  number = {3}
}

@article{vantveer_Preregistration_2016,
  title = {Pre-Registration in Social Psychology\textemdash{{A}} Discussion and Suggested Template},
  author = {{van 't Veer}, Anna Elisabeth and {Giner-Sorolla}, Roger},
  year = {2016},
  month = nov,
  volume = {67},
  pages = {2--12},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2016.03.004},
  abstract = {Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied\textemdash reviewed and unreviewed pre-registration\textemdash and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts.},
  journal = {Journal of Experimental Social Psychology},
  keywords = {Pre-registration,Research methods,Reviewed pre-registration (RPR),Solid science,Unreviewed pre-registration (UPR)},
  language = {en},
  series = {Special {{Issue}}: {{Confirmatory}}}
}

@article{warren_How_2019,
  title = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}? {{Or}} to {{Get Tenure}}? {{Trends}} over a {{Generation}}},
  shorttitle = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}?},
  author = {Warren, John Robert},
  year = {2019},
  month = feb,
  volume = {6},
  pages = {172--196},
  issn = {2330-6696},
  doi = {10.15195/v6.a7},
  abstract = {Many sociologists suspect that publication expectations have risen over time\textemdash that how much graduate students have published to get assistant professor jobs and how much assistant professors have published to be promoted have gone up. Using information about faculty in 21 top sociology departments from the American Sociological Association's Guide to Graduate Departments of Sociology, online curricula vitae, and other public records, I provide empirical evidence to support this suspicion. On the day they start their first jobs, new assistant professors in recent years have already published roughly twice as much as their counterparts did in the early 1990s. Trends for promotion to associate professor are not as dramatic but are still remarkable. I evaluate several potential explanations for these trends and conclude that they are driven mainly by changes over time in the fiscal and organizational realities of universities and departments.},
  journal = {Sociological Science},
  language = {en-US}
}

@article{weston_Recommendations_2019,
  title = {Recommendations for {{Increasing}} the {{Transparency}} of {{Analysis}} of {{Preexisting Data Sets}}},
  author = {Weston, Sara J. and Ritchie, Stuart J. and Rohrer, Julia M. and Przybylski, Andrew K.},
  year = {2019},
  month = sep,
  volume = {2},
  pages = {214--227},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919848684},
  abstract = {Secondary data analysis, or the analysis of preexisting data, provides a powerful tool for the resourceful psychological scientist. Never has this been more true than now, when technological advances enable both sharing data across labs and continents and mining large sources of preexisting data. However, secondary data analysis is easily overlooked as a key domain for developing new open-science practices or improving analytic methods for robust data analysis. In this article, we provide researchers with the knowledge necessary to incorporate secondary data analysis into their methodological toolbox. We explain that secondary data analysis can be used for either exploratory or confirmatory work, and can be either correlational or experimental, and we highlight the advantages and disadvantages of this type of research. We describe how transparency-enhancing practices can improve and alter interpretations of results from secondary data analysis and discuss approaches that can be used to improve the robustness of reported results. We close by suggesting ways in which scientific subfields and institutions could address and improve the use of secondary data analysis.},
  journal = {Advances in Methods and Practices in Psychological Science},
  keywords = {bias,file drawer,p-hacking,panel design,preexisting data,preregistration,reproducibility,secondary analysis,transparency},
  language = {en},
  number = {3}
}

@article{wilson_Replication_1973,
  title = {The {{Replication Problem}} in {{Sociology}}: {{A Report}} and a {{Suggestion}}*},
  shorttitle = {The {{Replication Problem}} in {{Sociology}}},
  author = {Wilson, Franklin D. and Smoke, Gale L. and Martin, J. David},
  year = {1973},
  volume = {43},
  pages = {141--149},
  issn = {1475-682X},
  doi = {10.1111/j.1475-682X.1973.tb00711.x},
  abstract = {The deleterious effects of joint bias in favor of statistical inference and against replication are becoming well known. The acceptance of numerous Type I errors into the literature is by far the most serious of these. Data on the contents of three major journals support the contention that a joint bias for statistical significance tests, for rejections, and against replication exists in modern sociology. This finding replicates that of Sterling (1959) for psychology. A speculative analysis of the dynamics of publication decisions suggests that a compact format for reporting replications might make their publication more attractive to editors, and thus increase their frequency in the literature. A possible format for briefly reporting replication studies is suggested.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-682X.1973.tb00711.x},
  journal = {Sociological Inquiry},
  language = {en},
  number = {2}
}

@article{wingen_No_2020,
  title = {No {{Replication}}, {{No Trust}}? {{How Low Replicability Influences Trust}} in {{Psychology}}},
  shorttitle = {No {{Replication}}, {{No Trust}}?},
  author = {Wingen, Tobias and Berkessel, Jana B. and Englich, Birte},
  year = {2020},
  month = may,
  volume = {11},
  pages = {454--463},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550619877412},
  abstract = {In the current psychological debate, low replicability of psychological findings is a central topic. While the discussion about the replication crisis has a huge impact on psychological research, we know less about how it impacts public trust in psychology. In this article, we examine whether low replicability damages public trust and how this damage can be repaired. Studies 1\textendash 3 provide correlational and experimental evidence that low replicability reduces public trust in psychology. Additionally, Studies 3\textendash 5 evaluate the effectiveness of commonly used trust-repair strategies such as information about increased transparency (Study 3), explanations for low replicability (Study 4), or recovered replicability (Study 5). We found no evidence that these strategies significantly repair trust. However, it remains possible that they have small but potentially meaningful effects, which could be detected with larger samples. Overall, our studies highlight the importance of replicability for public trust in psychology.},
  journal = {Social Psychological and Personality Science},
  keywords = {crisis,open science,public trust,replicability,replication crisis},
  language = {en},
  number = {4}
}

@article{zenk-moltgen_Factors_2018,
  title = {Factors Influencing the Data Sharing Behavior of Researchers in Sociology and Political Science},
  author = {{Zenk-M{\"o}ltgen}, Wolfgang and Akdeniz, Esra and Katsanidou, Alexia and Na{\ss}hoven, Verena and Balaban, Ebru},
  year = {2018},
  month = jan,
  volume = {74},
  pages = {1053--1073},
  publisher = {{Emerald Publishing Limited}},
  issn = {0022-0418},
  doi = {10.1108/JD-09-2017-0126},
  abstract = {Purpose Open data and data sharing should improve transparency of research. The purpose of this paper is to investigate how different institutional and individual factors affect the data sharing behavior of authors of research articles in sociology and political science. Design/methodology/approach Desktop research analyzed attributes of sociology and political science journals (n=262) from their websites. A second data set of articles (n=1,011; published 2012-2014) was derived from ten of the main journals (five from each discipline) and stated data sharing was examined. A survey of the authors used the Theory of Planned Behavior to examine motivations, behavioral control, and perceived norms for sharing data. Statistical tests (Spearman's {$\rho$}, {$\chi$}2) examined correlations and associations. Findings Although many journals have a data policy for their authors (78 percent in sociology, 44 percent in political science), only around half of the empirical articles stated that the data were available, and for only 37 percent of the articles could the data be accessed. Journals with higher impact factors, those with a stated data policy, and younger journals were more likely to offer data availability. Of the authors surveyed, 446 responded (44 percent). Statistical analysis indicated that authors' attitudes, reported past behavior, social norms, and perceived behavioral control affected their intentions to share data. Research limitations/implications Less than 50 percent of the authors contacted provided responses to the survey. Results indicate that data sharing would improve if journals had explicit data sharing policies but authors also need support from other institutions (their universities, funding councils, and professional associations) to improve data management skills and infrastructures. Originality/value This paper builds on previous similar research in sociology and political science and explains some of the barriers to data sharing in social sciences by combining journal policies, published articles, and authors' responses to a survey.},
  journal = {Journal of Documentation},
  keywords = {Data availability,Data policy,Data sharing,Political science,practices,Replication,Research data management,Research transparency,Sociology,Theory of Planned Behaviour},
  number = {5}
}


